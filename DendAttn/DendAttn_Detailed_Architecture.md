# DendAttn 详细架构示意图

> 配置: B=2, L=1024, D=2048, H=8, d=256, E=8, Es=1, K=2, N=2, O=64
>
> 代码行号标注格式: `← Lxxx` 表示对应 `mob_gated_deltanet_moe/layer.py` 的第 xxx 行

---

## 完整架构总览图

```
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃                                    DendAttn Layer                                        ┃
┃                            (Multi-Branch Gated Delta Attention)                          ┃
┃                                                                                          ┃
┃  代码文件: /root/Chat/models/src/mob_gated_deltanet_moe/layer.py                         ┃
┃  主类: MobGatedDeltaNetMoE                                                               ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
                                           │
                                           ▼
┌─────────────────────────────────────────────────────────────────────────────────────────┐
│  INPUT: hidden_states                                                    ← L291         │
│  Shape: [B=2, L=1024, D=2048]                     batch_size, q_len, _ = hidden_states  │
│  Memory: 8.4 MB (FP16)                                                    .shape        │
└─────────────────────────────────────────────────────────────────────────────────────────┘
                                           │
          ┌────────────────────────────────┼────────────────────────────────┐
          │                                │                                │
          ▼                                ▼                                ▼
┌─────────────────────┐      ┌─────────────────────┐      ┌─────────────────────┐
│     Q Projection    │      │     K Projection    │      │     V Projection    │
│  ┌───────────────┐  │      │  ┌───────────────┐  │      │  ┌───────────────┐  │
│  │ Linear        │  │      │  │ Linear        │  │      │  │ Linear        │  │
│  │ W: [2048,2048]│  │      │  │ W: [2048,2048]│  │      │  │ W: [2048,4096]│  │
│  │ Params: 4.19M │  │      │  │ Params: 4.19M │  │      │  │ Params: 8.39M │  │
│  └───────────────┘  │      │  └───────────────┘  │      │  └───────────────┘  │
│  Out: [2,1024,2048] │      │  Out: [2,1024,2048] │      │  Out: [2,1024,4096] │
│        ← L301       │      │        ← L302       │      │        ← L303       │
│  q = self.q_proj(x) │      │  k = self.k_proj(x) │      │  v = self.v_proj(x) │
└─────────────────────┘      └─────────────────────┘      └─────────────────────┘
          │                                │                                │
          ▼                                ▼                                ▼
┌─────────────────────────────────────────────────────────────────────────────────────────┐
│  RESHAPE TO MULTI-HEAD FORMAT                                                ← L305     │
│  ═══════════════════════════════════════════════════════════════════════════════════   │
│  q: [2,1024,2048] → rearrange('b l (h d) -> h b l d') → [H=8, B=2, L=1024, d=256]      │
│  k: [2,1024,2048] → rearrange('b l (h d) -> h b l d') → [8, 2, 1024, 256]              │
│  v: [2,1024,4096] → rearrange('b l (h d) -> h b l d') → [8, 2, 1024, 512]              │
│                                                                                         │
│  q, k, v = map(lambda x: rearrange(x, 'b t (h d) -> h b t d', h=self.num_heads), ...)  │
└─────────────────────────────────────────────────────────────────────────────────────────┘
                                           │
                    ┌──────────────────────┴──────────────────────┐
                    │                                            │
                    ▼                                            ▼
┌───────────────────────────────────────┐    ┌───────────────────────────────────────────┐
│         SPARSE ROUTER                 │    │         MULTI-BRANCH EXPANSION            │
│  ┌─────────────────────────────────┐  │    │                                           │
│  │ Input: q [8,2,1024,256]         │  │    │  ┌─────────────────────────────────────┐  │
│  │    ↓ rearrange                  │  │    │  │ Per-Head Expert Projection          │  │
│  │ [HB=16, L=1024, d=256]          │  │    │  │                          ← L309-310 │  │
│  │    ↓                            │  │    │  │ q_proj_expand[0..7]:                │  │
│  │ ┌─────────────────────────┐     │  │    │  │   Linear(256 → 2048) × 8 heads     │  │
│  │ │ Gate Network   ← L258   │     │  │    │  │   Params: 4.19M total              │  │
│  │ │ Linear(256 → 7)         │     │  │    │  │                                     │  │
│  │ │ (ratio - shared_head)   │     │  │    │  │ k_proj_expand[0..7]:                │  │
│  │ └─────────────────────────┘     │  │    │  │   Linear(256 → 2048) × 8 heads     │  │
│  │    ↓                            │  │    │  │   Params: 4.19M total              │  │
│  │ router_logits: [16,1024,7]      │  │    │  └─────────────────────────────────────┘  │
│  │    ↓ softmax          ← L259    │  │    │                                           │
│  │ scores: [16,1024,7]             │  │    │  q[i] → q_proj_expand[i] → q_expanded[i] │
│  │    ↓ topk(k=2)        ← L260    │  │    │  k[i] → k_proj_expand[i] → k_expanded[i] │
│  │ routing_weights: [16,1024,2]    │  │    │                                           │
│  │ selected_idx: [16,1024,2]       │  │    │  q: [8,2,1024,256] → [8,2,1024,2048]     │
│  │    ↓                            │  │    │  k: [8,2,1024,256] → [8,2,1024,2048]     │
│  │ ┌─────────────────────────┐     │  │    │                                           │
│  │ │ Build Full Weights      │     │  │    │  ┌─────────────────────────────────────┐  │
│  │ │ shared: idx[0:1] = 1.0  │     │  │    │  │ Rearrange to Branch Dimension      │  │
│  │ │ routed: scatter topk    │     │  │    │  │ 'h b l (e d) -> e h b l d' ← L312  │  │
│  │ │ normalize: sum=1 ← L268 │     │  │    │  │                                     │  │
│  │ └─────────────────────────┘     │  │    │  │ q: [8,8,2,1024,256]                │  │
│  │    ↓ rearrange        ← L270    │  │    │  │    E H B  L   d                    │  │
│  │ router_weight: [E=8,B=2,L,H=8]  │  │    │  │ k: [8,8,2,1024,256]                │  │
│  │ router_mask: [8,2,1024,8] (int) │  │    │  │ v: repeat → [8,8,2,1024,512] ←L313│  │
│  └─────────────────────────────────┘  │    │  └─────────────────────────────────────┘  │
│                                       │    │                                           │
│  ← sparse() 方法: L241-273            │    │                                           │
│  调用位置: L307                        │    │                                           │
└───────────────────────────────────────┘    └───────────────────────────────────────────┘
                    │                                            │
                    │         ┌──────────────────────────────────┘
                    │         │
                    │         ▼
                    │    ┌───────────────────────────────────────────────────────────────┐
                    │    │  MERGE FOR CONVOLUTION                             ← L315     │
                    │    │  ═══════════════════════════════════════════════════════════ │
                    │    │  q: [8,8,2,1024,256] → '(e b) l (h d)' → [16, 1024, 2048]    │
                    │    │  k: [8,8,2,1024,256] → '(e b) l (h d)' → [16, 1024, 2048]    │
                    │    │  v: [8,8,2,1024,512] → '(e b) l (h d)' → [16, 1024, 4096]    │
                    │    └───────────────────────────────────────────────────────────────┘
                    │                                            │
                    │                                            ▼
                    │    ┌───────────────────────────────────────────────────────────────┐
                    │    │  SHORT CONVOLUTION (1D Depthwise Conv)             ← L326-346 │
                    │    │  ┌─────────────────┬─────────────────┬─────────────────┐     │
                    │    │  │   q_conv1d      │   k_conv1d      │   v_conv1d      │     │
                    │    │  │ kernel_size=4   │ kernel_size=4   │ kernel_size=4   │     │
                    │    │  │ hidden=2048     │ hidden=2048     │ hidden=4096     │     │
                    │    │  │ + SiLU activ.   │ + SiLU activ.   │ + SiLU activ.   │     │
                    │    │  └─────────────────┴─────────────────┴─────────────────┘     │
                    │    │  q: [16,1024,2048] → [16,1024,2048]                          │
                    │    │  k: [16,1024,2048] → [16,1024,2048]                          │
                    │    │  v: [16,1024,4096] → [16,1024,4096]                          │
                    │    └───────────────────────────────────────────────────────────────┘
                    │                                            │
                    │                                            ▼
                    │    ┌───────────────────────────────────────────────────────────────┐
                    │    │  RESHAPE BACK & APPLY ROUTER MASK                  ← L347-350 │
                    │    │  ═══════════════════════════════════════════════════════════ │
                    │    │  q: [16,1024,2048] → '(e b) l (h d) -> e b l h d'            │
                    │    │                   → [E=8, B=2, L=1024, H=8, d=256]           │
                    │    │  k: same → [8, 2, 1024, 8, 256]                              │
                    │    │  v: same → [8, 2, 1024, 8, 512]                              │
                    │    │                                                               │
                    │    │  ┌─────────────────────────────────────────────────────────┐ │
                    │    │  │ APPLY SPARSE MASK (Zero out non-selected branches)      │ │
                    │    │  │ q = q * router_mask[..., None]                          │ │
                    │    │  │ k = k * router_mask[..., None]                          │ │
                    │    │  │ v = v * router_mask[..., None]                          │ │
                    │    │  │                                                         │ │
                    │    │  │ router_mask: [8,2,1024,8] × [...,1]                     │ │
                    │    │  │ Only 3 branches active per token (1 shared + 2 routed)  │ │
                    │    │  └─────────────────────────────────────────────────────────┘ │
                    │    └───────────────────────────────────────────────────────────────┘
                    │                                            │
                    └────────────────────────────────────────────┤
                                                                 │
                                                                 ▼
┌─────────────────────────────────────────────────────────────────────────────────────────┐
│  GATING PARAMETERS (β and γ)                                         ← L363-366         │
│  ═══════════════════════════════════════════════════════════════════════════════════    │
│                                                                                         │
│  ┌──────────────────────────────────┐  ┌──────────────────────────────────────────┐     │
│  │ Beta (Input Gate)     ← L363     │  │ Gamma (Decay Gate)          ← L365       │     │
│  │ ────────────────────             │  │ ────────────────────                     │     │
│  │ b_proj: Linear(2048 → 64)        │  │ a_proj: Linear(2048 → 64)                │     │
│  │ Params: 131K                     │  │ A_log: Parameter([64])                   │     │
│  │                                  │  │ dt_bias: Parameter([64])                 │     │
│  │ beta = sigmoid(b_proj(x))        │  │                                          │     │
│  │ [2,1024,2048] → [2,1024,64]      │  │ g = -exp(A_log) * softplus(a_proj(x) +   │     │
│  │       ↓ rearrange                │  │                           dt_bias)       │     │
│  │ [E=8, B=2, L=1024, H=8]          │  │ [2,1024,64] → [8,2,1024,8]               │     │
│  │       ↓ × router_mask            │  │       ↓ × router_mask                    │     │
│  │ [8, 2, 1024, 8] (masked)         │  │ [8, 2, 1024, 8] (masked)                 │     │
│  └──────────────────────────────────┘  └──────────────────────────────────────────┘     │
└─────────────────────────────────────────────────────────────────────────────────────────┘
                                                                 │
                                                                 ▼
┌─────────────────────────────────────────────────────────────────────────────────────────┐
│  BLOCK-SPARSE PROCESSING (re_process function)                       ← L377-401         │
│  ═══════════════════════════════════════════════════════════════════════════════════    │
│                                                                                         │
│  Step 1: Merge branches and heads                                          ← L379       │
│  ───────────────────────────────────────────────────────────────────────────────────    │
│  q: [8,2,1024,8,256] → 'e b l h d -> b l (e h) d' → [2, 1024, 64, 256]                  │
│  k: [8,2,1024,8,256] →                            → [2, 1024, 64, 256]                │
│  v: [8,2,1024,8,512] →                            → [2, 1024, 64, 512]                │
│  g: [8,2,1024,8]     →                            → [2, 1024, 64]                     │
│  beta: [8,2,1024,8]  →                            → [2, 1024, 64]                     │
│                                                                                       │
│  Step 2: Q/K Block Splitting (re_process)                       ← re_process() L36-73 │
│  ───────────────────────────────────────────────────────────────────────────────────  │
│  Parameters: num_block=2, overlap=64                                                  │
│                                                                                       │
│  ┌─────────────────────────────────────────────────────────────────────────────────┐  │
│  │                         Q/K Dimension Splitting                                 │  │
│  │                                                                                 │  │
│  │  Original: d = 256                                                              │  │
│  │  window_size = (256 + (2-1)*64) / 2 = 160                                       │  │
│  │  step = 160 - 64 = 96                                                           │  │
│  │                                                                                 │  │
│  │  ┌──────────────────────────────────────────────────────────────────────────┐   │  │
│  │  │  d=256                                                                   │   │  │
│  │  │  ├─────────────────────────────────────────────────────────────────────┤ │   │  │
│  │  │  0                              160                                  256 │   │  │
│  │  │                                                                          │   │  │
│  │  │  Block 0: [0:160]  ─────────────────►  window_size=160                   │   │  │
│  │  │  Block 1: [96:256] ─────────────────►  window_size=160                   │   │  │
│  │  │            ↑                                                             │   │  │
│  │  │         overlap=64 (indices 96-159 are shared)                           │   │  │
│  │  └──────────────────────────────────────────────────────────────────────────┘   │  │
│  │                                                                                 │  │
│  │  q: [2,1024,64,256] → cat([slice0, slice1], dim=2) → [2,1024,128,160]           │  │
│  │  k: [2,1024,64,256] →                              → [2,1024,128,160]           │  │
│  │                       (64 heads × 2 blocks = 128 "virtual" heads)               │  │
│  └─────────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                         │
│  Step 3: V/g/beta Replication (for each block)                              ← L384     │
│  ───────────────────────────────────────────────────────────────────────────────────   │
│  v: [2,1024,64,512] → repeat('b l h d -> b l (k h) d', k=2) → [2,1024,128,512]       │
│  g: [2,1024,64]     → repeat('b l h -> b l (k h)', k=2)     → [2,1024,128]           │
│  beta: [2,1024,64]  → repeat                                → [2,1024,128]           │
│                                                                                      
│
└─────────────────────────────────────────────────────────────────────────────────────────┘
                                                                 │
                                                                 ▼
┌─────────────────────────────────────────────────────────────────────────────────────────┐
│  GATED DELTA RULE COMPUTATION (Core Recurrence)                      ← L390-401       │
│  ═══════════════════════════════════════════════════════════════════════════════════   │
│                                                                                         │
│  chunk_gated_delta_rule(                                                               │
│      q     = [2, 1024, 128, 160],   # [B, L, H', d_k']                                 │
│      k     = [2, 1024, 128, 160],   # [B, L, H', d_k']                                 │
│      v     = [2, 1024, 128, 512],   # [B, L, H', d_v]                                  │
│      g     = [2, 1024, 128],        # [B, L, H']                                       │
│      beta  = [2, 1024, 128],        # [B, L, H']                                       │
│  )                                                                                      │
│                                                                                         │
│  ┌─────────────────────────────────────────────────────────────────────────────────┐  │
│  │                         State Update Equation                                   │  │
│  │  ═══════════════════════════════════════════════════════════════════════════    │  │
│  │                                                                                  │  │
│  │  For each head h ∈ [0, 127] and timestep t ∈ [0, 1023]:                         │  │
│  │                                                                                  │  │
│  │  ┌─────────────────────────────────────────────────────────────────────────┐   │  │
│  │  │                                                                          │   │  │
│  │  │   S_t = g_t · S_{t-1} + β_t · (k_t^T ⊗ v_t)                            │   │  │
│  │  │                                                                          │   │  │
│  │  │   where:                                                                 │   │  │
│  │  │     S_t    : [d_k', d_v] = [160, 512]  (state matrix per head)          │   │  │
│  │  │     g_t    : scalar (decay gate, typically ∈ [-16, 0])                  │   │  │
│  │  │     β_t    : scalar (input gate, ∈ [0, 1])                              │   │  │
│  │  │     k_t    : [d_k'] = [160]                                             │   │  │
│  │  │     v_t    : [d_v] = [512]                                              │   │  │
│  │  │     k_t^T⊗v_t : [d_k', d_v] = [160, 512] (outer product)               │   │  │
│  │  │                                                                          │   │  │
│  │  └─────────────────────────────────────────────────────────────────────────┘   │  │
│  │                                                                                  │  │
│  │  ┌─────────────────────────────────────────────────────────────────────────┐   │  │
│  │  │                                                                          │   │  │
│  │  │   o_t = q_t · S_t                                                       │   │  │
│  │  │                                                                          │   │  │
│  │  │   where:                                                                 │   │  │
│  │  │     q_t : [d_k'] = [160]                                                │   │  │
│  │  │     S_t : [d_k', d_v] = [160, 512]                                      │   │  │
│  │  │     o_t : [d_v] = [512]                                                 │   │  │
│  │  │                                                                          │   │  │
│  │  └─────────────────────────────────────────────────────────────────────────┘   │  │
│  │                                                                                  │  │
│  │  Recurrent State Memory:                                                        │  │
│  │    S: [B=2, H'=128, d_k'=160, d_v=512] = 26.2 MB (fixed, independent of L!)    │  │
│  │                                                                                  │  │
│  └─────────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                         │
│  Output: o = [2, 1024, 128, 512]                                                       │
│                                                                                         │
└─────────────────────────────────────────────────────────────────────────────────────────┘
                                                                 │
                                                                 ▼
┌─────────────────────────────────────────────────────────────────────────────────────────┐
│  BLOCK AGGREGATION                                                   ← L403-405       │
│  ═══════════════════════════════════════════════════════════════════════════════════   │
│                                                                                         │
│  Step 1: Reshape to separate blocks                                                    │
│  o: [2,1024,128,512] → rearrange('b l (k h) d -> b l h k d', k=2) → [2,1024,64,2,512] │
│                                                                                         │
│  Step 2: Sum across blocks                                                             │
│  o: [2,1024,64,2,512] → sum(dim=-2) → [2,1024,64,512]                                 │
│                                                                                         │
│  (The two blocks' outputs are combined by summation)                                   │
│                                                                                         │
└─────────────────────────────────────────────────────────────────────────────────────────┘
                                                                 │
                                                                 ▼
┌─────────────────────────────────────────────────────────────────────────────────────────┐
│  MULTI-BRANCH WEIGHTED AGGREGATION                                   ← L407-408       │
│  ═══════════════════════════════════════════════════════════════════════════════════   │
│                                                                                         │
│  Step 1: Reshape back to branch dimension                                              │
│  o: [2,1024,64,512] → rearrange('b l (e h) d -> e b l h d', e=8) → [8,2,1024,8,512]   │
│                                                                                         │
│  Step 2: Weighted sum using router weights                                             │
│  ┌─────────────────────────────────────────────────────────────────────────────────┐  │
│  │                                                                                  │  │
│  │   o_final = Σ_{e=0}^{7} (router_weight[e] × o[e])                               │  │
│  │                                                                                  │  │
│  │   o: [E=8, B=2, L=1024, H=8, d=512]                                             │  │
│  │   router_weight: [E=8, B=2, L=1024, H=8]                                        │  │
│  │                                                                                  │  │
│  │   einsum('eblhd, eblh -> blhd')                                                 │  │
│  │                                                                                  │  │
│  │   Example weights for one token:                                                 │  │
│  │   ┌─────────────────────────────────────────────────────────────────────────┐   │  │
│  │   │ Branch │  0   │  1   │  2   │  3   │  4   │  5   │  6   │  7   │         │   │  │
│  │   │ Weight │ 0.33 │ 0.00 │ 0.00 │ 0.33 │ 0.00 │ 0.33 │ 0.00 │ 0.00 │ sum=1.0 │   │  │
│  │   │ Type   │shared│      │      │routed│      │routed│      │      │         │   │  │
│  │   └─────────────────────────────────────────────────────────────────────────┘   │  │
│  │                                                                                  │  │
│  └─────────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                         │
│  Output: o = [B=2, L=1024, H=8, d=512]                                                 │
│                                                                                         │
└─────────────────────────────────────────────────────────────────────────────────────────┘
                                                                 │
                                                                 ▼
┌─────────────────────────────────────────────────────────────────────────────────────────┐
│  OUTPUT GATING & PROJECTION                                          ← L449-457       │
│  ═══════════════════════════════════════════════════════════════════════════════════   │
│                                                                                         │
│  ┌─────────────────────────────────────────────────────────────────────────────────┐  │
│  │ Gate Projection                                                     ← L451          │  │
│  │ ────────────────                                                                │  │
│  │ g_proj: Linear(2048 → 4096), Params: 8.39M                                      │  │
│  │                                                                                  │  │
│  │ g = g_proj(hidden_states)                                                       │  │
│  │ [2,1024,2048] → [2,1024,4096]                                                   │  │
│  │        ↓ rearrange('b l (h d) -> b l h d', h=8)                                 │  │
│  │ [2,1024,8,512]                                                                  │  │
│  └─────────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                         │
│  ┌─────────────────────────────────────────────────────────────────────────────────┐  │
│  │ Fused RMSNorm + Swish Gate                                          ← L452      │  │
│  │ ─────────────────────────────                                                   │  │
│  │ o_norm: FusedRMSNormSwishGate(head_v_dim=512, eps=1e-5)                         │  │
│  │                                                                                  │  │
│  │ o = o_norm(o, g)                                                                │  │
│  │                                                                                  │  │
│  │ Computation:                                                                     │  │
│  │   o_normalized = o / sqrt(mean(o²) + eps)                                       │  │
│  │   o_gated = o_normalized * swish(g)                                             │  │
│  │   where swish(x) = x * sigmoid(x)                                               │  │
│  │                                                                                  │  │
│  │ Output: [2,1024,8,512]                                                          │  │
│  └─────────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                         │
│  ┌─────────────────────────────────────────────────────────────────────────────────┐  │
│  │ Final Reshape & Output Projection                                   ← L456-457     │  │
│  │ ─────────────────────────────────                                               │  │
│  │ o: [2,1024,8,512] → rearrange('b l h d -> b l (h d)') → [2,1024,4096]           │  │
│  │                                                                                  │  │
│  │ o_proj: Linear(4096 → 2048), Params: 8.39M                                      │  │
│  │                                                                                  │  │
│  │ o = o_proj(o)                                                                   │  │
│  │ [2,1024,4096] → [2,1024,2048]                                                   │  │
│  └─────────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                         │
└─────────────────────────────────────────────────────────────────────────────────────────┘
                                                                 │
                                                                 ▼
┌─────────────────────────────────────────────────────────────────────────────────────────┐
│  OUTPUT                                                              ← L459             │
│  Shape: [B=2, L=1024, D=2048]                                                           │
│  Memory: 8.4 MB (FP16)                                                                  │
│                                                                                         │
│  Return: (output, None, past_key_values, router_logits)                                 │
│          return o, None, past_key_values, None                                          │
└─────────────────────────────────────────────────────────────────────────────────────────┘
```

---

## 维度变化流程表

| 阶段 | 操作 | Q维度 | K维度 | V维度 | 其他 |
|------|------|-------|-------|-------|------|
| 输入 | - | - | - | - | hidden:[2,1024,2048] |
| 投影 | Linear | [2,1024,2048] | [2,1024,2048] | [2,1024,4096] | |
| 多头重排 | rearrange | [8,2,1024,256] | [8,2,1024,256] | [8,2,1024,512] | |
| 路由计算 | gate+topk | - | - | - | weight:[8,2,1024,8] |
| 分支扩展 | expert proj | [8,2,1024,2048] | [8,2,1024,2048] | - | |
| 重排分支 | rearrange | [8,8,2,1024,256] | [8,8,2,1024,256] | [8,8,2,1024,512] | |
| 合并 | rearrange | [16,1024,2048] | [16,1024,2048] | [16,1024,4096] | |
| 短卷积 | Conv1D | [16,1024,2048] | [16,1024,2048] | [16,1024,4096] | |
| 拆分+掩码 | reshape+mask | [8,2,1024,8,256] | [8,2,1024,8,256] | [8,2,1024,8,512] | |
| 合并头 | rearrange | [2,1024,64,256] | [2,1024,64,256] | [2,1024,64,512] | g,β:[2,1024,64] |
| 分块 | re_process | [2,1024,128,160] | [2,1024,128,160] | [2,1024,128,512] | g,β:[2,1024,128] |
| Delta Rule | chunk_gated | - | - | - | o:[2,1024,128,512] |
| 块聚合 | sum | - | - | - | o:[2,1024,64,512] |
| 分支聚合 | einsum | - | - | - | o:[2,1024,8,512] |
| 输出门控 | RMSNorm+Gate | - | - | - | o:[2,1024,8,512] |
| 最终投影 | Linear | - | - | - | o:[2,1024,2048] |

---

## 参数量详细分解

```
┌────────────────────────────────────────────────────────────────────────────────┐
│                              Parameter Count                                    │
├────────────────────────────────────────────────────────────────────────────────┤
│                                                                                │
│  ┌─────────────────────────────────────────────────────────────────────────┐  │
│  │ Input Projections                                                        │  │
│  │ ─────────────────                                                       │  │
│  │ q_proj:  [2048, 2048] = 4,194,304                                       │  │
│  │ k_proj:  [2048, 2048] = 4,194,304                                       │  │
│  │ v_proj:  [2048, 4096] = 8,388,608                                       │  │
│  │                        ───────────                                       │  │
│  │ Subtotal:              16,777,216 (16.78M)                              │  │
│  └─────────────────────────────────────────────────────────────────────────┘  │
│                                                                                │
│  ┌─────────────────────────────────────────────────────────────────────────┐  │
│  │ Expert Projections (Multi-Branch)                                        │  │
│  │ ──────────────────────────────────                                      │  │
│  │ q_proj_expand: [256, 2048] × 8 = 4,194,304                              │  │
│  │ k_proj_expand: [256, 2048] × 8 = 4,194,304                              │  │
│  │                                  ───────────                             │  │
│  │ Subtotal:                        8,388,608 (8.39M)                       │  │
│  └─────────────────────────────────────────────────────────────────────────┘  │
│                                                                                │
│  ┌─────────────────────────────────────────────────────────────────────────┐  │
│  │ Router                                                                   │  │
│  │ ──────                                                                  │  │
│  │ gate: [256, 7] × 8 heads (shared) = 14,336                              │  │
│  │                                     ──────                               │  │
│  │ Subtotal:                           14,336 (0.01M)                       │  │
│  └─────────────────────────────────────────────────────────────────────────┘  │
│                                                                                │
│  ┌─────────────────────────────────────────────────────────────────────────┐  │
│  │ Gating Parameters                                                        │  │
│  │ ──────────────────                                                      │  │
│  │ a_proj:   [2048, 64] = 131,072                                          │  │
│  │ b_proj:   [2048, 64] = 131,072                                          │  │
│  │ A_log:    [64]       = 64                                               │  │
│  │ dt_bias:  [64]       = 64                                               │  │
│  │                        ───────                                           │  │
│  │ Subtotal:              262,272 (0.26M)                                   │  │
│  └─────────────────────────────────────────────────────────────────────────┘  │
│                                                                                │
│  ┌─────────────────────────────────────────────────────────────────────────┐  │
│  │ Short Convolutions                                                       │  │
│  │ ──────────────────                                                      │  │
│  │ q_conv1d: kernel [2048, 1, 4] + bias = 8,192 + 2,048 ≈ 10,240          │  │
│  │ k_conv1d: kernel [2048, 1, 4] + bias ≈ 10,240                          │  │
│  │ v_conv1d: kernel [4096, 1, 4] + bias ≈ 20,480                          │  │
│  │                                        ──────                            │  │
│  │ Subtotal:                              ~40,960 (0.04M)                   │  │
│  └─────────────────────────────────────────────────────────────────────────┘  │
│                                                                                │
│  ┌─────────────────────────────────────────────────────────────────────────┐  │
│  │ Output Projections                                                       │  │
│  │ ───────────────────                                                     │  │
│  │ g_proj:  [2048, 4096] = 8,388,608                                       │  │
│  │ o_proj:  [4096, 2048] = 8,388,608                                       │  │
│  │ o_norm:  RMSNorm [512] ≈ 512                                            │  │
│  │                         ──────────                                       │  │
│  │ Subtotal:               16,777,728 (16.78M)                              │  │
│  └─────────────────────────────────────────────────────────────────────────┘  │
│                                                                                │
│  ═══════════════════════════════════════════════════════════════════════════  │
│  TOTAL PARAMETERS PER LAYER: ~42.26M                                          │
│  ═══════════════════════════════════════════════════════════════════════════  │
│                                                                                │
│  For 24-layer model:  42.26M × 24 = 1.01B parameters                          │
│  (excluding embeddings and LM head)                                           │
│                                                                                │
└────────────────────────────────────────────────────────────────────────────────┘
```

---

## 稀疏激活可视化

```
┌─────────────────────────────────────────────────────────────────────────────────────────┐
│  SPARSE ACTIVATION PATTERN (per token per head)                                         │
│  ═══════════════════════════════════════════════════════════════════════════════════   │
│                                                                                         │
│  Configuration: ratio=8, shared_head=1, topk=2                                         │
│                                                                                         │
│  Branch Index:    0      1      2      3      4      5      6      7                   │
│                   │      │      │      │      │      │      │      │                   │
│                   ▼      ▼      ▼      ▼      ▼      ▼      ▼      ▼                   │
│                ┌──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┐               │
│  Token 0:      │ ████ │      │      │ ████ │      │ ████ │      │      │               │
│                │shared│      │      │routed│      │routed│      │      │               │
│                └──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┘               │
│                                                                                         │
│                ┌──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┐               │
│  Token 1:      │ ████ │      │ ████ │      │      │      │      │ ████ │               │
│                │shared│      │routed│      │      │      │      │routed│               │
│                └──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┘               │
│                                                                                         │
│                ┌──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┐               │
│  Token 2:      │ ████ │ ████ │      │      │ ████ │      │      │      │               │
│                │shared│routed│      │      │routed│      │      │      │               │
│                └──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┘               │
│                                                                                         │
│                   ...  (pattern varies per token based on router decision)             │
│                                                                                         │
│  ───────────────────────────────────────────────────────────────────────────────────   │
│                                                                                         │
│  Statistics:                                                                            │
│  • Active branches per token: 3 (1 shared + 2 routed)                                  │
│  • Activation ratio: 3/8 = 37.5%                                                       │
│  • Computation reduction: 62.5%                                                        │
│  • Memory reduction: State only computed for active branches                           │
│                                                                                         │
│  Total activations for batch:                                                          │
│  • Theoretical max: B×L×H×E = 2×1024×8×8 = 131,072                                    │
│  • Actual: B×L×H×(Es+K) = 2×1024×8×3 = 49,152                                         │
│  • Sparsity: 62.5%                                                                     │
│                                                                                         │
└─────────────────────────────────────────────────────────────────────────────────────────┘
```

---

## Block-Sparse连接模式可视化

```
┌─────────────────────────────────────────────────────────────────────────────────────────┐
│  BLOCK-SPARSE CONNECTIVITY (inspired by dendritic compartments)                         │
│  ═══════════════════════════════════════════════════════════════════════════════════   │
│                                                                                         │
│  num_block=2, overlap=64, d=256                                                        │
│                                                                                         │
│  ┌───────────────────────────────────────────────────────────────────────────────────┐ │
│  │                                                                                    │ │
│  │   Original Q/K dimension: d = 256                                                  │ │
│  │                                                                                    │ │
│  │   ┌─────────────────────────────────────────────────────────────────────────────┐│ │
│  │   │                                 d = 256                                      ││ │
│  │   │   0         32        64        96       128       160       192       256  ││ │
│  │   │   ├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼───────┤││ │
│  │   │                                                                              ││ │
│  │   │   Block 0:  ████████████████████████████████████                            ││ │
│  │   │             [0 ─────────────────────────────── 160]                          ││ │
│  │   │                                                                              ││ │
│  │   │   Block 1:                      ████████████████████████████████████        ││ │
│  │   │                                 [96 ─────────────────────────────── 256]    ││ │
│  │   │                                                                              ││ │
│  │   │   Overlap:                      ████████████████                            ││ │
│  │   │                                 [96 ─────── 160]                             ││ │
│  │   │                                  (64 elements shared)                        ││ │
│  │   │                                                                              ││ │
│  │   └─────────────────────────────────────────────────────────────────────────────┘│ │
│  │                                                                                    │ │
│  │   This overlap enables:                                                           │ │
│  │   • Local dense connectivity (within each block)                                  │ │
│  │   • Global sparse connectivity (between blocks via overlap)                       │ │
│  │   • Information propagation across the entire dimension                           │ │
│  │                                                                                    │ │
│  │   Biological analogy:                                                             │ │
│  │   ┌────────────────────────────────────────────────────────────────────────────┐ │ │
│  │   │                                                                             │ │ │
│  │   │   Dendrite compartments:    ○───○───○───○───○───○                          │ │ │
│  │   │                              └─────┘   └─────┘                              │ │ │
│  │   │                              Block 0   Block 1                              │ │ │
│  │   │                                   └───┘                                     │ │ │
│  │   │                                  overlap                                    │ │ │
│  │   │                            (axial resistance)                               │ │ │
│  │   │                                                                             │ │ │
│  │   └────────────────────────────────────────────────────────────────────────────┘ │ │
│  │                                                                                    │ │
│  └───────────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                         │
└─────────────────────────────────────────────────────────────────────────────────────────┘
```

---

## 与Transformer对比

```
┌─────────────────────────────────────────────────────────────────────────────────────────┐
│                         DendAttn vs Transformer Comparison                              │
├─────────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                         │
│  ┌─────────────────────────────────┐    ┌─────────────────────────────────┐           │
│  │        Transformer              │    │          DendAttn               │           │
│  │  ┌─────────────────────────┐   │    │  ┌─────────────────────────┐   │           │
│  │  │ Attention Matrix        │   │    │  │ Recurrent State         │   │           │
│  │  │ A = softmax(QK^T/√d)   │   │    │  │ S_t = g·S_{t-1} + K^TV │   │           │
│  │  │                         │   │    │  │                         │   │           │
│  │  │ Shape: [B,H,L,L]       │   │    │  │ Shape: [B,H,d_k,d_v]    │   │           │
│  │  │ = [2,8,1024,1024]      │   │    │  │ = [2,128,160,512]       │   │           │
│  │  │ = 16.8 MB              │   │    │  │ = 26.2 MB (fixed!)      │   │           │
│  │  │                         │   │    │  │                         │   │           │
│  │  │ Complexity: O(L²)      │   │    │  │ Complexity: O(L)        │   │           │
│  │  └─────────────────────────┘   │    │  └─────────────────────────┘   │           │
│  │                                 │    │                                 │           │
│  │  KV Cache (inference):         │    │  State Cache (inference):       │           │
│  │  Size: O(L) per layer          │    │  Size: O(1) per layer           │           │
│  │  At 64k: 1075 MB              │    │  At 64k: 26.2 MB (same!)       │           │
│  │  At 512k: 8600 MB             │    │  At 512k: 26.2 MB (same!)      │           │
│  └─────────────────────────────────┘    └─────────────────────────────────┘           │
│                                                                                         │
│  ═══════════════════════════════════════════════════════════════════════════════════   │
│                                                                                         │
│  Inference Time Comparison (512k sequence):                                            │
│  ┌─────────────────────────────────────────────────────────────────────────────────┐  │
│  │                                                                                  │  │
│  │  Transformer:  ████████████████████████████████████████████████  4082 ms       │  │
│  │                                                                                  │  │
│  │  DendAttn:     ██                                                 121 ms        │  │
│  │                                                                                  │  │
│  │                                                    Speedup: 33.7×               │  │
│  │                                                                                  │  │
│  └─────────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                         │
│  Memory Comparison (varying sequence length):                                          │
│  ┌─────────────────────────────────────────────────────────────────────────────────┐  │
│  │  Seq Len  │ Transformer Memory │ DendAttn Memory │ Reduction                    │  │
│  │  ─────────┼────────────────────┼─────────────────┼─────────────                 │  │
│  │  1k       │ 16.8 MB            │ 26.2 MB         │ -56% (worse)                 │  │
│  │  4k       │ 67.2 MB            │ 26.2 MB         │ 61% better                   │  │
│  │  16k      │ 268.8 MB           │ 26.2 MB         │ 90% better                   │  │
│  │  64k      │ 1075 MB            │ 26.2 MB         │ 97.6% better                 │  │
│  │  512k     │ 8600 MB            │ 26.2 MB         │ 99.7% better                 │  │
│  └─────────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                         │
└─────────────────────────────────────────────────────────────────────────────────────────┘
```

---

## 生物学对应图

```
┌─────────────────────────────────────────────────────────────────────────────────────────┐
│                    Biological Neuron ←→ DendAttn Mapping                                │
├─────────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                         │
│  ┌─────────────────────────────────────────────────────────────────────────────────┐  │
│  │                                                                                  │  │
│  │                        Biological Dendritic Neuron                               │  │
│  │                                                                                  │  │
│  │                              ┌───┐                                               │  │
│  │                              │   │ ◄─── Soma (Cell Body)                         │  │
│  │                              └─┬─┘      = Weighted Aggregation                   │  │
│  │                    ┌──────────┼──────────┐                                       │  │
│  │                    │          │          │                                       │  │
│  │                    ▼          ▼          ▼                                       │  │
│  │               ┌────────┐ ┌────────┐ ┌────────┐                                   │  │
│  │               │ Branch │ │ Branch │ │ Branch │ ◄─── Dendritic Branches          │  │
│  │               │   1    │ │   2    │ │   3    │      = Expert Branches (ratio)   │  │
│  │               └────────┘ └────────┘ └────────┘                                   │  │
│  │                    │          │          │                                       │  │
│  │                 ┌──┴──┐    ┌──┴──┐    ┌──┴──┐                                    │  │
│  │                 ○  ○  ○    ○  ○  ○    ○  ○  ○  ◄─── Compartments                 │  │
│  │                                                    = num_block splits            │  │
│  │                 └──┬──┘    └──┬──┘    └──┬──┘                                    │  │
│  │                    │ overlap │ overlap │       ◄─── Axial Resistance            │  │
│  │                    └────┬────┴────┬────┘            = overlap parameter          │  │
│  │                         │         │                                              │  │
│  │                    Synaptic Inputs                                               │  │
│  │                    = Q, K, V projections                                         │  │
│  │                                                                                  │  │
│  └─────────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                         │
│  ┌─────────────────────────────────────────────────────────────────────────────────┐  │
│  │                         Mapping Table                                            │  │
│  │  ═══════════════════════════════════════════════════════════════════════════   │  │
│  │                                                                                  │  │
│  │  Biological Concept       │ DendAttn Implementation        │ Parameter          │  │
│  │  ─────────────────────────┼────────────────────────────────┼───────────         │  │
│  │  Dendritic branches       │ Expert branches                │ ratio=8            │  │
│  │  Sparse activation        │ TopK routing                   │ topk=2             │  │
│  │  Shared trunk             │ Shared branches                │ shared_head=1      │  │
│  │  Compartments             │ Block splitting                │ num_block=2        │  │
│  │  Axial resistance         │ Overlap between blocks         │ overlap=64         │  │
│  │  Membrane potential       │ Recurrent state S_t            │ [d_k, d_v]         │  │
│  │  Synaptic input           │ K^T ⊗ V outer product         │ key×value          │  │
│  │  Leak conductance         │ Decay gate g_t                 │ A_log, dt_bias     │  │
│  │  Input conductance        │ Input gate β_t                 │ b_proj             │  │
│  │  Soma integration         │ Weighted branch aggregation    │ router_weight      │  │
│  │  Axon output              │ Output projection o_proj       │ [4096, 2048]       │  │
│  │                                                                                  │  │
│  └─────────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                         │
└─────────────────────────────────────────────────────────────────────────────────────────┘
```

---

## 代码结构与文件对应

```
┌─────────────────────────────────────────────────────────────────────────────────────────┐
│                              Code Structure Mapping                                     │
├─────────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                         │
│  /root/Chat/models/src/                                                                │
│  │                                                                                      │
│  ├── gated_deltanet_p/                    ◄─── 基线模型 (单分支)                         │
│  │   ├── __init__.py                                                                   │
│  │   ├── configuration_gated_deltanet_p.py   配置类                                     │
│  │   ├── layer.py                            GatedDeltaNetp Layer实现                   │
│  │   └── modeling_gated_deltanet_p.py        完整模型定义                               │
│  │                                                                                      │
│  ├── mob_gated_deltanet/                  ◄─── 多分支模型 (密集激活)                     │
│  │   ├── __init__.py                                                                   │
│  │   ├── configuration_mob_gated_deltanet.py                                           │
│  │   ├── layer.py                            MobGatedDeltaNet Layer                    │
│  │   │   ├── re_process()      [L36-73]      Block分割函数                             │
│  │   │   ├── sparse()          [L243-292]    路由计算(带policy)                        │
│  │   │   └── forward()         [L295-478]    前向传播                                  │
│  │   └── modeling_mob_gated_deltanet.py                                                │
│  │                                                                                      │
│  └── mob_gated_deltanet_moe/              ◄─── 完整DendAttn (稀疏激活)                  │
│      ├── __init__.py                                                                   │
│      ├── configuration_mob_gated_deltanet_moe.py                                       │
│      ├── layer.py                            MobGatedDeltaNetMoE Layer                 │
│      │   ├── re_process()      [L36-73]      Block分割函数                             │
│      │   ├── sparse()          [L241-273]    路由计算(简化版)                          │
│      │   └── forward()         [L276-459]    前向传播                                  │
│      └── modeling_mob_gated_deltanet_moe.py                                            │
│                                                                                         │
│  Key Dependencies:                                                                      │
│  ├── fla.ops.gated_delta_rule                                                          │
│  │   ├── chunk_gated_delta_rule()        Chunk模式Delta Rule                           │
│  │   └── fused_recurrent_gated_delta_rule()  融合递归模式                               │
│  ├── fla.modules                                                                       │
│  │   ├── ShortConvolution                短卷积层                                       │
│  │   ├── RMSNorm                         RMS归一化                                      │
│  │   └── FusedRMSNormSwishGate          融合门控归一化                                  │
│  └── einops                                                                            │
│      ├── rearrange()                     张量重排                                       │
│      └── repeat()                        张量复制                                       │
│                                                                                         │
└─────────────────────────────────────────────────────────────────────────────────────────┘
```

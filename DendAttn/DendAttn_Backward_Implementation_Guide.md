# DendAttn 反向传播实现流程图

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    DendAttn 反向传播实现路线图                                    │
└─────────────────────────────────────────────────────────────────────────────────┘

══════════════════════════════════════════════════════════════════════════════════
                            阶段一: 前向计算图分析
══════════════════════════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────────────────────────────────────┐
│  Step 1: 绘制完整的前向计算图 (DAG)                                              │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│   需要做的事:                                                                    │
│   ┌────────────────────────────────────────────────────────────────────────┐    │
│   │ 1. 列出所有操作节点 (Linear, Conv1d, Softmax, TopK, einsum, etc.)      │    │
│   │ 2. 标注每个节点的输入/输出张量及其形状                                  │    │
│   │ 3. 标注数据流向 (哪个输出连接到哪个输入)                                │    │
│   │ 4. 识别分支点和汇合点                                                   │    │
│   └────────────────────────────────────────────────────────────────────────┘    │
│                                                                                  │
│   输出:                                                                          │
│   ┌────────────────────────────────────────────────────────────────────────┐    │
│   │  hidden_states ─┬─→ q_proj ─→ rearrange ─→ sparse() ─→ q_proj_expand   │    │
│   │                 ├─→ k_proj ─→ rearrange ─────────────→ k_proj_expand   │    │
│   │                 ├─→ v_proj ─→ rearrange ─────────────→ ...             │    │
│   │                 ├─→ a_proj ─→ softplus ─→ g                            │    │
│   │                 ├─→ b_proj ─→ sigmoid ─→ beta                          │    │
│   │                 └─→ g_proj ─→ ...                                      │    │
│   └────────────────────────────────────────────────────────────────────────┘    │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘
                                      │
                                      ▼
┌─────────────────────────────────────────────────────────────────────────────────┐
│  Step 2: 识别需要保存的中间变量                                                  │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│   分析原则:                                                                      │
│   ┌────────────────────────────────────────────────────────────────────────┐    │
│   │ • 非线性操作的输入 (sigmoid, softmax, relu 等需要保存输入或输出)       │    │
│   │ • 矩阵乘法的两个输入 (dY = dO @ B.T 需要 B; dB = A.T @ dO 需要 A)      │    │
│   │ • 有多个下游的中间结果                                                  │    │
│   │ • 递归状态 S_t (或 checkpoint)                                          │    │
│   └────────────────────────────────────────────────────────────────────────┘    │
│                                                                                  │
│   DendAttn 需要保存:                                                             │
│   ┌────────────────────────────────────────────────────────────────────────┐    │
│   │ • hidden_states (多个投影的输入)                                        │    │
│   │ • q, k, v (投影后, 扩展前)                                              │    │
│   │ • router_logits, router_weight, selected_idx (路由相关)                 │    │
│   │ • q_conv, k_conv, v_conv (卷积后)                                       │    │
│   │ • beta, g (门控参数)                                                    │    │
│   │ • o_branches (各分支输出, 用于路由权重梯度)                             │    │
│   │ • A 矩阵 (Delta Rule 的 WY 表示)                                        │    │
│   │ • h (chunk 状态) 或 checkpoints                                         │    │
│   └────────────────────────────────────────────────────────────────────────┘    │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘

══════════════════════════════════════════════════════════════════════════════════
                            阶段二: 逐操作推导梯度公式
══════════════════════════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────────────────────────────────────┐
│  Step 3: 按拓扑逆序分解每个操作                                                  │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│   反向传播顺序 (从输出到输入):                                                   │
│                                                                                  │
│   ┌─────────────────────────────────────────────────────────────────────────┐   │
│   │  12. o_proj          ← Linear                                           │   │
│   │  11. o_norm          ← FusedRMSNormSwishGate                            │   │
│   │  10. 分支加权聚合     ← einsum('eblhd,eblh->blhd')                       │   │
│   │   9. Block 聚合       ← sum(dim=-2)                                      │   │
│   │   8. Delta Rule      ← chunk_gated_delta_rule (核心!)                    │   │
│   │   7. Block 分割       ← re_process (重叠窗口)                            │   │
│   │   6. 门控参数         ← sigmoid, softplus, exp                           │   │
│   │   5. 稀疏掩码         ← 逐元素乘法                                       │   │
│   │   4. 短卷积           ← Conv1D + SiLU                                    │   │
│   │   3. 分支扩展         ← per-head Linear                                  │   │
│   │   2. 稀疏路由         ← Softmax + TopK                                   │   │
│   │   1. 输入投影         ← Linear (q, k, v, a, b, g)                        │   │
│   └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘
                                      │
                                      ▼
┌─────────────────────────────────────────────────────────────────────────────────┐
│  Step 4: 每个操作的梯度公式推导                                                  │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│  ┌───────────────────────────────────────────────────────────────────────────┐  │
│  │ 操作类型           │ 前向公式              │ 需要推导的反向公式            │  │
│  ├───────────────────────────────────────────────────────────────────────────┤  │
│  │ Linear             │ Y = XW + b            │ dX = dY @ W.T                 │  │
│  │                    │                       │ dW = X.T @ dY                 │  │
│  ├───────────────────────────────────────────────────────────────────────────┤  │
│  │ Sigmoid            │ y = σ(x)              │ dx = dy · σ(x) · (1-σ(x))     │  │
│  ├───────────────────────────────────────────────────────────────────────────┤  │
│  │ Softplus           │ y = log(1+exp(x))     │ dx = dy · σ(x)                │  │
│  ├───────────────────────────────────────────────────────────────────────────┤  │
│  │ Softmax            │ y_i = exp(x_i)/Σexp   │ dx = y · (dy - Σ(y·dy))       │  │
│  ├───────────────────────────────────────────────────────────────────────────┤  │
│  │ TopK               │ 选择前k个             │ 只有被选中的位置有梯度         │  │
│  │ (稀疏)             │                       │ (Straight-Through Estimator)  │  │
│  ├───────────────────────────────────────────────────────────────────────────┤  │
│  │ einsum             │ o = Σ_e w_e · o_e     │ do_e = do · w_e               │  │
│  │ (加权求和)         │                       │ dw_e = Σ(do · o_e)            │  │
│  ├───────────────────────────────────────────────────────────────────────────┤  │
│  │ sum                │ y = Σ_i x_i           │ dx_i = dy (广播)              │  │
│  ├───────────────────────────────────────────────────────────────────────────┤  │
│  │ 逐元素乘法         │ y = x · mask          │ dx = dy · mask                │  │
│  ├───────────────────────────────────────────────────────────────────────────┤  │
│  │ RMSNorm            │ y = x/rms(x) · γ      │ 需要完整推导 (见下方)         │  │
│  ├───────────────────────────────────────────────────────────────────────────┤  │
│  │ re_process         │ 重叠窗口分割          │ 重叠区域梯度累加              │  │
│  │ (Block分割)        │                       │                               │  │
│  ├───────────────────────────────────────────────────────────────────────────┤  │
│  │ Delta Rule         │ S_t = g·S_{t-1}+β·kv  │ ★ 核心推导 (见阶段三)         │  │
│  │ (递归)             │ o_t = q_t · S_t       │                               │  │
│  └───────────────────────────────────────────────────────────────────────────┘  │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘

══════════════════════════════════════════════════════════════════════════════════
                     阶段三: Delta Rule 递归反向 (核心难点)
══════════════════════════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────────────────────────────────────┐
│  Step 5: Delta Rule 前向递归公式分析                                             │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│   前向公式:                                                                      │
│   ┌────────────────────────────────────────────────────────────────────────┐    │
│   │                                                                         │    │
│   │   S_t = g_t · S_{t-1} + β_t · (k_t ⊗ v_t)      [状态更新]              │    │
│   │                                                                         │    │
│   │   o_t = q_t · S_t                              [输出计算]              │    │
│   │                                                                         │    │
│   │   其中:                                                                 │    │
│   │   • S_t ∈ R^{d_k × d_v}  (状态矩阵)                                    │    │
│   │   • g_t ∈ R              (衰减门, 标量或向量)                          │    │
│   │   • β_t ∈ R              (输入门)                                      │    │
│   │   • k_t ⊗ v_t            (外积, d_k × d_v)                             │    │
│   │                                                                         │    │
│   └────────────────────────────────────────────────────────────────────────┘    │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘
                                      │
                                      ▼
┌─────────────────────────────────────────────────────────────────────────────────┐
│  Step 6: Delta Rule 反向递归公式推导                                             │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│   给定: ∂L/∂o_t (从下游传来的梯度)                                              │
│   求解: ∂L/∂q_t, ∂L/∂k_t, ∂L/∂v_t, ∂L/∂g_t, ∂L/∂β_t                           │
│                                                                                  │
│   推导过程:                                                                      │
│   ┌────────────────────────────────────────────────────────────────────────┐    │
│   │                                                                         │    │
│   │  1. 从 o_t = q_t · S_t 得:                                             │    │
│   │     ─────────────────────                                               │    │
│   │     ∂L/∂q_t = ∂L/∂o_t · S_t^T                                          │    │
│   │                                                                         │    │
│   │     ∂L/∂S_t += q_t^T · ∂L/∂o_t    (累加, 因为 S_t 还影响后续时刻)       │    │
│   │                                                                         │    │
│   │  2. 从 S_t = g_t · S_{t-1} + β_t · (k_t ⊗ v_t) 得:                     │    │
│   │     ───────────────────────────────────────────                         │    │
│   │     ∂L/∂g_t = Σ_{i,j} (∂L/∂S_t)_{ij} · (S_{t-1})_{ij}                  │    │
│   │             = tr(∂L/∂S_t · S_{t-1}^T)                                   │    │
│   │                                                                         │    │
│   │     ∂L/∂β_t = Σ_{i,j} (∂L/∂S_t)_{ij} · (k_t ⊗ v_t)_{ij}               │    │
│   │             = tr(∂L/∂S_t · (k_t ⊗ v_t)^T)                              │    │
│   │             = (∂L/∂S_t · v_t) · k_t                                    │    │
│   │                                                                         │    │
│   │     ∂L/∂k_t = β_t · (∂L/∂S_t · v_t)                                    │    │
│   │                                                                         │    │
│   │     ∂L/∂v_t = β_t · (k_t^T · ∂L/∂S_t)                                  │    │
│   │                                                                         │    │
│   │  3. 状态梯度的递归传播:                                                 │    │
│   │     ─────────────────────                                               │    │
│   │     ∂L/∂S_{t-1} = g_t · ∂L/∂S_t                                        │    │
│   │                                                                         │    │
│   │  ★ 反向递归: 从 t=T 到 t=1, 累积 ∂L/∂S_t 并传播                        │    │
│   │                                                                         │    │
│   └────────────────────────────────────────────────────────────────────────┘    │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘
                                      │
                                      ▼
┌─────────────────────────────────────────────────────────────────────────────────┐
│  Step 7: Chunk-wise 反向优化                                                     │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│   问题: 逐时刻递归太慢, 需要 chunk 并行                                          │
│                                                                                  │
│   分析要点:                                                                      │
│   ┌────────────────────────────────────────────────────────────────────────┐    │
│   │ 1. Chunk 内部: 使用矩阵形式并行计算                                     │    │
│   │    • 构建 chunk 内的注意力矩阵 A (下三角)                               │    │
│   │    • 利用 WY 表示处理 Delta Rule 的更新                                 │    │
│   │                                                                         │    │
│   │ 2. Chunk 之间: 状态传递                                                 │    │
│   │    • 前向: h_{chunk} 传递到下一个 chunk                                 │    │
│   │    • 反向: dh_{chunk} 从后向前传递                                      │    │
│   │                                                                         │    │
│   │ 3. 参考 fla 库的实现:                                                   │    │
│   │    • chunk_gated_delta_rule_bwd_dhu: 计算 dh, du                        │    │
│   │    • chunk_bwd_dqkwg: 计算 dq, dk, dw, dg                               │    │
│   └────────────────────────────────────────────────────────────────────────┘    │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘

══════════════════════════════════════════════════════════════════════════════════
                            阶段四: 实现与验证
══════════════════════════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────────────────────────────────────┐
│  Step 8: 确定计算顺序和依赖关系                                                  │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│   反向传播依赖图:                                                                │
│   ┌────────────────────────────────────────────────────────────────────────┐    │
│   │                                                                         │    │
│   │   d_output ─→ d_o_proj ─→ d_o_norm ─→ d_branch_agg                     │    │
│   │                              │              │                           │    │
│   │                              ▼              ▼                           │    │
│   │                          d_g_gate    ┌─────┴─────┐                      │    │
│   │                              │       │           │                      │    │
│   │                              ▼       ▼           ▼                      │    │
│   │                        d_g_proj  d_o_branches  d_router_weight          │    │
│   │                                       │              │                  │    │
│   │                                       ▼              ▼                  │    │
│   │                              d_delta_rule      d_sparse_router          │    │
│   │                             /    |    \              │                  │    │
│   │                            ▼     ▼     ▼             ▼                  │    │
│   │                         d_q   d_k   d_v,d_g,d_β   d_gate                │    │
│   │                            \    |    /              │                  │    │
│   │                             ▼   ▼   ▼               │                  │    │
│   │                         d_q_expand, d_k_expand      │                  │    │
│   │                                  │                  │                  │    │
│   │                                  ▼                  │                  │    │
│   │                          d_q_proj, d_k_proj ←───────┘                  │    │
│   │                                  │                                      │    │
│   │                                  ▼                                      │    │
│   │                          d_hidden_states (汇总所有路径)                 │    │
│   │                                                                         │    │
│   └────────────────────────────────────────────────────────────────────────┘    │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘
                                      │
                                      ▼
┌─────────────────────────────────────────────────────────────────────────────────┐
│  Step 9: 数值验证策略                                                            │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│   验证方法:                                                                      │
│   ┌────────────────────────────────────────────────────────────────────────┐    │
│   │                                                                         │    │
│   │  1. 有限差分验证 (Finite Difference Check)                              │    │
│   │     ─────────────────────────────────────────                           │    │
│   │     对每个参数 θ:                                                       │    │
│   │     ∂L/∂θ ≈ [L(θ+ε) - L(θ-ε)] / (2ε)                                   │    │
│   │                                                                         │    │
│   │     比较解析梯度与数值梯度, 相对误差 < 1e-5                             │    │
│   │                                                                         │    │
│   │  2. torch.autograd.gradcheck                                            │    │
│   │     ─────────────────────────────                                       │    │
│   │     使用 float64 精度, 自动验证                                         │    │
│   │                                                                         │    │
│   │  3. 分模块验证                                                          │    │
│   │     ────────────                                                        │    │
│   │     先验证每个子操作的梯度正确性, 再组合验证                            │    │
│   │                                                                         │    │
│   │  4. 与 PyTorch 自动微分对比                                             │    │
│   │     ──────────────────────────                                          │    │
│   │     用纯 PyTorch 实现一个慢版本, 验证 Triton 版本的正确性               │    │
│   │                                                                         │    │
│   └────────────────────────────────────────────────────────────────────────┘    │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘
                                      │
                                      ▼
┌─────────────────────────────────────────────────────────────────────────────────┐
│  Step 10: 内存与性能优化                                                         │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│   优化决策:                                                                      │
│   ┌────────────────────────────────────────────────────────────────────────┐    │
│   │                                                                         │    │
│   │  1. 保存 vs 重计算 (Memory-Compute Trade-off)                           │    │
│   │     ─────────────────────────────────────────                           │    │
│   │     • 哪些中间结果保存? 哪些反向时重新计算?                             │    │
│   │     • Checkpoint 策略: 每隔多少 chunk 保存一次状态?                     │    │
│   │                                                                         │    │
│   │  2. Kernel 融合                                                         │    │
│   │     ───────────                                                         │    │
│   │     • 哪些连续操作可以融合到一个 kernel?                                │    │
│   │     • 减少全局内存读写次数                                              │    │
│   │                                                                         │    │
│   │  3. 并行策略                                                            │    │
│   │     ─────────                                                           │    │
│   │     • Batch 维度并行                                                    │    │
│   │     • Head 维度并行                                                     │    │
│   │     • Chunk 维度并行 (前向可以, 反向需要顺序)                           │    │
│   │                                                                         │    │
│   └────────────────────────────────────────────────────────────────────────┘    │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘

══════════════════════════════════════════════════════════════════════════════════
                               总结: 关键公式清单
══════════════════════════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────────────────────────────────────┐
│  必须推导的核心公式:                                                             │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│  ┌───────────────────────────────────────────────────────────────────────────┐  │
│  │  1. Delta Rule 状态梯度递归                                               │  │
│  │     ∂L/∂S_{t-1} = g_t · ∂L/∂S_t + 来自 o_{t-1} 的贡献                    │  │
│  │                                                                           │  │
│  │  2. Delta Rule 各变量梯度                                                 │  │
│  │     ∂L/∂q_t = ∂L/∂o_t · S_t^T                                            │  │
│  │     ∂L/∂k_t = β_t · (∂L/∂S_t · v_t)                                      │  │
│  │     ∂L/∂v_t = β_t · (k_t^T · ∂L/∂S_t)                                    │  │
│  │     ∂L/∂g_t = tr(∂L/∂S_t · S_{t-1}^T)                                    │  │
│  │     ∂L/∂β_t = tr(∂L/∂S_t · (k_t ⊗ v_t)^T)                               │  │
│  │                                                                           │  │
│  │  3. Softmax + TopK 路由梯度 (Straight-Through)                           │  │
│  │     ∂L/∂logits_i = softmax_i · (∂L/∂softmax_i - Σ_j softmax_j·∂L/∂s_j)  │  │
│  │                                                                           │  │
│  │  4. 分支加权聚合梯度                                                      │  │
│  │     ∂L/∂o_e = ∂L/∂o · w_e                                                │  │
│  │     ∂L/∂w_e = Σ(∂L/∂o · o_e)                                             │  │
│  │                                                                           │  │
│  │  5. Block 重叠分割的梯度累加                                              │  │
│  │     重叠区域: ∂L/∂x_overlap = ∂L/∂block1_overlap + ∂L/∂block2_overlap    │  │
│  │                                                                           │  │
│  │  6. FusedRMSNormSwishGate 梯度                                           │  │
│  │     (涉及 RMSNorm 和 Swish 门控的联合反向)                                │  │
│  │                                                                           │  │
│  └───────────────────────────────────────────────────────────────────────────┘  │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 简化版检查清单

| 阶段 | 任务 | 产出 |
|------|------|------|
| **分析** | 画出前向计算图 | DAG 图 |
| **分析** | 标注张量形状和数据流 | 带标注的计算图 |
| **分析** | 确定需要保存的变量 | 变量列表 |
| **推导** | 逐操作推导梯度公式 | 公式文档 |
| **推导** | Delta Rule 递归反向推导 | 核心公式 |
| **推导** | Chunk-wise 并行化公式 | 并行算法 |
| **实现** | 确定 kernel 划分和融合策略 | 设计文档 |
| **实现** | 编写 Triton kernel | 代码 |
| **验证** | 有限差分检查 | 测试通过 |
| **优化** | 内存/性能调优 | 性能报告 |

# Parallelizing Linear Transformers with the Delta Rule over Sequence Length - 论文总结

## 1. 研究背景和问题

### 现有技术的局限
- **Transformer的计算瓶颈**:标准Transformer的注意力机制具有序列长度的二次复杂度(O(L²)),这在处理长序列时会带来巨大的计算和内存开销
- **线性注意力的不足**:虽然线性Transformer和状态空间模型提供了线性时间复杂度的替代方案,但它们在需要上下文检索的任务上表现不佳
- **DeltaNet的训练效率问题**:DeltaNet虽然在联想记忆任务上表现出色,但现有的训练算法无法跨序列长度并行化,导致在现代硬件上训练效率低下

### 核心挑战
如何设计一个硬件高效的算法来训练使用delta规则的线性Transformer,使其能够在保持优秀记忆能力的同时,实现高效的并行训练?

## 2. 核心贡献

本文提出了一种用于训练DeltaNet的硬件高效算法,主要贡献包括:

1. **内存高效的重参数化**:将DeltaNet重新表述为矩阵值RNN,其递归由广义Householder变换给出,这使得可以使用WY表示来紧凑地表示Householder矩阵的乘积

2. **分块并行训练**:基于WY表示,扩展了线性注意力模型的分块并行训练策略,避免了在每个时间步显式化d×d隐藏状态,从而降低了I/O成本

3. **大规模语言建模验证**:将DeltaNet扩展到中等规模的语言建模任务(1.3B参数,100B tokens训练),在困惑度和下游任务的零样本性能上优于Mamba和GLA等强基线

4. **混合架构探索**:实验了两种混合模型:
   - DeltaNet + 滑动窗口注意力(每隔一层)
   - DeltaNet + 全局注意力(两层)
   这些混合模型超越了强Transformer基线

## 3. DeltaNet机制详解

### Delta规则 vs 加性更新

**标准线性Transformer的加性更新**:
```
St = St-1 + vt·kt^T
```

**DeltaNet的delta规则更新**:
```
St = St-1 - βt(St-1·kt - vt)·kt^T
```

其中:
- `St-1·kt`表示当前预测
- `vt`是目标值
- `βt`是学习率
- 更新基于预测与目标之间的"delta"(差异)

### 从两个角度理解Delta规则

**1. 在线回归优化视角**:
```
Lt(S) = 1/2 ||S·kt - vt||²
St = St-1 - βt·∇St-1·Lt(St-1)
```
这相当于使用单步SGD优化在线回归损失

**2. 键值检索视角**:
- 首先检索旧值:`v_old = St-1·kt`
- 插值得到新值:`v_new = βt·vt + (1-βt)·v_old`
- 移除旧关联并写入新关联:
  ```
  St = St-1 - v_old·kt^T + v_new·kt^T
  ```

### Delta规则的优势

- **更好的内存容量**:相比于Hebbian更新规则,delta规则具有更好的记忆容量
- **避免键冲突**:当序列长度L > 维度d时,加性更新容易导致键冲突,而delta规则可以学习移除不重要的键值关联
- **更强的梯度信号**:二次损失提供的梯度与误差幅度成比例,当预测远离目标时提供更强的修正

## 4. 并行算法实现

### 核心技术创新:WY表示

**问题**:直接计算需要在每个时间步显式化O(d²)的隐藏状态矩阵

**解决方案**:利用Householder矩阵乘积的WY表示

DeltaNet的状态转移可以写为:
```
St = St-1·(I - βt·kt·kt^T) + βt·vt·kt^T
```

这是一个广义Householder变换。关键观察:虽然St是d×d矩阵,但可以表示为:
```
St = Σ(i=1 to t) ui·ki^T
```

其中`ui`可以在O(d)内存中递归计算:
```
ut = βt·(vt - Σ(i=1 to t-1) ui·(ki^T·kt))
```

### 分块并行形式

将序列分成L/C个块,每块长度为C:

**块间递归**(方程8):
```
S[t+1] = S[t] + (U[t] - W[t]·S[t]^T)^T·K[t]
```

**块内并行**(方程9):
```
O[t] = Q[t]·S[t]^T + (Q[t]·K[t]^T ⊙ M)·(U[t] - W[t]·S[t]^T)
```

其中:
- `W[t], U[t]`是通过WY表示高效计算的块内矩阵
- 使用UT变换将递归计算转换为矩阵乘法

### UT变换优化

为了利用张量核心加速器,使用UT变换将方程7的递归形式重写为矩阵形式:

```python
T[t] = (I + tril(diag(β[t])·K[t]·K[t]^T, -1))^(-1) · diag(β[t])
W[t] = T[t]·K[t]
U[t] = T[t]·V[t]
```

下三角矩阵的逆可以通过前向替换高效求解。

### 复杂度分析

- **分块并行形式复杂度**:O(LCd + Ld²)
- **步骤数**(不含块级并行扫描):O(L/C)
- **特殊情况**:
  - C=L:恢复完全并行形式
  - C=1:恢复递归形式
- **实践中**:C通常设置为64或128,实现亚二次训练

## 5. 实验结果

### 5.1 合成基准测试

**MQAR(多查询联想回忆)**:
- DeltaNet在最难设置下达到完美准确率
- 在低维设置下优于Mamba

**MAD(机制架构设计)基准**:
- 在Fuzzy Recall任务上表现最佳(35.7% vs Transformer的29.8%)
- 在In-Context Recall达到100%
- 在Selective Copy达到100%

**RegBench(上下文语言学习)**:
- DeltaNet展现出强大的上下文学习能力

### 5.2 语言建模结果

**340M参数模型(15B tokens训练)**:

| 模型 | WikiText PPL | LAMBADA Acc | PIQA | HellaSwag |
|------|-------------|-------------|------|-----------|
| Transformer++ | 28.39 | 31.0 | 63.3 | 34.0 |
| Mamba | 28.39 | 30.6 | 65.0 | 35.4 |
| GLA (w. conv) | 29.47 | 31.3 | 65.1 | 33.8 |
| DeltaNet (w. conv) | **28.24** | **32.1** | 64.8 | 34.3 |
| + Sliding Attn | **27.06** | **33.4** | 64.0 | **35.3** |
| + Global Attn | **27.51** | **33.5** | 64.0 | 34.5 |

**1.3B参数模型(100B tokens训练)**:

| 模型 | WikiText PPL | LAMBADA Acc | ARC-c | 平均 |
|------|-------------|-------------|-------|------|
| Transformer++ | 16.85 | 48.9 | 26.5 | 50.9 |
| Mamba | 17.06 | 46.2 | 28.2 | 50.0 |
| GLA (w. conv) | 17.25 | 46.2 | 27.0 | 50.4 |
| DeltaNet | **16.87** | **48.9** | **28.3** | **51.6** |
| + Sliding Attn | **16.56** | **49.2** | **28.8** | **52.1** |
| + Global Attn | **16.55** | 48.8 | 28.1 | 51.8 |

### 5.3 召回密集型任务

在SWDE、SQuAD、FDA等真实世界召回任务上:
- 340M规模:DeltaNet(26.4/28.9/12.8)显著优于GLA(24.0/24.7/7.3)
- 1.3B规模:由于状态大小扩展性限制,DeltaNet略逊于GLA

### 5.4 训练吞吐量

在H100 GPU上,1.3B模型的训练速度:
- DeltaNet接近GLA的速度
- 显著快于Mamba
- 所有线性时间模型在长序列训练上都优于Transformer

速度提升(相比递归形式):
- 序列长度16K,头维度256:约30倍加速
- 随序列长度和头维度增加,加速比提升

## 6. 架构设计要点

### 6.1 DeltaNet层设计

**特征映射和归一化**:
```
kt = SiLU(WK·xt) / ||SiLU(WK·xt)||₂
qt = SiLU(WQ·xt) / ||SiLU(WQ·xt)||₂
βt = σ(Wβ·xt) ∈ (0,1)
```

- 使用SiLU激活(优于ELU+1)
- L2归一化(优于L1归一化)
- 当βt=1时,`I - kt·kt^T`成为投影矩阵

**参数分配**:
- DeltaNet层:4d² 参数
- SwiGLU FFN层:8d² 参数
- 与Transformer++大致相同

### 6.2 混合架构

**卷积层**:
- 在query/key/value投影后添加轻量级深度可分离卷积
- 推广shift SSM
- 参数和计算成本都很低效

**滑动窗口注意力**:
- 仿照Griffin和Samba,交错放置DeltaNet层和滑动窗口注意力层
- 提供局部比较能力

**全局注意力**:
- 仅在两层使用全局注意力:第2层和第(N/2+1)层
- 即使只替换少数层也很有帮助

## 7. 局限性与未来工作

### 7.1 当前局限

**1. 训练速度**:
- 虽然提出了硬件高效算法,但训练速度仍落后于GLA
- 原因:需要在核内对头维度进行"边缘化",类似softmax attention
- GLA由于元素级操作可以轻松使用tiling支持任意头维度

**2. 状态大小扩展性**:
- 受头维度限制,影响大模型的记忆容量
- 在1.3B规模的召回密集型任务上表现不如GLA
- 潜在解决方案:采用块对角广义Householder转移矩阵

**3. 长度泛化能力有限**:
- GLA和RetNet(以及某种程度上的Mamba)可以外推到训练长度之外
- DeltaNet缺乏显式衰减因子
- 可通过在递归中引入门控项来改进

### 7.2 未来研究方向

**更广泛的矩阵类**:
- 当前使用`Mt = I - βt·kt·kt^T`
- 可以泛化到对角加低秩(DPLR)形式:`Mt = D - at·bt^T`
- S4探索了数据无关的DPLR转移矩阵

**表达能力增强**:
- Recurrent DeltaNet和Modern Self-Referential Weight Matrix被发现更优
- 但这些模型超出线性RNN范围,无法跨序列长度并行化
- 存在并行性与表达能力之间的基本权衡

**混合策略**:
- TTT的混合跨块非线性和块内线性策略可能提供一个中间地带

## 8. 理论贡献和实现细节

### 8.1 与状态空间模型/线性RNN的关系

**通用联想RNN框架**:
```
St = St-1 • Mt + vt·kt^T  (递归)
ot = St·qt                (内存读出)
```

其中`•`是联想算子(如Hadamard积、矩阵乘法等)

**DeltaNet的独特之处**:
- 使用结构化矩阵`Mt = I - βt·kt·kt^T`来高效建模超越元素级递归的交互
- 标准矩阵乘法在没有结构假设时需要O(dn²),DeltaNet通过低秩结构实现O(dn)

**与其他模型的比较**:
- Mamba/GLA:使用Hadamard积(元素级)
- DeltaNet:使用结构化矩阵乘法
- 见论文表2的详细模型对比

### 8.2 与Hopfield网络的联系

- 线性Transformer可视为迭代Hopfield网络
- 普通线性Transformer使用Hebbian更新,内存容量有限
- DeltaNet使用delta规则,内存容量更好
- 高阶多项式和指数核可以增强内存容量
- Delta规则在固定状态大小下实现更好的召回-内存权衡

### 8.3 实现优化

**前向传播伪代码要点**:
1. 将Q, K, V, beta按块大小C重塑
2. 使用向量化前向替换计算T矩阵(方程10)
3. 计算W = T @ K_beta, U = T @ V_beta
4. 块间递归更新状态S
5. 结合块间和块内计算得到输出O

**反向传播**:
- 重新计算隐藏状态以节省GPU内存
- 改编FLASHLINEARATTENTION库

**FlashLinearAttention集成**:
- 分块并行DeltaNet层已集成到FlashLinearAttention库
- Triton实现,针对现代GPU优化

### 8.4 消融实验

**归一化方法**(340M模型):
- L1范数 + ELU+1: WikiText 31.12
- L2范数 + ELU+1: WikiText 28.03
- **L2范数 + SiLU**: WikiText **28.24** (最佳)
- L2范数 + ReLU: WikiText 28.75

**卷积的作用**:
- 340M DeltaNet无卷积: WikiText 29.08
- 340M DeltaNet有卷积: WikiText 28.24
- 卷积带来显著改进

## 9. 相关工作扩展

### 9.1 分块线性注意力

- Hua et al. [34]首次提出分块形式
- Sun et al. [108], Yang et al. [124]展示如何计算纯线性注意力的精确输出
- Qin et al. [91]和Yang et al. [124]讨论I/O感知硬件优化
- Sun et al. [107]推广到多节点分布式训练

### 9.2 混合模型

近期研究越来越关注开发结合线性递归层与以下组件的混合架构:
- 局部块注意力[61, 130, 25, 62, 74]
- 滑动窗口注意力[130, 6, 21, 95]
- 全局注意力[51, 52, 35, 26, 55, 78, 109]

Poli et al. [84]系统研究了混合模型的缩放规律。

### 9.3 Householder矩阵

- 广泛用于机器学习中的正交参数化
- 在归一化流中的应用
- Mathiasen et al.为归一化流开发了分块快速算法
- Mhammedi et al.使用WY表示减少训练非线性RNN时的内存占用

## 10. 结论

本文描述了一种跨序列长度维度并行化DeltaNet训练的算法,在现代硬件上实现了相对于现有实现的显著加速。这使得DeltaNet能够扩展到中等规模的语言建模基准(在100B tokens上训练1.3B模型),其中DeltaNet在语言建模和零样本下游任务性能方面优于Mamba和GLA等强线性递归基线。

**关键技术突破**:
- WY表示实现内存高效的Householder矩阵乘积计算
- UT变换将递归转换为适合张量核心的矩阵乘法
- 分块并行策略平衡计算效率和序列级并行性

**实践影响**:
- 证明delta规则可以在实际规模上带来性能提升
- 混合架构(DeltaNet + 注意力)提供了一个有前景的方向
- 为构建高效且强大的长序列模型提供了新工具

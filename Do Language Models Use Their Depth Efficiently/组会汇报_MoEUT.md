# MoEUT 组会汇报

> **论文**: Do Language Models Use Their Depth Efficiently?
> **会议**: NeurIPS 2025
> **作者**: Róbert Csordás, Christopher D. Manning, Christopher Potts (Stanford)
> **代码**: https://github.com/robertcsordas/llm_effective_depth

---

## 1. 核心问题：深度利用不足

### 发现：深层模型后半部分在"摸鱼"

```
📊 [INSERT: 论文 Figure 2 - Layer Contribution vs Depth]

关键观察：
┌────────────────────────────────────────┐
│ Layer 1-40%:  相对贡献 0.4-0.6        │  ← 真正在推理
│              余弦相似度 负值            │     （建立新理解）
│                                        │
│ ━━━━━━━━━ 相位转换点 ━━━━━━━━━       │
│                                        │
│ Layer 41-100%: 相对贡献 <0.2          │  ← 在摸鱼！
│               余弦相似度 正值           │     （微调概率）
│                                        │
│ Logitlens验证：40层就"想明白了"        │
└────────────────────────────────────────┘
```

**📷 截图位置1**: 论文Figure 2（层贡献度随深度变化）
**📷 截图位置2**: 论文Figure 3（余弦相似度分析）

---

### 问题：计算深度与任务复杂度无关

```
📊 [INSERT: 论文 Figure 6 - Depth Score vs Problem Difficulty]

MATH数据集（5个难度级别）：
┌──────────────────────────────────┐
│ Level 1 (Easy)   → Depth: 固定   │
│ Level 2          → Depth: 固定   │  ← 都用相同深度！
│ Level 3          → Depth: 固定   │
│ Level 4          → Depth: 固定   │
│ Level 5 (Hard)   → Depth: 固定   │
└──────────────────────────────────┘

结论：模型不会动态调整计算深度
     = 用"固定预算"处理所有问题
```

**📷 截图位置3**: 论文Figure 6（深度分数与难度无关）

---

## 2. MoEUT：更高效的深度利用

### 三大核心创新

```
┌─────────────────────────────────────────────────────────────┐
│ 1. UNIVERSAL TRANSFORMER (参数共享)              ⭐⭐⭐⭐⭐ │
│    ────────────────────────────────────────────────────    │
│    18层 → 只用9个物理层 → 反复应用                        │
│                                                            │
│    ┌────────────────────────────────────────────────┐     │
│    │ Layer 0,2,4,6,8,10,12,14,16  → Physical Layer 0│     │
│    │ Layer 1,3,5,7,9,11,13,15,17  → Physical Layer 1│     │
│    │           ...                                   │     │
│    └────────────────────────────────────────────────┘     │
│                                                            │
│    作用：可以"迭代"计算（类似RNN展开）                     │
│                                                            │
├─────────────────────────────────────────────────────────────┤
│ 2. SIGMOID ROUTING (非竞争性专家选择)            ⭐⭐⭐⭐⭐ │
│    ────────────────────────────────────────────────────    │
│    Softmax (传统):  scores = [0.35, 0.21, 0.28, ...]      │
│                     sum = 1.0 (零和博弈)                   │
│                     → 一个高，其他必然低                   │
│                                                            │
│    Sigmoid (MoEUT): scores = [0.88, 0.82, 0.86, ...]      │
│                     独立评分 ∈ [0,1]                       │
│                     → 可以同时都高！                       │
│                                                            │
│    关键：允许同一专家在多层被调用                          │
│          → 实现"潜空间迭代推理"                            │
│                                                            │
├─────────────────────────────────────────────────────────────┤
│ 3. CVMM 稀疏计算 (Triton优化)                    ⭐⭐⭐⭐⭐ │
│    ────────────────────────────────────────────────────    │
│    传统Dense MoE:                                          │
│    • 计算所有128个专家 → 选择16个 → 浪费87.5%             │
│                                                            │
│    MoEUT cvmm:                                             │
│    • 只计算选中的16个专家 → 节省8倍计算！                  │
│    • Triton JIT编译 → GPU高效执行                         │
│                                                            │
│    @triton.jit                                             │
│    def cvmm_kernel(...):                                   │
│        for expert_id in selected_experts:  # 只16个       │
│            result += matmul(x, W[expert_id])               │
│                                                            │
└─────────────────────────────────────────────────────────────┘
```

---

## 3. 潜空间迭代推理示例

### 数学题: "What is ((14)/(-6))/(1162/(-4980))?"

```
┌──────────────────────────────────────────────────────────────┐
│  标准Transformer (无法迭代)                                   │
│  ─────────────────────────                                   │
│  Layer 1-8:   理解问题，提取数字                             │
│  Layer 9-18:  停止计算！开始微调概率                         │
│               (无法重复使用"除法"能力)                       │
│                                                              │
└──────────────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────────────┐
│  MoEUT (潜空间迭代)                                          │
│  ─────────────────                                           │
│  Layer 1-2:   提取数字和运算符                               │
│               激活: [数字识别#3, 运算符#23]                  │
│                                                              │
│  Layer 3-5:   计算 14/(-6)                                   │
│               激活: [除法#89, 符号处理#34, 分数#56] ⭐       │
│                                                              │
│  Layer 6-8:   计算 1162/(-4980)                              │
│               激活: [除法#89, 符号处理#34, 分数#56] ⭐       │
│               ↑ 除法专家被重用！                             │
│                                                              │
│  Layer 9-12:  组合结果                                       │
│               激活: [除法#89, 复合运算#12] ⭐                │
│               ↑ 除法专家第三次被调用！                       │
│                                                              │
│  Layer 13-17: 验证输出                                       │
│               激活: [验证#45, 输出#101]                      │
│                                                              │
│  关键：同一专家（除法#89）在多层被迭代调用                   │
│       → 这就是"潜空间CoT"                                    │
│                                                              │
└──────────────────────────────────────────────────────────────┘
```

**📷 截图位置4**: 自己绘制的迭代推理示意图

---

## 4. 熵正则化：防止专家坍缩

### 问题：专家坍缩

```
┌──────────────────────────────────────────┐
│ 没有熵正则化：                            │
│                                          │
│ 客户1 → 专家#3, #7                       │
│ 客户2 → 专家#3, #7                       │
│ 客户3 → 专家#3, #7                       │
│   ...                                    │
│                                          │
│ 结果：                                    │
│ ✅ 专家#3, #7: 过度使用                  │
│ ❌ 其他126个专家: 完全闲置               │
│ 💸 浪费93.75%参数                        │
└──────────────────────────────────────────┘
```

### 解决方案：熵 = 不确定性 = 多样性

```
┌──────────────────────────────────────────────────────────┐
│ 低熵（坏）：                                              │
│ 专家#3: 90% ████████████████████                         │
│ 专家#7: 10% ██                                           │
│ 其他:    0%                                              │
│ 熵 ≈ 0.47  （很确定 → 没多样性）                         │
│                                                          │
├──────────────────────────────────────────────────────────┤
│ 高熵（好）：                                              │
│ 专家#1: 16% ████                                         │
│ 专家#2: 15% ████                                         │
│ 专家#3: 14% ███                                          │
│ 专家#4: 13% ███                                          │
│ 专家#5: 14% ███                                          │
│ ...                                                      │
│ 熵 ≈ 1.95  （很不确定 → 多样性高）                       │
│                                                          │
└──────────────────────────────────────────────────────────┘

实现：
loss = cross_entropy + 0.01×FFN_熵正则 + 0.001×Att_熵正则

def entropy_reg(sel, dim, mask):
    sel = F.log_softmax(sel, dim=-1)      # 归一化
    sel = log_mean(sel, dim, mask)         # 跨层/时间平均
    entropy = -(sel * sel.exp()).sum(-1)   # H = -Σp*log(p)
    return -entropy.mean()                 # 最小化负熵 = 最大化熵
```

**📷 截图位置5**: 熵正则化效果对比图（可自己绘制）

---

## 5. 关键实验结果

### Answer-only模式：验证深层推理能力

```
训练模式解释：
┌─────────────────────────────────────────────────────────┐
│ Question: "What is 14/(-6) ÷ 1162/(-4980)?"            │
│           ├───── 问题部分 ─────┤                       │
│                                                         │
│ Answer: "5.8"                                           │
│         ├─ 答案 ─┤                                      │
│                                                         │
│ ─────────────────────────────────────────────────────  │
│                                                         │
│ Q+A 模式:  整个序列都算损失                             │
│            → 模型可以边看题边"写草稿"                   │
│            → 外化推理过程                               │
│                                                         │
│ Answer-only 模式: 只在答案部分算损失                    │
│                   → 必须在"脑中"完成推理                │
│                   → 内化推理能力的真实考验              │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### 🔥 核心结果

```
📊 [INSERT: 论文 Table 1 - Extrapolation Results]

┌──────────────────────────────────────────────────────────┐
│                DeepMind Math 外推任务                     │
├──────────────────────────────────────────────────────────┤
│                                                          │
│  训练模式         标准Transformer    MoEUT    提升       │
│  ────────────────────────────────────────────────────   │
│  Q+A 都算损失          41%           36%      -5%       │
│  Answer-only          48%           63%     +15% 🔥    │
│                                                          │
│  ════════════════════════════════════════════════════   │
│                                                          │
│  关键洞察：                                              │
│  • 标准Transformer: 48% → 只提升7%                      │
│  • MoEUT: 63% → 提升27%                                 │
│  • 证明MoEUT真正学会了"潜空间迭代推理"                   │
│                                                          │
└──────────────────────────────────────────────────────────┘
```

**📷 截图位置6**: 论文Table 1（外推结果对比）

---

### 参数效率对比

```
┌──────────────────────────────────────────────────────────┐
│  模型配置 (18层, hidden=1024)                             │
├──────────────────────────────────────────────────────────┤
│                                                          │
│  指标              标准Transformer    MoEUT              │
│  ───────────────────────────────────────────────────    │
│  总参数            800M              712M                │
│  有效参数          800M              356M (50%!)         │
│  物理层数          18               9 (共享)             │
│  激活参数/token    800M              ~90M (稀疏)         │
│                                                          │
│  训练速度          1.0×             0.8× (稍慢)          │
│  推理速度          1.0×             ~2.0× (稀疏)         │
│                                                          │
│  Answer-only性能   48%              63% (+15%)          │
│                                                          │
└──────────────────────────────────────────────────────────┘

结论：用更少的参数，达到更好的性能！
```

**📷 截图位置7**: 参数量对比表（可自己制作）

---

## 6. 技术实现要点

### 核心代码片段

#### 1. Universal Transformer (参数共享)

```python
class UniversalTransformer:
    def __init__(self, create_layer, n_layers, group_size=2):
        # 关键：只创建 group_size 个物理层
        self.n_repeats = n_layers // group_size  # 18/2 = 9次
        self.layers = [create_layer() for _ in range(group_size)]  # 2层

    def forward(self, x):
        # 重复应用这些层
        for r in range(self.n_repeats):  # 重复9次
            for layer in self.layers:
                x = layer(x)  # 同一层，不同专家组合
        return x

# 效果：18个逻辑层，只用2个物理层的参数
```

#### 2. Sigmoid路由 + cvmm稀疏计算

```python
class SigmaMoE:
    def forward(self, input):
        # 1. Sigmoid选择（独立评分）
        sel = F.sigmoid(F.linear(input, self.expert_sel))  # [B,L,128]
        sel_val, sel_idx = sel.topk(k=16)  # 选16个

        # 2. 稀疏Up投影（只计算选中的！）
        sel_indices = cvmm_prepare_sel2(sel_idx, sel_val)
        scores = cvmm(input, sel_indices, self.keys)  # 只算16/128
        scores = F.relu(scores)

        # 3. 稀疏Down投影（带权重聚合）
        sel_indices.reduction_weight = sel_val
        out = cvmm(scores, sel_indices, self.values)

        return out  # 节省8倍计算！
```

#### 3. 熵正则化

```python
def collect_losses(model):
    reg_loss = 0
    for layer in model.modules():
        if isinstance(layer, SigmaMoE):
            # FFN熵正则（权重0.01）
            reg_loss += 0.01 * entropy_reg(layer.sel_hist)
        elif isinstance(layer, SwitchHead):
            # 注意力熵正则（权重0.001）
            reg_loss += 0.001 * layer.get_reg_loss()
    return reg_loss

# 总损失
total_loss = cross_entropy + reg_loss
```

---

## 7. 架构对比图

```
┌─────────────────────────────────┐    ┌─────────────────────────────────┐
│   Standard Transformer          │    │          MoEUT                  │
│  ┌─────────────────────────┐   │    │  ┌─────────────────────────┐   │
│  │ Layer 0: 独立参数       │   │    │  │ Layer 0: Group 0        │   │
│  │ Layer 1: 独立参数       │   │    │  │ Layer 1: Group 1        │   │
│  │ Layer 2: 独立参数       │   │    │  │ Layer 2: Group 0 ◄──────┼───┐
│  │ ...                    │   │    │  │ Layer 3: Group 1 ◄──────┼───┤
│  │ Layer 17: 独立参数      │   │    │  │ ...                     │   │重用
│  │                         │   │    │  │ Layer 17: Group 1       │   │
│  │ 18个独立层              │   │    │  │ 9个物理层 × 重复2次     │   │
│  └─────────────────────────┘   │    │  └─────────────────────────┘   │
│                                 │    │                                 │
│  后半部分闲置：                  │    │  全程激活：                      │
│  ┌─────────────────────────┐   │    │  ┌─────────────────────────┐   │
│  │ Layer 0-8:   ████████   │   │    │  │ All layers: ████████    │   │
│  │ Layer 9-17:  ░░░░        │   │    │  │   (通过专家迭代)         │   │
│  │                         │   │    │  │                         │   │
│  │ 有效深度: ~50%          │   │    │  │ 有效深度: ~90%          │   │
│  └─────────────────────────┘   │    │  └─────────────────────────┘   │
└─────────────────────────────────┘    └─────────────────────────────────┘
```

**📷 截图位置8**: 架构对比图（需自己绘制）

---

## 8. 核心贡献总结

### 🎯 论文贡献

1. **发现问题**：深层模型后半部分严重利用不足
   - 相位转换点在50%深度
   - 计算深度与问题复杂度无关

2. **提出MoEUT**：更高效的深度利用方案
   - Universal Transformer: 参数共享 → 可迭代
   - Sigmoid路由: 非竞争 → 支持重用
   - cvmm稀疏计算: 8倍加速

3. **实验验证**：Answer-only模式 +15%
   - 证明真正的"潜空间迭代推理"
   - 用更少参数达到更好性能

---

### 💡 关键洞察

```
┌──────────────────────────────────────────────────────────┐
│  当前Transformer的本质问题：                              │
│  ────────────────────────────                            │
│  • 固定深度 → 不能自适应                                 │
│  • 独立层 → 不能迭代计算                                 │
│  • 后半部分 → 在"拉伸"而非"深化"                         │
│                                                          │
│  MoEUT的解决方案：                                       │
│  ─────────────────                                       │
│  • 参数共享 → 可以迭代（像RNN展开）                      │
│  • Sigmoid路由 → 专家可重用（支持迭代推理）              │
│  • 稀疏激活 → 高效计算（8倍加速）                        │
│                                                          │
│  ════════════════════════════════════════════════════   │
│                                                          │
│  "深度不等于层数，而是有效计算步骤数"                     │
│                                                          │
│  MoEUT实现了：                                           │
│  • 潜空间CoT（向量空间的思维链）                         │
│  • 类似大脑反复调用不同脑区处理问题                       │
│                                                          │
└──────────────────────────────────────────────────────────┘
```

---

## 9. 截图位置汇总

| 位置 | 内容 | 来源 | 页码/图号 |
|------|------|------|-----------|
| 📷1 | 层贡献度 vs 深度 | 论文Figure 2 | p.5 |
| 📷2 | 余弦相似度分析 | 论文Figure 3 | p.6 |
| 📷3 | 深度分数 vs 难度 | 论文Figure 6 | p.8 |
| 📷4 | 迭代推理示意图 | 自己绘制 | - |
| 📷5 | 熵正则化效果 | 自己绘制 | - |
| 📷6 | 外推结果对比 | 论文Table 1 | p.7 |
| 📷7 | 参数量对比表 | 自己制作 | - |
| 📷8 | 架构对比图 | 自己绘制 | - |

### 补充截图建议

- **论文Figure 4**: Residual Erasure分析（显示MoEUT的阶梯状使用）
- **论文Figure 5**: 层跳过实验（对未来token的影响）
- **论文Figure 7**: 不同模型的线性映射对角线模式
- **代码示例**: cvmm Triton kernel关键代码

---

## 10. Q&A 预期问题

### Q1: 为什么Sigmoid比Softmax好？
**A**: Softmax是零和博弈，一个专家得分高会压低其他专家。Sigmoid独立评分，允许同一专家在多层都得高分，这对迭代推理至关重要。

### Q2: cvmm为什么这么快？
**A**: 传统MoE要计算所有128个专家再选16个，浪费87.5%。cvmm用Triton JIT只计算选中的16个，节省8倍计算和内存访问。

### Q3: 熵正则化的权重怎么定？
**A**: FFN用0.01，注意力用0.001。注意力更敏感，需要更小的权重。这是实验调出来的，太大会损害性能，太小防不了坍缩。

### Q4: 为什么Answer-only模式这么重要？
**A**: 这是检验"真正深层推理"的试金石。Q+A模式可以在生成问题部分时"写草稿"（外化推理），但Answer-only必须在内部完成推理（内化推理）。MoEUT提升27% vs 标准7%，证明真正学会了潜空间迭代。

### Q5: 能否扩展到100B+规模？
**A**: 论文只在244M规模验证，大规模扩展是开放问题。稀疏激活和参数共享理论上有优势，但训练稳定性（专家坍缩、负载均衡）需要更多工程。

---

*汇报时长: 15-20分钟*
*建议每页停留: 1-2分钟*
*重点: 第5节（实验结果）和第8节（核心贡献）*

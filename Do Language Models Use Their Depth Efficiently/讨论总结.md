# ã€ŠDo Language Models Use Their Depth Efficiently?ã€‹è®ºæ–‡è®¨è®ºæ€»ç»“

> **è®ºæ–‡ä¿¡æ¯**
> æ ‡é¢˜ï¼šDo Language Models Use Their Depth Efficiently?
> ä½œè€…ï¼šRÃ³bert CsordÃ¡s, Christopher D. Manning, Christopher Potts (Stanford University)
> ä¼šè®®ï¼šNeurIPS 2025
> æ—¶é—´ï¼š2025å¹´10æœˆ

---

## ä¸€ã€è®ºæ–‡æ ¸å¿ƒå‘ç°

### 1.1 ç ”ç©¶é—®é¢˜

**æ ¸å¿ƒç–‘é—®**ï¼šæ·±å±‚è¯­è¨€æ¨¡å‹æ˜¯å¦çœŸçš„åœ¨åš"æ›´æ·±çš„è®¡ç®—"ï¼Ÿ

- âœ… ç†æƒ³æƒ…å†µï¼šæ›´æ·±çš„å±‚ â†’ ç»„åˆæ›´å¤æ‚çš„ç‰¹å¾ â†’ æ›´å¼ºçš„æ¨ç†èƒ½åŠ›
- âš ï¸ å®é™…æƒ…å†µï¼šå±‚æ•°å¢åŠ å¸¦æ¥é€’å‡å›æŠ¥ï¼Œèƒ½åˆ æ‰ä¸€åŠå±‚è€Œæ€§èƒ½å½±å“ä¸å¤§

### 1.2 ä¸»è¦å‘ç°

#### ğŸ”´ å‘ç°1ï¼šå±‚çš„è´¡çŒ®ä¸å‡è¡¡ï¼ˆç›¸ä½è½¬æ¢ï¼‰

```
å‰åŠéƒ¨åˆ†ï¼ˆ1-50%ï¼‰ï¼š
- ç›¸å¯¹è´¡çŒ®ï¼š0.4-0.6
- ä½™å¼¦ç›¸ä¼¼åº¦ï¼šè´Ÿå€¼ï¼ˆæ“¦é™¤å’Œé‡ç»„ç‰¹å¾ï¼‰
- å±‚é—´ä¾èµ–ï¼šå¼ºï¼ˆåç»­å±‚ä¾èµ–å‰é¢å±‚ï¼‰

â”â”â”â”â”â”â”â”â” ç›¸ä½è½¬æ¢ç‚¹ï¼ˆçº¦50%å¤„ï¼‰â”â”â”â”â”â”â”â”â”

ååŠéƒ¨åˆ†ï¼ˆ51-100%ï¼‰ï¼š
- ç›¸å¯¹è´¡çŒ®ï¼š0.2ä»¥ä¸‹ï¼ˆæ€¥å‰§ä¸‹é™ï¼‰
- ä½™å¼¦ç›¸ä¼¼åº¦ï¼šæ­£å€¼ï¼ˆå¼ºåŒ–ç°æœ‰ç‰¹å¾ï¼‰
- å±‚é—´ä¾èµ–ï¼šå¼±ï¼ˆç›¸äº’ç‹¬ç«‹å·¥ä½œï¼‰
```

**ç»“è®º**ï¼šååŠéƒ¨åˆ†ä¸»è¦åœ¨"å¾®è°ƒæ¦‚ç‡åˆ†å¸ƒ"è€Œé"æ‰§è¡Œæ–°è®¡ç®—"

---

#### ğŸ”´ å‘ç°2ï¼šå±‚è·³è¿‡å®éªŒæ­ç¤ºçš„é—®é¢˜

**å®éªŒæ–¹æ³•**ï¼šè·³è¿‡æŸä¸€å±‚ï¼Œè§‚å¯Ÿå¯¹åç»­è®¡ç®—å’Œè¾“å‡ºçš„å½±å“

**ç»“æœ**ï¼š
- **å¯¹å½“å‰token**ï¼šæ‰€æœ‰å±‚éƒ½é‡è¦ï¼ˆåŒ…æ‹¬ååŠéƒ¨åˆ†ï¼‰
- **å¯¹æœªæ¥token**ï¼šååŠéƒ¨åˆ†å½±å“æå°ï¼

**LogitlenséªŒè¯**ï¼š
```
Layer 10: é¢„æµ‹è¿˜æ¨¡ç³Šï¼ˆKLæ•£åº¦é«˜ï¼‰
Layer 40: å·²ç»å¾ˆæ¥è¿‘æœ€ç»ˆè¾“å‡ºï¼ˆKLæ•£åº¦ä½ï¼‰
Layer 80: å‡ ä¹å®Œå…¨ä¸€æ ·

è¯´æ˜ï¼šåˆ°40å±‚å°±"æƒ³æ˜ç™½äº†"ï¼Œåé¢40å±‚åªæ˜¯åœ¨å¾®è°ƒ
```

**å…³é”®æ´å¯Ÿ**ï¼š
- ååŠéƒ¨åˆ†ä¸äº§ç”Ÿ"å¯é‡ç”¨çš„ä¸­é—´ç»“æœ"
- åªä¸ºå½“å‰tokenæœåŠ¡ï¼Œä¸å‚ä¸æ·±å±‚æ¨ç†

---

#### ğŸ”´ å‘ç°3ï¼šè®¡ç®—æ·±åº¦ä¸é—®é¢˜å¤æ‚åº¦æ— å…³

**å®éªŒ**ï¼š
- MATHæ•°æ®é›†ï¼š5ä¸ªéš¾åº¦çº§åˆ«
- MQuAKEæ•°æ®é›†ï¼š2-4è·³çš„å¤šè·³é—®é¢˜

**æ·±åº¦åˆ†æ•°ï¼ˆDepth Scoreï¼‰åˆ†æ**ï¼š
```
é—®é¢˜éš¾åº¦ï¼šLevel 1 â†’ Level 5
ä½¿ç”¨æ·±åº¦ï¼šå‡ ä¹ä¸å˜ï¼

è·³æ•°ï¼š2è·³ â†’ 4è·³
ä½¿ç”¨æ·±åº¦ï¼šå‡ ä¹ä¸å˜ï¼
```

**ç»“è®º**ï¼šæ¨¡å‹ç”¨"å›ºå®šè®¡ç®—é¢„ç®—"å¤„ç†æ‰€æœ‰é—®é¢˜ï¼Œä¸ä¼šåŠ¨æ€è°ƒæ•´æ·±åº¦

---

#### ğŸ”´ å‘ç°4ï¼šæ·±å±‚æ¨¡å‹åªæ˜¯"æ‹‰ä¼¸"äº†æµ…å±‚è®¡ç®—

**å®éªŒ**ï¼šè®­ç»ƒçº¿æ€§æ˜ å°„ Qwen 1.5B â†’ Qwen 14B

**ç»“æœ**ï¼šå‡ºç°æ¸…æ™°çš„**å¯¹è§’çº¿æ¨¡å¼**
```
1.5Bçš„ç¬¬7å±‚ï¼ˆ25%ï¼‰ â†” 14Bçš„ç¬¬12å±‚ï¼ˆ25%ï¼‰
1.5Bçš„ç¬¬14å±‚ï¼ˆ50%ï¼‰ â†” 14Bçš„ç¬¬24å±‚ï¼ˆ50%ï¼‰
```

**ç»“è®º**ï¼šæ·±å±‚æ¨¡å‹ä¸æ˜¯åšæ–°è®¡ç®—ï¼Œè€Œæ˜¯æŠŠç›¸åŒè®¡ç®—"åˆ†æ•£"åˆ°æ›´å¤šå±‚

---

### 1.3 è®ºæ–‡ç»“è®º

> **å½“å‰æ·±å±‚Transformerä¸æ˜¯åœ¨åš"æ·±åº¦ä¼˜å…ˆ"çš„è®¡ç®—ï¼Œè€Œæ˜¯"å¹¿åº¦ä¼˜å…ˆ"çš„è®¡ç®—**

**å…·ä½“è¡¨ç°**ï¼š
- âŒ ä¸ä¼šæ ¹æ®é—®é¢˜å¤æ‚åº¦åŠ¨æ€ä½¿ç”¨æ·±åº¦
- âŒ ä¸ä¼šç»„åˆä¸­é—´ç»“æœè¿›è¡Œæ·±å±‚æ¨ç†
- âŒ ååŠéƒ¨åˆ†å±‚ä¸¥é‡åˆ©ç”¨ä¸è¶³
- âœ… åªæ˜¯åœ¨æ›´ç»†ç²’åº¦åœ°è°ƒæ•´æ¦‚ç‡åˆ†å¸ƒ

**å½±å“**ï¼š
- è§£é‡Šäº†scaling lawçš„é¥±å’Œç°è±¡
- è§£é‡Šäº†ä¸ºä»€ä¹ˆCoTè¿™ä¹ˆæœ‰æ•ˆï¼ˆå¤–åŒ–æ¨ç†è¿‡ç¨‹ï¼‰
- è§£é‡Šäº†ä¸ºä»€ä¹ˆå±‚å‰ªææ•ˆæœä¸é”™
- å¯¹Latent Thinkingæ–¹æ³•æå‡ºè´¨ç–‘

---

## äºŒã€Transformerå·¥ä½œåŸç†æ·±åº¦è§£æ

### 2.1 æ ¸å¿ƒæ¶æ„

```python
# æ¯å±‚çš„ç»“æ„
Layer l:
  h_l = è¾“å…¥å‘é‡
  â†“
  h_temp = h_l + Attention(Norm(h_l))     # æ³¨æ„åŠ›
  â†“
  h_{l+1} = h_temp + MLP(Norm(h_temp))   # MLP
```

**å…³é”®ç‚¹**ï¼š
1. **æ®‹å·®è¿æ¥**ï¼šæ¯å±‚éƒ½æ˜¯"ç´¯åŠ "è€Œé"æ›¿æ¢"
2. **å‘é‡æµåŠ¨**ï¼štokenæœ¬èº«ä¸å˜ï¼Œå˜çš„æ˜¯å‘é‡è¡¨ç¤º
3. **å¯ä»¥"å·çœ‹"**ï¼šæ¯å±‚éƒ½å¯ä»¥é€šè¿‡Logitlensè¾“å‡ºé¢„æµ‹

---

### 2.2 æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttentionï¼‰= ä¿¡æ¯æ”¶é›†ä¸å…³è”

**ä½œç”¨**ï¼šä»ä¸Šä¸‹æ–‡æ”¶é›†ç›¸å…³ä¿¡æ¯

**è¿‡ç¨‹**ï¼š
```
å¥å­: "Mary went to store. She bought milk."

"She"åœ¨æŸå±‚çš„æ³¨æ„åŠ›ï¼š
1. Query(She): "æˆ‘éœ€è¦æ‰¾æŒ‡ä»£å¯¹è±¡"
2. æ‰«ææ‰€æœ‰è¯çš„Key
3. å‘ç°"Mary"çš„KeyåŒ¹é…åº¦æœ€é«˜
4. æå–"Mary"çš„Valueï¼ˆè¯­ä¹‰ç‰¹å¾ï¼‰
5. åŠ åˆ°æ®‹å·®æµï¼šç»™"She"æ ‡æ³¨"â†’Mary"
```

**å¤šå¤´æ³¨æ„åŠ›**ï¼šåŒæ—¶ä»å¤šä¸ªè§’åº¦ç†è§£
- å¤´1ï¼šè¯­æ³•å…³ç³»ï¼ˆä¸»è°“å®¾ï¼‰
- å¤´2ï¼šè¯­ä¹‰å…³ç³»ï¼ˆåŒä¹‰ã€åä¹‰ï¼‰
- å¤´3ï¼šæŒ‡ä»£å…³ç³»
- å¤´4ï¼šæ—¶é—´å…³ç³»
- ...

---

### 2.3 MLPå±‚ = æ·±åº¦ç‰¹å¾æå–

**ä½œç”¨**ï¼šå¯¹æ¯ä¸ªè¯ç‹¬ç«‹è¿›è¡Œéçº¿æ€§å˜æ¢

**è¿‡ç¨‹**ï¼š
```python
MLP(x) = Down(GELU(Up(x)))

Up:   [d, 4d]   # å±•å¼€åˆ°é«˜ç»´ç©ºé—´
GELU: éçº¿æ€§æ¿€æ´»  # é€‰æ‹©æ€§æ¿€æ´»
Down: [4d, d]   # å‹ç¼©å›åŸç»´åº¦
```

**ä¾‹å­**ï¼š"bank"çš„æ¶ˆæ­§
```
è¾“å…¥ï¼š"bank" + attentionæä¾›çš„"river"ä¸Šä¸‹æ–‡

1. UpæŠ•å½±ï¼šå±•å¼€æ¦‚å¿µ
   [é“¶è¡Œç‰¹å¾: 0.3, æ²³å²¸ç‰¹å¾: 0.1, å€¾æ–œç‰¹å¾: 0.05, ...]

2. GELUæ¿€æ´»ï¼šæ ¹æ®ä¸Šä¸‹æ–‡é€‰æ‹©
   [é“¶è¡Œ: 0.1â†“, æ²³å²¸: 0.9â†‘, å€¾æ–œ: 0.0â†“, ...]

3. DownæŠ•å½±ï¼šæå–å…³é”®ç‰¹å¾
   â†’ å¼ºåŒ–"æ²³å²¸"è¯­ä¹‰
```

---

### 2.4 å‰åŠvsååŠçš„å·¥ä½œå·®å¼‚

#### **å‰åŠéƒ¨åˆ†ï¼ˆLayer 1-50%ï¼‰**

**æ³¨æ„åŠ›**ï¼š
- ğŸ” ä¿¡æ¯æ”¶é›†ï¼šå¤§é‡ä»ä¸Šä¸‹æ–‡æå–ä¿¡æ¯
- ğŸ”— å»ºç«‹å…³è”ï¼šè¿æ¥ç›¸å…³è¯æ±‡
- ğŸ“Š ç‰¹å¾ç»„åˆï¼šæ„å»ºåˆæ­¥é«˜å±‚ç‰¹å¾

**MLP**ï¼š
- ğŸ§© ç‰¹å¾æå–ï¼šè¯†åˆ«æ¨¡å¼å’Œæ¦‚å¿µ
- ğŸ”„ éçº¿æ€§å˜æ¢ï¼šå»ºç«‹å¤æ‚æ˜ å°„
- ğŸ“ˆ è¡¨ç¤ºå­¦ä¹ ï¼šæ„å»ºæŠ½è±¡è¡¨ç¤º

**ç‰¹ç‚¹**ï¼š
- ä½™å¼¦ç›¸ä¼¼åº¦ï¼š**è´Ÿå€¼**ï¼ˆæ“¦é™¤é”™è¯¯ï¼Œå»ºç«‹æ–°ç†è§£ï¼‰
- å±‚é—´ä¾èµ–ï¼š**å¼º**ï¼ˆåç»­å±‚ä¸¥é‡ä¾èµ–å‰é¢å±‚ï¼‰
- å¯¹æœªæ¥tokenï¼š**å½±å“å¤§**ï¼ˆäº§ç”Ÿå¯é‡ç”¨çš„ä¸­é—´ç»“æœï¼‰

**æ¯”å–»**ï¼šåƒåœ¨ç†è§£å’Œæ¨ç†ï¼Œä¸æ–­ä¿®æ­£å’Œæ·±åŒ–ç†è§£

---

#### **ååŠéƒ¨åˆ†ï¼ˆLayer 51-100%ï¼‰**

**æ³¨æ„åŠ›**ï¼š
- ğŸ“‰ è´¡çŒ®éª¤é™ï¼ˆä»0.6é™åˆ°0.2ï¼‰
- ğŸ”’ ä¿¡æ¯å›ºåŒ–ï¼šä¸å†å¤§é‡ç§»åŠ¨ä¿¡æ¯
- ğŸ¨ ç»†å¾®è°ƒæ•´ï¼šå°å¹…ä¿®æ­£

**MLP**ï¼š
- ğŸ¯ è¾“å‡ºè°ƒä¼˜ï¼šè°ƒæ•´æ¦‚ç‡åˆ†å¸ƒ
- ğŸ”§ å¾®è°ƒç‰¹å¾ï¼šä¸åˆ›å»ºæ–°ç‰¹å¾
- ğŸ“Š åˆ†å¸ƒæ•´å½¢ï¼šè®©æ¦‚ç‡æ›´smooth

**ç‰¹ç‚¹**ï¼š
- ä½™å¼¦ç›¸ä¼¼åº¦ï¼š**æ­£å€¼**ï¼ˆå¼ºåŒ–ç°æœ‰ç‰¹å¾ï¼‰
- å±‚é—´ä¾èµ–ï¼š**å¼±**ï¼ˆå„å±‚ç‹¬ç«‹å·¥ä½œï¼‰
- å¯¹æœªæ¥tokenï¼š**å½±å“å°**ï¼ˆä¸äº§ç”Ÿå¯é‡ç”¨ç»“æœï¼‰

**æ¯”å–»**ï¼šåƒåœ¨æ¶¦è‰²æ–‡ç« ï¼Œåå¤ä¿®æ”¹æªè¾ä½†ä¸æ”¹å†…å®¹

---

### 2.5 ç†æƒ³ vs å®é™…

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ç†æƒ³æƒ…å†µï¼ˆæ·±åº¦æ¨ç†ï¼‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Layer 1-20:   ç†è§£å•è¯å’ŒçŸ­è¯­
Layer 21-40:  ç†è§£å¥å­ç»“æ„
Layer 41-60:  ç†è§£æ®µè½è¯­ä¹‰
Layer 61-80:  æå–æ–‡ç« ä¸»æ—¨
              â†“
         æ¯å±‚éƒ½åœ¨åšæ›´é«˜çº§çš„æŠ½è±¡

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
å®é™…æƒ…å†µï¼ˆå¹¿åº¦è°ƒæ•´ï¼‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Layer 1-40:   ç†è§£å¹¶æ¨ç†å‡ºç­”æ¡ˆï¼ˆå·²å®Œæˆï¼ï¼‰
              Logitlens: "ç­”æ¡ˆ" (99%ç½®ä¿¡åº¦)

Layer 41-80:  å¾®è°ƒæ¦‚ç‡åˆ†å¸ƒ
              Logitlens: "ç­”æ¡ˆ" (99%â†’99.1%â†’99.2%)

         ååŠéƒ¨åˆ†åœ¨æ‘¸é±¼ï¼
```

---

## ä¸‰ã€MoEUTï¼šæ›´é«˜æ•ˆçš„æ·±åº¦åˆ©ç”¨

### 3.1 ä»€ä¹ˆæ˜¯MoEUTï¼Ÿ

**MoEUT = Mixture-of-Experts + Universal Transformer**

#### **Universal Transformerï¼ˆé€šç”¨Transformerï¼‰**

```python
# æ ‡å‡†Transformerï¼šæ¯å±‚å‚æ•°ä¸åŒ
Layer 1: W1_attn, W1_mlp
Layer 2: W2_attn, W2_mlp  # å®Œå…¨ä¸åŒ
Layer 3: W3_attn, W3_mlp

# Universal Transformerï¼šæ‰€æœ‰å±‚å…±äº«å‚æ•°
Layer 1: W_shared
Layer 2: W_shared  # åŒä¸€ç»„å‚æ•°
Layer 3: W_shared
```

**ä¼˜åŠ¿**ï¼š
- å‚æ•°å¤ç”¨ â†’ å¯ä»¥"è¿­ä»£"è®¡ç®—
- åƒRNNä¸€æ ·åå¤åº”ç”¨åŒä¸€å˜æ¢
- ç†è®ºä¸Šå¯ä»¥è‡ªé€‚åº”æ·±åº¦

---

#### **Mixture of Expertsï¼ˆä¸“å®¶æ··åˆï¼‰**

```python
# æ ‡å‡†MLPï¼šæ‰€æœ‰è¾“å…¥èµ°åŒæ ·çš„è·¯å¾„
output = MLP(input)

# MoEï¼šæœ‰å¤šä¸ª"ä¸“å®¶"ï¼Œæ ¹æ®è¾“å…¥é€‰æ‹©
expert_1 = MLP_1(input)  # ä¸“å®¶1ï¼šæ“…é•¿æ•°å­¦
expert_2 = MLP_2(input)  # ä¸“å®¶2ï¼šæ“…é•¿è¯­æ³•
expert_3 = MLP_3(input)  # ä¸“å®¶3ï¼šæ“…é•¿å¸¸è¯†

# é—¨æ§ç½‘ç»œé€‰æ‹©ä¸“å®¶
gate = Router(input)
output = Î£ gate[i] * expert_i  # åŠ æƒç»„åˆ
```

**ä¼˜åŠ¿**ï¼š
- ä¸“ä¸šåŒ–åˆ†å·¥
- è™½ç„¶å‚æ•°å¤šï¼Œä½†æ¿€æ´»å°‘ï¼ˆTop-Kè·¯ç”±ï¼‰
- å¯æ‰©å±•æ€§å¼º

---

### 3.2 MoEUTçš„åˆ›æ–°

```python
class MoEUT_Layer:
    def __init__(self):
        self.attention = SharedAttention()
        self.experts = [Expert_1(), ..., Expert_N()]
        self.router = Router()

    def forward(self, h):
        h = h + self.attention(h)

        # æ ¹æ®è¾“å…¥åŠ¨æ€é€‰æ‹©ä¸“å®¶
        weights = self.router(h)
        expert_outputs = [e(h) for e in self.experts]
        h = h + weighted_sum(expert_outputs, weights)

        return h

# æ‰€æœ‰å±‚å…±äº«è¿™ä¸ªMoEUT_Layer
shared_layer = MoEUT_Layer()
for step in range(N):
    h = shared_layer(h)  # åå¤åº”ç”¨
```

**å…³é”®ç‰¹æ€§**ï¼š

1. **å±‚çº§å‚æ•°å…±äº« + ä¸“å®¶ä¸“ä¸šåŒ–**
   - æ‰€æœ‰å±‚ä½¿ç”¨åŒä¸€ç»„ä¸“å®¶
   - ä½†æ¯å±‚å¯ä»¥é€‰æ‹©ä¸åŒä¸“å®¶ç»„åˆ

2. **åŠ¨æ€ä¸“å®¶è·¯ç”±**
   - æ¯å±‚ã€æ¯tokenå¯é€‰ä¸åŒä¸“å®¶
   - å®ç°"è‡ªé€‚åº”è®¡ç®—"

3. **è¿­ä»£æ¨ç†èƒ½åŠ›**
   - å¯ä»¥é‡å¤è°ƒç”¨åŒä¸€ä¸“å®¶
   - å®ç°ç±»ä¼¼"å¾ªç¯"çš„æ•ˆæœ

---

### 3.3 ä¸ºä»€ä¹ˆMoEUTæ›´å¥½ï¼Ÿ

#### **æ ‡å‡†Transformerçš„é—®é¢˜**ï¼š
```
æ•°å­¦é¢˜ï¼š"What is ((14)/(-6))/(1162/(-4980))?"

Layer 1-8:   ç†è§£é—®é¢˜ï¼Œæå–æ•°å­—
Layer 9-18:  åœæ­¢è®¡ç®—ï¼å¼€å§‹è°ƒæ•´æ¦‚ç‡
             ï¼ˆæ— æ³•"è¿­ä»£"è®¡ç®—ï¼‰
```

#### **MoEUTçš„ä¼˜åŠ¿**ï¼š
```
åŒæ ·çš„é—®é¢˜

Layer 1-2:   æå–æ•°å­—
             è·¯ç”±â†’[æ•°å­—è¯†åˆ«ä¸“å®¶, è¿ç®—ç¬¦ä¸“å®¶]

Layer 3-5:   è®¡ç®— 14/(-6)
             è·¯ç”±â†’[é™¤æ³•ä¸“å®¶, ç¬¦å·å¤„ç†ä¸“å®¶]

Layer 6-8:   è®¡ç®— 1162/(-4980)
             è·¯ç”±â†’[é™¤æ³•ä¸“å®¶, ç¬¦å·å¤„ç†ä¸“å®¶]  # é‡ç”¨é™¤æ³•ä¸“å®¶ï¼

Layer 9-12:  ç»„åˆç»“æœ
             è·¯ç”±â†’[å¤åˆè¿ç®—ä¸“å®¶, ç»“æœç»„åˆä¸“å®¶]

Layer 13-16: éªŒè¯è¾“å‡º
             è·¯ç”±â†’[éªŒè¯ä¸“å®¶, è¾“å‡ºä¸“å®¶]
```

**å…³é”®**ï¼šå¯ä»¥**åå¤ä½¿ç”¨"é™¤æ³•ä¸“å®¶"**ï¼Œå®ç°è¿­ä»£è®¡ç®—ï¼

---

### 3.4 å®éªŒè¯æ®

| æ¨¡å‹ | è®­ç»ƒè®¾ç½® | å±‚åˆ©ç”¨ç‡ | å¤–æ¨èƒ½åŠ› |
|------|----------|----------|----------|
| Transformer | Q+A | âš ï¸ ååŠéƒ¨åˆ†é—²ç½® | 41% |
| Transformer | A only | âœ… ä½¿ç”¨æ›´å¤šå±‚ | 48% |
| **MoEUT** | **Q+A** | **âœ… è‰¯å¥½åˆ©ç”¨** | **36%** |
| **MoEUT** | **A only** | **âœ…âœ… æœ€ä½³åˆ©ç”¨** | **63%** |

**Residual Erasureåˆ†æ**ï¼š
- æ ‡å‡†Transformerï¼šä¿¡æ¯åˆ°ä¸­é—´å°±å›ºå®š
- MoEUTï¼šä¿¡æ¯æŒç»­åˆ°æ›´æ·±å±‚ï¼Œå‘ˆ"é˜¶æ¢¯çŠ¶"ä½¿ç”¨

---

### 3.5 MoEUT = å•å±‚MoE + æ½œç©ºé—´CoTï¼Ÿ

**ç­”ï¼šâœ… æœ¬è´¨ä¸Šå¯¹ï¼**

```
ä¼ ç»ŸCoTï¼ˆæ˜¾å¼ï¼‰ï¼š
è¾“å…¥ â†’ "Step1: ..." â†’ "Step2: ..." â†’ è¾“å‡º
      (ç”Ÿæˆæ–°token)    (ç”Ÿæˆæ–°token)

MoEUTï¼ˆéšå¼ï¼‰ï¼š
è¾“å…¥ â†’ [v1] â†’ [v2] â†’ [v3] â†’ è¾“å‡º
      (å˜æ¢å‘é‡) (å˜æ¢å‘é‡) (å˜æ¢å‘é‡)

      ä½¿ç”¨åŒä¸€MoEï¼Œä¸åŒä¸“å®¶ç»„åˆ
```

**å¯¹æ¯”**ï¼š

| ç»´åº¦ | æ˜¾å¼CoT | MoEUT |
|------|---------|-------|
| ç©ºé—´ | ç¦»æ•£token | è¿ç»­å‘é‡ |
| å±•å¼€æ–¹å‘ | æ¨ªå‘ï¼ˆåºåˆ—é•¿åº¦ï¼‰ | çºµå‘ï¼ˆå±‚æ·±åº¦ï¼‰ |
| å¯è§æ€§ | âœ… å¯è¯» | âŒ é»‘ç›’ |
| Contextå¼€é”€ | âŒ å¤§ | âœ… å° |
| è‡ªé€‚åº”æ€§ | âœ… ä»»æ„é•¿ | âš ï¸ å›ºå®šå±‚æ•° |

**æ›´å‡†ç¡®çš„å‘½å**ï¼š
- "Latent Iterative Reasoning"ï¼ˆæ½œåœ¨è¿­ä»£æ¨ç†ï¼‰
- "æ½œç©ºé—´CoT"ä¹Ÿå¯ä»¥ï¼

---

### 3.6 å®½åº¦è¿˜æ˜¯æ·±åº¦ï¼Ÿ

**ç­”ï¼šéƒ½æœ‰ï¼**

**è¡¨é¢**ï¼šå¢åŠ å®½åº¦
- å¤šä¸ªå¹¶è¡Œä¸“å®¶ï¼ˆ8ä¸ª vs 1ä¸ªMLPï¼‰
- å‚æ•°é‡å¢åŠ 

**å®è´¨**ï¼šå¢åŠ æœ‰æ•ˆæ·±åº¦
- é€šè¿‡é‡ç”¨ä¸“å®¶å®ç°è¿­ä»£
- ç®€å•é—®é¢˜ï¼šç”¨1-2å±‚å°±å¤Ÿ
- å¤æ‚é—®é¢˜ï¼šå¯ä»¥ç”¨æ»¡æ‰€æœ‰å±‚
- åŒä¸€ä¸“å®¶å¯ä»¥è¢«å¤šæ¬¡è°ƒç”¨

**å…³é”®æ´å¯Ÿ**ï¼š
```
å®½åº¦ï¼ˆå¤šä¸“å®¶ï¼‰ Ã— å…±äº«ï¼ˆå‚æ•°é‡ç”¨ï¼‰ = æœ‰æ•ˆæ·±åº¦
```

**ç±»æ¯”**ï¼š
```
æ ‡å‡†Transformer = æµæ°´çº¿
- æ¯ä¸ªå·¥ä½ä¸åŒ
- ä¸èƒ½å›å¤´
- æ·±åº¦ = å·¥ä½æ•°ï¼ˆå›ºå®šï¼‰

MoEUT = æŠ€èƒ½å·¥äººæ±  + è°ƒåº¦ç³»ç»Ÿ
- å¯ä»¥é‡å¤è°ƒç”¨åŒä¸€å·¥äºº
- å¯ä»¥è¿­ä»£
- æœ‰æ•ˆæ·±åº¦ = å®é™…è°ƒç”¨æ¬¡æ•°ï¼ˆè‡ªé€‚åº”ï¼‰
```

---

### 3.7 MoEUT ä»£ç å®ç°è¯¦è§£

> **ä»£ç ä½ç½®**ï¼š`llm_effective_depth-master/training/layers/moeut.py`

#### **æ ¸å¿ƒæ¶æ„å±‚æ¬¡**

```
MoEUT (moeut.py:321)
  â””â”€> UniversalTransformer (å‚æ•°å…±äº«åŸºç¡€)
       â””â”€> MoEUTLayer Ã— group_size (å…±äº«çš„å±‚)
            â”œâ”€> SwitchHeadRope (MoEæ³¨æ„åŠ›)
            â”‚    â”œâ”€> Q, K æŠ•å½± (æ ‡å‡†)
            â”‚    â”œâ”€> V æŠ•å½± (æ¯å¤´å¤šä¸“å®¶)
            â”‚    â”œâ”€> O æŠ•å½± (æ¯å¤´å¤šä¸“å®¶)
            â”‚    â””â”€> ä¸“å®¶é€‰æ‹© (Sigmoid + TopK)
            â””â”€> SigmaMoE (MoE FFN)
                 â”œâ”€> Up æŠ•å½± (æ¯ä¸“å®¶ç‹¬ç«‹)
                 â”œâ”€> Down æŠ•å½± (æ¯ä¸“å®¶ç‹¬ç«‹)
                 â”œâ”€> ä¸“å®¶é€‰æ‹© (Sigmoid + TopK)
                 â””â”€> cvmm (æ¡ä»¶çŸ©é˜µä¹˜æ³•ä¼˜åŒ–)
```

---

#### **1. UniversalTransformer - å‚æ•°å…±äº«æœºåˆ¶**

**æ–‡ä»¶**ï¼š`universal_transformer.py:16-51`

```python
class UniversalTransformer:
    def __init__(self, create_layer, d_model, n_layers, group_size=2):
        # å…³é”®ï¼šåªåˆ›å»º group_size ä¸ªç‰©ç†å±‚
        self.n_repeats = n_layers // group_size
        self.layers = torch.nn.ModuleList([
            create_layer() for _ in range(group_size)
        ])

    def forward(self, x, mask, kv_cache):
        # é‡å¤åº”ç”¨è¿™äº›å±‚
        for r in range(self.n_repeats):
            for li, layer in enumerate(self.layers):
                li_abs = r * len(self.layers) + li
                x, cache = layer(x, mask, kv_cache=cache, strength=1)
        return x
```

**åˆ›æ–°ç‚¹**ï¼š
- å‡è®¾ `n_layers=16, group_size=2`
- åªåˆ›å»º **2ä¸ªç‰©ç†å±‚**ï¼Œé‡å¤ **8æ¬¡**
- æ€»å…± **16ä¸ªé€»è¾‘å±‚**ï¼Œä½†å‚æ•°é‡åªæœ‰æ™®é€šTransformerçš„ **1/8**
- æ¯æ¬¡è¿­ä»£å¯ä»¥é€‰æ‹©ä¸åŒä¸“å®¶ï¼Œå®ç°"æ½œç©ºé—´æ¨ç†"

**åˆå§‹åŒ–ç­–ç•¥**ï¼š
```python
# å‚æ•°ç¼©æ”¾è€ƒè™‘äº†é‡å¤æ¬¡æ•°
scale = sqrt(2 / (n_repeats * len(layers)))  # moeut.py:49
```

---

#### **2. SigmaMoE - MoE FFN æ ¸å¿ƒå®ç°**

**æ–‡ä»¶**ï¼š`moeut.py:41-126`

##### **å‚æ•°ç»“æ„**ï¼š

```python
class SigmaMoE:
    def __init__(self, dmodel, n_experts, expert_size, k):
        # ä¸“å®¶å‚æ•°ï¼š[n_experts, dmodel, expert_size]
        self.keys = Parameter([n_experts, dmodel, expert_size])
        # ä¸‹æŠ•å½±ï¼š[n_experts, expert_size, dmodel]
        self.values = Parameter([n_experts, expert_size, dmodel])
        # ä¸“å®¶é€‰æ‹©å™¨ï¼š[n_experts, dmodel]
        self.expert_sel = Parameter([n_experts, dmodel])

        self.k_vec_dim = dmodel
        self.n_heads = k  # Top-K é€‰æ‹©
```

##### **å‰å‘ä¼ æ’­æµç¨‹**ï¼š

```python
def forward(self, input):
    # 1. ä¸“å®¶é€‰æ‹© (Sigmoid + TopK)
    sel = F.linear(input, self.expert_sel)        # [batch, seq, n_experts]
    sel = F.sigmoid(sel)                          # ä¸ç”¨ softmaxï¼
    sel_val, sel_index = sel.topk(k, dim=-1)     # é€‰ Top-K ä¸“å®¶

    # 2. é¢„å¤„ç†é€‰æ‹©ç´¢å¼•ï¼ˆä¼˜åŒ–GPUè®¿é—®ï¼‰
    sel_indices = cvmm_prepare_sel2(sel_index.int())

    # 3. Up æŠ•å½±ï¼ˆåªè®¡ç®—é€‰ä¸­çš„ä¸“å®¶ï¼‰
    scores = cvmm(input, sel_indices, self.keys)  # æ¡ä»¶çŸ©é˜µä¹˜æ³•
    scores = self.activation(scores)              # ReLU/GELU

    # 4. Down æŠ•å½±ï¼ˆå¸¦æƒé‡èšåˆï¼‰
    sel_indices.reduction_weight = sel_val        # è®¾ç½®èšåˆæƒé‡
    out = cvmm(scores, sel_indices, self.values)

    return out  # [batch, seq, dmodel]
```

**å…³é”®åˆ›æ–°**ï¼š
- **Sigmoid è€Œé Softmax**ï¼šå…è®¸ä¸“å®¶ç‹¬ç«‹è¯„åˆ†ï¼Œä¸å¼ºåˆ¶ç«äº‰
- **cvmm ç¨€ç–è®¡ç®—**ï¼šåªè®¡ç®—é€‰ä¸­çš„ k ä¸ªä¸“å®¶ï¼ˆk=8ï¼Œè€Œ n_experts=128ï¼‰
- **èŠ‚çœè®¡ç®—é‡**ï¼š128ä¸ªä¸“å®¶åªè®¡ç®—8ä¸ª = èŠ‚çœ **16å€** è®¡ç®—

---

#### **3. SwitchHeadRope - MoE æ³¨æ„åŠ›å®ç°**

**æ–‡ä»¶**ï¼š`moeut.py:286-284`

##### **å‚æ•°ç»“æ„**ï¼š

```python
class SwitchHeadRope(SwitchHeadCore):
    def __init__(self, state_size, n_heads, n_experts, moe_k=2):
        # æ ‡å‡† Q, K æŠ•å½±
        self.q = Linear(state_size, projection_size * n_heads)
        self.k = Linear(state_size, projection_size * n_heads)

        # å…³é”®ï¼šV å’Œ O éƒ½æ˜¯å¤šä¸“å®¶ï¼
        # å½¢çŠ¶ï¼š[n_heads * n_experts, ...]
        self.v = Parameter([n_heads * n_experts, state_size, projection_size])
        self.o = Parameter([n_heads * n_experts, projection_size, state_size])

        # æ¯ä¸ªå¤´ç‹¬ç«‹é€‰æ‹©ä¸“å®¶
        self.sel_v = Parameter([n_heads * n_experts, state_size])
        self.sel_o = Parameter([n_heads * n_experts, state_size])
```

**åˆ›æ–°**ï¼šæ¯ä¸ªæ³¨æ„åŠ›å¤´æœ‰ `n_experts` ä¸ªä¸“å®¶ç‰ˆæœ¬ï¼

##### **å‰å‘ä¼ æ’­**ï¼š

```python
def forward(self, q_src, k_src, v_src, mask):
    # 1. æ ‡å‡† Q, K æŠ•å½±
    q = self.q(q_src)  # [batch, seq, n_heads * proj_size]
    k = self.k(k_src)

    # 2. ä¸ºæ¯ä¸ªå¤´çš„ V é€‰æ‹©ä¸“å®¶
    v_sel = self.get_sel(k_src, self.sel_v)  # Sigmoid + TopK
    v = cvmm(v_src, v_sel, self.v)            # åªè®¡ç®—é€‰ä¸­çš„ä¸“å®¶
    v = v.transpose(-2, -3)                   # è½¬ä¸º [batch, n_heads, seq, proj]

    # 3. æ ‡å‡†æ³¨æ„åŠ›è®¡ç®—
    q = self.project_to_torch_order(q)
    k = self.project_to_torch_order(k)
    attention_out = self.attend(pos_offset, v, k, q, mask)

    # 4. ä¸ºæ¯ä¸ªå¤´çš„ O é€‰æ‹©ä¸“å®¶
    o_sel = self.get_sel(q_src, self.sel_o)
    output = cvmm(attention_out, o_sel, self.o)

    return output
```

**åŒé‡ä¸“å®¶é€‰æ‹©**ï¼š
- V æŠ•å½±ï¼šæ ¹æ® **Key çš„å†…å®¹**é€‰æ‹©ä¸“å®¶ï¼ˆ"éœ€è¦æå–ä»€ä¹ˆä¿¡æ¯"ï¼‰
- O æŠ•å½±ï¼šæ ¹æ® **Query çš„å†…å®¹**é€‰æ‹©ä¸“å®¶ï¼ˆ"å¦‚ä½•ä½¿ç”¨è¿™äº›ä¿¡æ¯"ï¼‰

---

#### **4. cvmm - æ¡ä»¶å‘é‡çŸ©é˜µä¹˜æ³•ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰**

**æ–‡ä»¶**ï¼š`cvmm.py`

##### **é—®é¢˜**ï¼šæ ‡å‡† MoE çš„ä½æ•ˆ

```python
# æœ´ç´ æ–¹æ³•ï¼šè®¡ç®—æ‰€æœ‰ä¸“å®¶ï¼Œå†é€‰æ‹©
all_outputs = []
for i in range(128):  # 128ä¸ªä¸“å®¶
    all_outputs.append(expert_i(x))  # å…¨éƒ¨è®¡ç®—ï¼
output = sum(all_outputs[selected_indices])  # åªç”¨å…¶ä¸­8ä¸ª

# æµªè´¹ï¼šè®¡ç®—äº† 120 ä¸ªä¸éœ€è¦çš„ä¸“å®¶ï¼ˆ93.75%æµªè´¹ï¼‰
```

##### **cvmm è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# æ™ºèƒ½æ–¹æ³•ï¼šåªè®¡ç®—é€‰ä¸­çš„ä¸“å®¶
def cvmm(x, sel_indices, expert_weights):
    """
    x: [batch*seq, dmodel]
    sel_indices: [batch*seq, k] - æ¯ä¸ªtokené€‰ä¸­çš„kä¸ªä¸“å®¶ç´¢å¼•
    expert_weights: [n_experts, dmodel, expert_size]

    è¿”å›: [batch*seq, k, expert_size] - åªåŒ…å«é€‰ä¸­ä¸“å®¶çš„è¾“å‡º
    """
    # ä½¿ç”¨ Triton JIT ç¼–è¯‘çš„é«˜æ•ˆ GPU kernel
    return cvmm_triton_call(x, sel_indices, expert_weights)
```

##### **Triton Kernel å®ç°**ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š

```python
@triton.jit
def cvmm_kernel(a_ptr, b_ptr, c_ptr, index_ptr, sel_ptr, ...):
    """
    å¯¹æ¯ä¸ª tokenï¼ŒåªåŠ è½½å’Œè®¡ç®—å…¶é€‰ä¸­çš„ä¸“å®¶
    """
    for matrix_id in range(sel_first, sel_last + 1):
        # åªå¤„ç†è¢«è¿™ä¸ª batch é€‰ä¸­çš„ä¸“å®¶
        if current_token_selected(matrix_id):
            # æ‰§è¡ŒçŸ©é˜µä¹˜æ³•ï¼šx @ expert_weights[matrix_id]
            accumulator += dot(a_block, b_block[matrix_id])

    # å†™å›ç»“æœ
    store(c_ptr, accumulator)
```

**æ€§èƒ½ä¼˜åŠ¿**ï¼š
- ä¼ ç»Ÿæ–¹æ³•ï¼š`O(n_experts * batch * seq * dmodel * expert_size)`
- cvmm æ–¹æ³•ï¼š`O(k * batch * seq * dmodel * expert_size)`
- åŠ é€Ÿæ¯”ï¼š`n_experts / k = 128 / 8 = 16å€`ï¼

**åå‘ä¼ æ’­ä¼˜åŒ–**ï¼š
```python
# cvmm.py:434-470
def cvmm_triton_backward(...):
    # ä½¿ç”¨äºŒåˆ†æœç´¢å¿«é€Ÿå®šä½æ¯ä¸ªä¸“å®¶è¢«å“ªäº›tokené€‰ä¸­
    # åªè®¡ç®—è¿™äº›ä½ç½®çš„æ¢¯åº¦
    # é¿å…éå†æ‰€æœ‰token
```

---

#### **5. ä¸“å®¶é€‰æ‹©æœºåˆ¶ - Sigmoid + TopK**

**æ–‡ä»¶**ï¼š`moeut.py:91-107, 196-210`

##### **ä¼ ç»Ÿ MoE (Softmax è·¯ç”±)**ï¼š

```python
logits = linear(x, router_weights)      # [batch, seq, n_experts]
probs = softmax(logits, dim=-1)         # å¼ºåˆ¶å½’ä¸€åŒ–ï¼šsum=1
selected = topk(probs, k)               # ç«äº‰é€‰æ‹©
```

**é—®é¢˜**ï¼šä¸“å®¶ä¹‹é—´æ˜¯"é›¶å’Œåšå¼ˆ"ï¼Œä¸€ä¸ªä¸“å®¶å¾—åˆ†é«˜å¿…ç„¶å¯¼è‡´å…¶ä»–ä¸“å®¶å¾—åˆ†ä½

##### **MoEUT åˆ›æ–° (Sigmoid è·¯ç”±)**ï¼š

```python
scores = linear(x, expert_sel)          # [batch, seq, n_experts]
probs = sigmoid(scores)                 # ç‹¬ç«‹è¯„åˆ†ï¼šæ¯ä¸ª âˆˆ [0,1]
selected = topk(probs, k)               # é€‰æœ€é«˜çš„kä¸ª
```

**ä¼˜åŠ¿**ï¼š
- **ç‹¬ç«‹è¯„åˆ†**ï¼šæ¯ä¸ªä¸“å®¶ç‹¬ç«‹åˆ¤æ–­"æˆ‘èƒ½å¤„ç†è¿™ä¸ªè¾“å…¥å—"
- **éç«äº‰**ï¼šå¤šä¸ªä¸“å®¶å¯ä»¥åŒæ—¶å¾—é«˜åˆ†
- **é€‚åˆè¿­ä»£**ï¼šåŒä¸€ä¸“å®¶å¯ä»¥åœ¨ä¸åŒå±‚è¢«éœ€è¦ï¼ˆå¦‚é™¤æ³•ä¸“å®¶å¯ä»¥è¢«è°ƒç”¨å¤šæ¬¡ï¼‰

##### **ä»£ç å®ç°ç»†èŠ‚**ï¼š

```python
def get_sel(self, t: torch.Tensor, w: torch.Tensor):
    # 1. è®¡ç®—åŸå§‹åˆ†æ•°
    sel_raw = F.linear(t, w)                    # [batch, seq, n_heads, n_experts]
    sel = sel_raw.sigmoid()                     # Sigmoid æ¿€æ´»

    # 2. Expert Dropoutï¼ˆè®­ç»ƒæ—¶éšæœºå±è”½ä¸“å®¶ï¼‰
    if self.training and self.expert_dropout > 0:
        mask = torch.rand_like(sel) < self.expert_dropout
        sel = sel.masked_fill(mask, float('-inf'))

    # 3. Top-K é€‰æ‹©
    sel_val, sel_index = sel.topk(self.moe_k, dim=-1, sorted=False)

    # 4. å‡†å¤‡ cvmm ç´¢å¼•
    return cvmm_prepare_sel2(sel_index, sel_val), sel_raw
```

**å‚æ•°åˆå§‹åŒ–**ï¼š
```python
# moeut.py:167-169
def init_sel(self, w, std_scale):
    torch.nn.init.normal_(w, 0, std_scale / sqrt(input_size))
    self.renorm_rows(w)  # å½’ä¸€åŒ–æ¯è¡Œï¼Œä¿æŒæ ‡å‡†å·®
```

---

#### **6. ç†µæ­£åˆ™åŒ– - è´Ÿè½½å‡è¡¡**

**æ–‡ä»¶**ï¼š`moeut.py:35-39, 330-338`

##### **é—®é¢˜**ï¼šä¸“å®¶åç¼©

```
æ²¡æœ‰æ­£åˆ™åŒ–æ—¶å¯èƒ½å‘ç”Ÿï¼š
- æ‰€æœ‰ token éƒ½åªé€‰æ‹©ä¸“å®¶ #3 å’Œä¸“å®¶ #7
- å…¶ä»– 126 ä¸ªä¸“å®¶ä»ä¸è¢«ä½¿ç”¨
- å‚æ•°åˆ©ç”¨ç‡æä½
```

##### **è§£å†³æ–¹æ¡ˆ**ï¼šé¼“åŠ±é€‰æ‹©åˆ†å¸ƒçš„ç†µæœ€å¤§åŒ–

```python
def entropy_reg(sel, dim, mask):
    """
    sel: [batch, seq, n_layers, n_experts] - æ‰€æœ‰å±‚çš„é€‰æ‹©logits
    dim: èšåˆçš„ç»´åº¦ï¼ˆè·¨å±‚æˆ–è·¨æ—¶é—´ï¼‰
    """
    # 1. å¯¹æ¯ä¸ªä¸“å®¶è®¡ç®—è·¨å±‚/è·¨tokençš„å¹³å‡é€‰æ‹©æ¦‚ç‡
    sel = F.log_softmax(sel, dim=-1)           # å½’ä¸€åŒ–
    sel = log_mean(sel, dim, mask)             # å¯¹æ•°åŸŸå¹³å‡

    # 2. è®¡ç®—ç†µï¼šH = -Î£ p*log(p)
    entropy = - (sel * sel.exp()).sum(-1)

    # 3. è¿”å›è´Ÿç†µï¼ˆæœ€å°åŒ– = æœ€å¤§åŒ–ç†µï¼‰
    return - entropy.mean()

# è®­ç»ƒæ—¶æ·»åŠ åˆ°æŸå¤±
class MoEUT:
    def collect_losses(self):
        loss = 0
        for layer in self.modules():
            if isinstance(layer, SigmaMoE):
                # FFN çš„ç†µæ­£åˆ™åŒ–ï¼ˆæƒé‡ 0.01ï¼‰
                loss += 0.01 * layer.get_reg_loss()
            elif isinstance(layer, SwitchHeadCore):
                # æ³¨æ„åŠ›çš„ç†µæ­£åˆ™åŒ–ï¼ˆæƒé‡ 0.001ï¼‰
                loss += 0.001 * layer.get_reg_loss()
        return loss
```

**ä½œç”¨**ï¼š
- é¼“åŠ±ä¸åŒ token ä½¿ç”¨ä¸åŒä¸“å®¶
- é¼“åŠ±ä¸åŒå±‚æ¿€æ´»ä¸åŒä¸“å®¶
- é˜²æ­¢ä¸“å®¶"åç¼©"åˆ°å°‘æ•°å‡ ä¸ª

---

#### **7. MoEUTLayer - å®Œæ•´çš„å±‚ç»“æ„**

**æ–‡ä»¶**ï¼š`moeut.py:297-319`

```python
class MoEUTLayer(torch.nn.Module):
    def __init__(self, d_model, n_heads, ff_expert_size, ff_n_experts,
                 att_n_experts, att_k=2, ff_k=8):
        super().__init__()

        # 1. MoE æ³¨æ„åŠ›ï¼ˆé»˜è®¤ï¼š8ä¸ªå¤´ï¼Œæ¯å¤´8ä¸ªä¸“å®¶ï¼Œé€‰top-2ï¼‰
        self.attention = SwitchHeadRope(
            d_model, n_heads, att_n_experts,
            moe_k=att_k
        )

        # 2. MoE FFNï¼ˆé»˜è®¤ï¼š128ä¸ªä¸“å®¶ï¼Œæ¯ä¸ª128ç»´ï¼Œé€‰top-8ï¼‰
        self.ffn = SigmaMoE(
            d_model, ff_n_experts, ff_expert_size,
            k=ff_k
        )

        # 3. LayerNormï¼ˆPre-LNï¼‰
        self.ln1 = LayerNorm(d_model)
        self.ln2 = LayerNorm(d_model)
        self.drop = Dropout(dropout)

    def forward(self, x, mask, kv_cache, strength=1):
        # Pre-LN Transformer ç»“æ„
        xnorm = self.ln1(x)
        att, cache = self.attention(xnorm, xnorm, x, mask, kv_cache)
        x = x + strength * self.drop(att)  # æ®‹å·®è¿æ¥

        upd = self.ffn(x, self.ln2(x))
        return x + strength * self.drop(upd), cache
```

**strength å‚æ•°**ï¼š
- ç”¨äºæ§åˆ¶æ¯å±‚çš„è´¡çŒ®å¼ºåº¦
- åœ¨ Universal Transformer ä¸­é€šå¸¸è®¾ä¸º 1
- å¯ä»¥ç”¨äºè‡ªé€‚åº”æ·±åº¦æ§åˆ¶

---

#### **8. å®Œæ•´æ¨¡å‹æ„å»º**

**æ–‡ä»¶**ï¼š`transformer_templates.py:30-66`

```python
class MoEUTTask:
    def create_layer(self):
        # åˆ›å»ºå•ä¸ª MoEUTLayer
        return MoEUTLayer(
            d_model=512,
            n_heads=8,
            ff_expert_size=128,      # æ¯ä¸ªä¸“å®¶çš„éšè—å±‚å¤§å°
            ff_n_experts=128,        # FFN ä¸“å®¶æ€»æ•°
            att_n_experts=8,         # æ³¨æ„åŠ›ä¸“å®¶æ€»æ•°
            att_k=2,                 # æ³¨æ„åŠ›é€‰ top-2
            ff_k=8                   # FFN é€‰ top-8
        )

    def create_inner_model(self):
        # åˆ›å»º MoEUTï¼ˆå‚æ•°å…±äº«ï¼‰
        return MoEUT(
            self.create_layer,
            d_model=512,
            n_layers=16,             # æ€»å±‚æ•°
            group_size=2             # å…±äº«ç»„å¤§å°ï¼š2å±‚å…±äº«å‚æ•°
        )

    def create_model(self):
        # å®Œæ•´è¯­è¨€æ¨¡å‹
        return LanguageModel(
            self.create_inner_model(),
            n_tokens=8000,           # è¯è¡¨å¤§å°
            d_model=512,
            n_layers=16,
            tied=False               # embedding ä¸å…±äº«
        )
```

**å‚æ•°é‡å¯¹æ¯”**ï¼š
```
æ ‡å‡† Transformer (16å±‚)ï¼š
- Attention: 16 Ã— 4 Ã— dÂ²
- FFN: 16 Ã— 8 Ã— dÂ²
- æ€»è®¡: â‰ˆ 192 Ã— dÂ²

MoEUT (16å±‚, group_size=2)ï¼š
- Attention: 2 Ã— 4 Ã— dÂ² (å…±äº«) + 2 Ã— n_heads Ã— n_att_experts Ã— ... (ä¸“å®¶)
- FFN: 2 Ã— n_ff_experts Ã— expert_size Ã— d (ä¸“å®¶)
- æ€»è®¡: â‰ˆ 24 Ã— dÂ² + ä¸“å®¶å‚æ•°

è™½ç„¶æ€»å‚æ•°å¯èƒ½æ›´å¤šï¼Œä½†æ¿€æ´»å‚æ•°å°‘å¾ˆå¤šï¼
```

---

#### **9. å®ç°çš„åˆ›æ–°ç‚¹æ€»ç»“**

##### **ğŸ”¥ æ ¸å¿ƒåˆ›æ–°ï¼ˆæŒ‰é‡è¦æ€§ï¼‰**

**1. cvmm - ç¨€ç–ä¸“å®¶è®¡ç®—** â­â­â­â­â­
- Triton JIT ç¼–è¯‘çš„é«˜æ•ˆ GPU kernel
- åªè®¡ç®—é€‰ä¸­çš„ä¸“å®¶ï¼šèŠ‚çœ 10-16å€ è®¡ç®—
- è‡ªå®šä¹‰åå‘ä¼ æ’­ï¼šæ¢¯åº¦è®¡ç®—ä¹Ÿæ˜¯ç¨€ç–çš„
- **è¿™æ˜¯ MoEUT é«˜æ•ˆè¿è¡Œçš„æ ¹æœ¬åŸå› **

**2. åŒé‡ MoE - æ³¨æ„åŠ›ä¹Ÿç”¨ä¸“å®¶** â­â­â­â­
- ä¼ ç»Ÿ MoEï¼šåªåœ¨ FFN å±‚ä½¿ç”¨ä¸“å®¶
- MoEUTï¼šæ³¨æ„åŠ›çš„ V å’Œ O æŠ•å½±ä¹Ÿç”¨ä¸“å®¶
- æ¯ä¸ªå¤´ç‹¬ç«‹é€‰æ‹©ä¸“å®¶
- æ›´ç»†ç²’åº¦çš„ä¸“ä¸šåŒ–

**3. Sigmoid + TopK é€‰æ‹©** â­â­â­â­
- ä¸ç”¨ Softmax å½’ä¸€åŒ–ï¼ˆéé›¶å’Œåšå¼ˆï¼‰
- ä¸“å®¶å¯ä»¥"ç‹¬ç«‹"è¢«æ¿€æ´»
- é€‚åˆè¿­ä»£æ¨ç†åœºæ™¯ï¼ˆåŒä¸€ä¸“å®¶å¯è¢«å¤šæ¬¡è°ƒç”¨ï¼‰
- é…åˆ Expert Dropout æé«˜é²æ£’æ€§

**4. å‚æ•°å…±äº« + è¿­ä»£** â­â­â­
- Universal Transformer æ¶æ„
- ç‰©ç†å±‚å°‘ï¼Œé€»è¾‘å±‚å¤š
- åŒä¸€ç»„ä¸“å®¶é‡å¤ä½¿ç”¨
- å®ç°"æ½œç©ºé—´ CoT"

**5. ç†µæ­£åˆ™åŒ–** â­â­â­
- è·¨å±‚ã€è·¨æ—¶é—´æ­¥çš„è´Ÿè½½å‡è¡¡
- é˜²æ­¢ä¸“å®¶åç¼©
- FFN å’Œæ³¨æ„åŠ›åˆ†åˆ«è®¾ç½®ä¸åŒæƒé‡
- åœ¨å¯¹æ•°åŸŸè®¡ç®—ï¼Œæ•°å€¼ç¨³å®š

**6. é«˜æ•ˆç´¢å¼•é¢„å¤„ç†** â­â­
```python
# cvmm_prepare_sel2: å°†é€‰æ‹©ç´¢å¼•è½¬æ¢ä¸º GPU å‹å¥½æ ¼å¼
# æ’åºã€è®¡ç®—åç§»ã€å‡†å¤‡æƒé‡
# ä¸€æ¬¡é¢„å¤„ç†ï¼Œå¤šæ¬¡ä½¿ç”¨
```

---

#### **10. ä¸æ ‡å‡† Transformer çš„ä»£ç å¯¹æ¯”**

**æ ‡å‡† Transformer FFN**ï¼š
```python
class TransformerFFN:
    def __init__(self, d_model, d_ff):
        self.up = Linear(d_model, d_ff)      # å•ä¸€æŠ•å½±
        self.down = Linear(d_ff, d_model)

    def forward(self, x):
        return self.down(F.gelu(self.up(x)))  # æ‰€æœ‰tokenèµ°ç›¸åŒè·¯å¾„
```

**MoEUT FFN**ï¼š
```python
class SigmaMoE:
    def __init__(self, d_model, n_experts, expert_size, k):
        self.keys = Parameter([n_experts, d_model, expert_size])     # 128ä¸ªä¸“å®¶
        self.values = Parameter([n_experts, expert_size, d_model])
        self.expert_sel = Parameter([n_experts, d_model])

    def forward(self, x):
        sel = sigmoid(linear(x, self.expert_sel))  # æ¯ä¸ªtokené€‰æ‹©ä¸“å®¶
        sel_idx = topk(sel, k)                     # åªé€‰8ä¸ª
        out = cvmm(x, sel_idx, self.keys)          # åªè®¡ç®—8ä¸ª
        return cvmm(out, sel_idx, self.values)     # ç¨€ç–è®¡ç®—
```

**å·®å¼‚**ï¼š
- æ ‡å‡†ï¼šå›ºå®šè·¯å¾„ï¼ŒO(d*d_ff) è®¡ç®—
- MoEUTï¼šåŠ¨æ€è·¯ç”±ï¼ŒO(k*d*expert_size) è®¡ç®—ï¼ˆk << n_expertsï¼‰

---

### 3.8 MoEUT è®­ç»ƒè¯¦è§£

> **ä»£ç ä½ç½®**ï¼š
> - è®­ç»ƒæ¡†æ¶ï¼š`framework/task/simple_task.py`
> - ä»»åŠ¡å®šä¹‰ï¼š`tasks/dm_math.py`
> - æŸå¤±è®¡ç®—ï¼š`tasks/helpers/lm_mixin.py`
> - é…ç½®æ–‡ä»¶ï¼š`sweeps/dm_math_moeut_big_lossonall_vs_noloss.yaml`

---

#### **1. è®­ç»ƒæŸå¤±å‡½æ•°çš„ç‰¹æ®Šè®¾è®¡**

##### **åŸºç¡€æŸå¤±ï¼šæ ‡å‡†äº¤å‰ç†µ**

**ä»£ç **ï¼š`tasks/helpers/lm_mixin.py:21-28`

```python
def loss(self, logits, data):
    target = data["data"].long()

    # å¯é€‰ï¼šåªåœ¨ç­”æ¡ˆéƒ¨åˆ†è®¡ç®—æŸå¤±
    if self.loss_mask_name in data:
        target = target.masked_fill(~data[self.loss_mask_name].bool(),
                                     self.ignore_index)

    # é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
    target = target.narrow(self.time_dim, 1, data["data"].shape[self.time_dim] - 1)
    return F.cross_entropy(logits.flatten(end_dim=-2),
                          target.flatten(),
                          ignore_index=self.ignore_index)
```

**ä¸¤ç§è®­ç»ƒæ¨¡å¼**ï¼ˆè®ºæ–‡é‡è¦å˜é‡ï¼‰ï¼š

**é‡è¦æ¾„æ¸…**ï¼šè¿™**ä¸æ˜¯**"æ¯å±‚æŸå¤± vs æœ€ç»ˆæŸå¤±"ï¼Œè€Œæ˜¯**åœ¨åºåˆ—çš„å“ªéƒ¨åˆ†è®¡ç®—æŸå¤±**

##### **æ•°æ®æ ¼å¼ç¤ºä¾‹**ï¼š

```python
è¾“å…¥åºåˆ—: "Question: What is 14/(-6) divided by 1162/(-4980)? Answer: 5.8"
           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ é—®é¢˜éƒ¨åˆ† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”œâ”€â”€â”€â”€â”€ ç­”æ¡ˆéƒ¨åˆ† â”€â”€â”€â”€â”€â”¤
```

##### **Mode 1: `loss_on_target_only = False` (æ•´ä¸ªåºåˆ—)**

```python
è®¡ç®—æŸå¤±çš„ä½ç½®ï¼šæ•´ä¸ªåºåˆ—éƒ½ç®—
"Question: What is 14/(-6)..." â†’ è®¡ç®—æŸå¤± âœ…
"Answer: 5.8"                  â†’ è®¡ç®—æŸå¤± âœ…

ä½œç”¨ï¼š
- æ¨¡å‹éœ€è¦å­¦ä¼šï¼šç†è§£é—®é¢˜ + ç”Ÿæˆç­”æ¡ˆ
- ç±»ä¼¼äº"è¾¹çœ‹é¢˜è¾¹æ€è€ƒè¾¹ç­”é¢˜"
```

##### **Mode 2: `loss_on_target_only = True` (åªåœ¨ç­”æ¡ˆ)**

```python
è®¡ç®—æŸå¤±çš„ä½ç½®ï¼šåªåœ¨ç­”æ¡ˆéƒ¨åˆ†
"Question: What is 14/(-6)..." â†’ å¿½ç•¥ï¼ˆmaskæ‰ï¼‰âŒ
"Answer: 5.8"                  â†’ è®¡ç®—æŸå¤± âœ…

ä½œç”¨ï¼š
- æ¨¡å‹åªéœ€è¦ï¼šç”Ÿæˆæ­£ç¡®ç­”æ¡ˆï¼ˆä¸ç®¡æ˜¯å¦ç†è§£é—®é¢˜ï¼‰
- ç±»ä¼¼äº"å·²ç»çœ‹å®Œé¢˜äº†ï¼Œåªè¯„åˆ¤ç­”æ¡ˆå¯¹ä¸å¯¹"
```

##### **ä»£ç å®ç°**ï¼š

```python
# dm_math.py:14-17
class DMMathMixin:
    def __init__(self, helper):
        if self.helper.args.dm_math.loss_on_target_only:
            self.loss_mask_name = "eval_mask"  # è®¾ç½®mask

# lm_mixin.py:21-28
def loss(self, logits, data):
    target = data["data"].long()

    # å¦‚æœå¯ç”¨ loss_on_target_only
    if self.loss_mask_name in data:
        # æŠŠé—®é¢˜éƒ¨åˆ†çš„targetè®¾ä¸ºignore_indexï¼Œä¸è®¡ç®—æŸå¤±
        target = target.masked_fill(
            ~data[self.loss_mask_name].bool(),  # eval_mask=Falseçš„ä½ç½®
            self.ignore_index  # è®¾ä¸º-100ï¼Œcross_entropyä¼šå¿½ç•¥
        )

    return F.cross_entropy(logits, target, ignore_index=self.ignore_index)
```

##### **ä¸ºä»€ä¹ˆè¿™ä¸ªå¾ˆé‡è¦ï¼Ÿè®ºæ–‡çš„å…³é”®å‘ç°**ï¼š

```
è®­ç»ƒæ¨¡å¼         æ ‡å‡†Transformer    MoEUT    æå‡
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Q+A éƒ½ç®—æŸå¤±          41%           36%     -5%
åªåœ¨ A ç®—æŸå¤±         48%           63%    +15% ğŸ”¥
```

**Answer-only æ¨¡å¼æ­ç¤ºäº†ä»€ä¹ˆ**ï¼š
- æ ‡å‡†Transformerï¼š48%ï¼ˆä»…æå‡7%ï¼‰
- MoEUTï¼š63%ï¼ˆæå‡27%ï¼‰

**æ·±å±‚å«ä¹‰**ï¼š
- Answer-only æ¨¡å¼ä¸‹ï¼Œæ¨¡å‹å¿…é¡»åœ¨**çœ‹å®Œé—®é¢˜åï¼Œåœ¨å†…éƒ¨è¿›è¡Œæ·±å±‚æ¨ç†**
- æ ‡å‡†Transformerï¼šååŠéƒ¨åˆ†å±‚é—²ç½®ï¼Œæ·±å±‚æ¨ç†èƒ½åŠ›æœ‰é™
- MoEUTï¼šé€šè¿‡**è¿­ä»£ä½¿ç”¨ä¸“å®¶**ï¼ŒçœŸæ­£å®ç°äº†"æ½œç©ºé—´æ¨ç†"

**ç±»æ¯”**ï¼š
```
Q+A æ¨¡å¼ = è¾¹è¯»é¢˜è¾¹åœ¨çº¸ä¸Šå†™è‰ç¨¿ï¼ˆå¤–åŒ–æ¨ç†è¿‡ç¨‹ï¼‰
         â†’ æ ‡å‡†Transformerä¹Ÿèƒ½åšï¼Œå› ä¸ºå¯ä»¥åœ¨ç”Ÿæˆé—®é¢˜éƒ¨åˆ†æ—¶"å†™å‡º"æ€è€ƒ

A-only æ¨¡å¼ = è¯»å®Œé¢˜ååœ¨è„‘ä¸­è®¡ç®—ï¼ˆå†…åŒ–æ¨ç†è¿‡ç¨‹ï¼‰
           â†’ éœ€è¦çœŸæ­£çš„æ·±å±‚æ¨ç†èƒ½åŠ›

MoEUT åœ¨ A-only æ¨¡å¼ä¸‹çš„ä¼˜åŠ¿ï¼š
â†’ è¯æ˜å®ƒçœŸçš„å­¦ä¼šäº†"åœ¨è„‘ä¸­è¿­ä»£è®¡ç®—"
â†’ è€Œä¸æ˜¯ä¾èµ–"åœ¨çº¸ä¸Šå†™æ­¥éª¤"
â†’ è¿™å°±æ˜¯"æ½œç©ºé—´CoT"çš„éªŒè¯ï¼
```

---

##### **æ ¸å¿ƒåˆ›æ–°ï¼šç†µæ­£åˆ™åŒ–æŸå¤±**

**ä»£ç **ï¼š`simple_task.py:354-356 + moeut.py:330-344`

```python
# è®­ç»ƒæ­¥éª¤ä¸­
def train_step(self):
    # åŸºç¡€æŸå¤±
    base_loss = cross_entropy(logits, targets)

    # æ­£åˆ™åŒ–æŸå¤±ï¼ˆMoEUTç‰¹æœ‰ï¼‰
    reg_loss = self.get_regularizers()

    # æ€»æŸå¤±
    total_loss = base_loss + reg_loss * self.args.reg

# MoEUT çš„æ­£åˆ™åŒ–æ”¶é›†
class MoEUT:
    def collect_losses(self, device):
        reg_loss = torch.zeros(1, device=device)

        for layer in self.modules():
            # FFN çš„ç†µæ­£åˆ™åŒ–ï¼ˆæƒé‡ï¼š0.01ï¼‰
            if isinstance(layer, SigmaMoE):
                reg_loss += 0.01 * layer.get_reg_loss()

            # æ³¨æ„åŠ›çš„ç†µæ­£åˆ™åŒ–ï¼ˆæƒé‡ï¼š0.001ï¼‰
            elif isinstance(layer, SwitchHeadCore):
                reg_loss += 0.001 * layer.get_reg_loss()

        return reg_loss
```

**ç†µæ­£åˆ™åŒ–çš„è®¡ç®—**ï¼š

```python
def entropy_reg(sel, dim, mask):
    """
    sel: [batch, seq, n_layers, n_experts] - æ‰€æœ‰å±‚çš„ä¸“å®¶é€‰æ‹©logits
    """
    # 1. å½’ä¸€åŒ–ï¼šè®¡ç®—æ¯ä¸ªä¸“å®¶çš„æ¦‚ç‡
    sel = F.log_softmax(sel, dim=-1)

    # 2. è·¨å±‚/è·¨æ—¶é—´å¹³å‡
    sel = log_mean(sel, dim, mask)  # å¯¹æ•°åŸŸå¹³å‡

    # 3. è®¡ç®—ç†µï¼šH = -Î£ p*log(p)
    entropy = - (sel * sel.exp()).sum(-1)

    # 4. è¿”å›è´Ÿç†µï¼ˆæœ€å°åŒ–è´Ÿç†µ = æœ€å¤§åŒ–ç†µï¼‰
    return - entropy.mean()
```

**ä½œç”¨ä¸æƒé‡**ï¼š
```
æ€»æŸå¤± = cross_entropy
       + ç†µæ­£åˆ™åŒ–æŸå¤±

å…¶ä¸­ï¼š
ç†µæ­£åˆ™åŒ–æŸå¤± = 0.01 Ã— FFN_ç†µæ­£åˆ™ + 0.001 Ã— æ³¨æ„åŠ›_ç†µæ­£åˆ™

ä½œç”¨ï¼š
âœ… é¼“åŠ±ä¸åŒ token ä½¿ç”¨ä¸åŒä¸“å®¶
âœ… é¼“åŠ±ä¸åŒå±‚æ¿€æ´»ä¸åŒä¸“å®¶
âœ… é˜²æ­¢æ‰€æœ‰æ ·æœ¬éƒ½åªä½¿ç”¨å°‘æ•°å‡ ä¸ªä¸“å®¶ï¼ˆä¸“å®¶åç¼©ï¼‰
```

##### **ç†µæ­£åˆ™åŒ–çš„å½¢è±¡è§£é‡Š**

**é—®é¢˜åœºæ™¯**ï¼šä¸“å®¶åç¼©ï¼ˆExpert Collapseï¼‰

æƒ³è±¡ä¸€ä¸ªæœ‰128ä¸ªæŠ€æœ¯ä¸“å®¶çš„å’¨è¯¢å…¬å¸ï¼š

```
ä¸“å®¶å›¢é˜Ÿï¼š
â”œâ”€ ä¸“å®¶#1ï¼šæ•°æ®åº“ä¼˜åŒ–
â”œâ”€ ä¸“å®¶#2ï¼šå‰ç«¯å¼€å‘
â”œâ”€ ä¸“å®¶#3ï¼šç®—æ³•è®¾è®¡  â­ï¼ˆæœ€å—æ¬¢è¿ï¼‰
â”œâ”€ ä¸“å®¶#4ï¼šç³»ç»Ÿæ¶æ„
â”œâ”€ ä¸“å®¶#5ï¼šç½‘ç»œå®‰å…¨
â”œâ”€ ä¸“å®¶#6ï¼šæœºå™¨å­¦ä¹ 
â”œâ”€ ä¸“å®¶#7ï¼šäº‘è®¡ç®—     â­ï¼ˆæœ€å—æ¬¢è¿ï¼‰
â”œâ”€ ...
â””â”€ ä¸“å®¶#128ï¼šç¡¬ä»¶ä¼˜åŒ–
```

**æ²¡æœ‰ç†µæ­£åˆ™åŒ–ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ**

```
å®¢æˆ·é¡¹ç›®1 â†’ é€‰æ‹©ä¸“å®¶#3, #7
å®¢æˆ·é¡¹ç›®2 â†’ é€‰æ‹©ä¸“å®¶#3, #7
å®¢æˆ·é¡¹ç›®3 â†’ é€‰æ‹©ä¸“å®¶#3, #7
...
å®¢æˆ·é¡¹ç›®100 â†’ é€‰æ‹©ä¸“å®¶#3, #7

ç»“æœï¼š
âœ… ä¸“å®¶#3, #7ï¼šè¿‡åº¦åŠ³ç´¯ï¼Œç»éªŒä¸°å¯Œ
âŒ å…¶ä»–126ä¸ªä¸“å®¶ï¼šå®Œå…¨é—²ç½®ï¼ŒæŠ€èƒ½é€€åŒ–
ğŸ’¸ å…¬å¸ç™½å…»äº†126ä¸ªä¸“å®¶ï¼ˆæµªè´¹å‚æ•°ï¼‰
```

##### **ç†µ = ä¸ç¡®å®šæ€§ = å¤šæ ·æ€§**

**ä½ç†µï¼ˆåï¼‰**ï¼š
```
ğŸ² ä½œå¼Šçš„éª°å­ï¼š
ä¸“å®¶#3: 90% æ¦‚ç‡è¢«é€‰  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
ä¸“å®¶#7: 10% æ¦‚ç‡è¢«é€‰  â–ˆâ–ˆ
å…¶ä»–:    0% æ¦‚ç‡è¢«é€‰

ç†µ â‰ˆ 0.47  ï¼ˆå¾ˆç¡®å®šï¼Œæ²¡å¤šæ ·æ€§ï¼‰
â†’ å‡ ä¹æ€»æ˜¯é€‰æ‹©ä¸“å®¶#3
```

**é«˜ç†µï¼ˆå¥½ï¼‰**ï¼š
```
ğŸ² å…¬å¹³çš„éª°å­ï¼š
ä¸“å®¶#1: 16% æ¦‚ç‡è¢«é€‰  â–ˆâ–ˆâ–ˆâ–ˆ
ä¸“å®¶#2: 15% æ¦‚ç‡è¢«é€‰  â–ˆâ–ˆâ–ˆâ–ˆ
ä¸“å®¶#3: 14% æ¦‚ç‡è¢«é€‰  â–ˆâ–ˆâ–ˆ
ä¸“å®¶#4: 13% æ¦‚ç‡è¢«é€‰  â–ˆâ–ˆâ–ˆ
ä¸“å®¶#5: 14% æ¦‚ç‡è¢«é€‰  â–ˆâ–ˆâ–ˆ
ä¸“å®¶#6: 14% æ¦‚ç‡è¢«é€‰  â–ˆâ–ˆâ–ˆ
ä¸“å®¶#7: 14% æ¦‚ç‡è¢«é€‰  â–ˆâ–ˆâ–ˆ

ç†µ â‰ˆ 1.95  ï¼ˆå¾ˆä¸ç¡®å®šï¼Œå¤šæ ·æ€§é«˜ï¼‰
â†’ å„ä¸ªä¸“å®¶éƒ½æœ‰æœºä¼šè¢«é€‰æ‹©
```

##### **ä¸ä¼ ç»ŸMoEæ–¹æ³•çš„å¯¹æ¯”**

**ä¼ ç»Ÿæ–¹æ³•ï¼šLoad Balancing Loss**ï¼ˆå¦‚Switch Transformerï¼‰

```python
# ç›®æ ‡ï¼šè®©æ¯ä¸ªä¸“å®¶å¤„ç†ç›¸åŒæ•°é‡çš„token
ä¸“å®¶#1ï¼šå¤„ç† 100 ä¸ªtoken  â† ç›®æ ‡
ä¸“å®¶#2ï¼šå¤„ç† 100 ä¸ªtoken  â† ç›®æ ‡
ä¸“å®¶#3ï¼šå¤„ç† 500 ä¸ªtoken  â† æƒ©ç½šï¼å¤ªå¤šäº†
ä¸“å®¶#7ï¼šå¤„ç† 400 ä¸ªtoken  â† æƒ©ç½šï¼å¤ªå¤šäº†

loss += importance_loss + load_loss
      = Î£(ä¸“å®¶é‡è¦æ€§) + Î£|å®é™…è´Ÿè½½ - å¹³å‡è´Ÿè½½|
```

**ç±»æ¯”**ï¼šå¼ºåˆ¶æ¯ä¸ªå‘˜å·¥æ¥ç›¸åŒæ•°é‡çš„é¡¹ç›®

**é—®é¢˜**ï¼š
- âŒ å¯èƒ½æŠŠä¸åˆé€‚çš„ä¸“å®¶åˆ†é…ç»™ä»»åŠ¡
- âŒ ç®€å•ç²—æš´åœ°å¹³å‡åˆ†é…
- âŒ éš¾ä»¥é‡å¤ä½¿ç”¨åŒä¸€ä¸“å®¶ï¼ˆè¿­ä»£æ¨ç†éœ€è¦ï¼‰

**MoEUTæ–¹æ³•ï¼šç†µæ­£åˆ™åŒ–**

```python
# ç›®æ ‡ï¼šè®©ä¸“å®¶é€‰æ‹©åˆ†å¸ƒçš„ç†µæœ€å¤§åŒ–
# é¼“åŠ±å¤šæ ·æ€§ï¼Œä½†ä¸å¼ºåˆ¶å¹³å‡

è·¨ç»´åº¦é¼“åŠ±å¤šæ ·æ€§ï¼š
- è·¨æ—¶é—´æ­¥ï¼štoken1â†’ä¸“å®¶#3,7  token2â†’ä¸“å®¶#1,5  token3â†’ä¸“å®¶#2,6
- è·¨å±‚ï¼šLayer1â†’ä¸“å®¶#3,7  Layer2â†’ä¸“å®¶#1,5  Layer3â†’ä¸“å®¶#2,6

å…è®¸è¿­ä»£é‡ç”¨ï¼ˆå…³é”®ï¼ï¼‰ï¼š
æ•°å­¦é¢˜å¤„ç†è¿‡ç¨‹ï¼š
Layer 1-2:  ä¸“å®¶#é™¤æ³•, ä¸“å®¶#æ•°å­—è¯†åˆ«
Layer 3-5:  ä¸“å®¶#é™¤æ³•, ä¸“å®¶#ç¬¦å·å¤„ç†  â† é™¤æ³•ä¸“å®¶å†æ¬¡å‡ºç°ï¼
Layer 6-8:  ä¸“å®¶#é™¤æ³•, ä¸“å®¶#åŒ–ç®€      â† é™¤æ³•ä¸“å®¶ç¬¬ä¸‰æ¬¡ï¼
```

**ç±»æ¯”**ï¼šé¼“åŠ±å‘˜å·¥å¤šæ ·åŒ–æ¥å•ï¼Œä½†å…è®¸ä¸“å®¶å¤šæ¬¡å‚ä¸å¤æ‚é¡¹ç›®

**ä¼˜åŠ¿**ï¼š
- âœ… æ ¹æ®ä»»åŠ¡éœ€è¦çµæ´»é€‰æ‹©ä¸“å®¶
- âœ… è½¯çº¦æŸï¼ˆé¼“åŠ±è€Œéå¼ºåˆ¶ï¼‰
- âœ… **æ”¯æŒè¿­ä»£æ¨ç†**ï¼ˆå¯ä»¥å¤šæ¬¡è°ƒç”¨åŒä¸€ä¸“å®¶ï¼‰

##### **ä¸ºä»€ä¹ˆMoEUTèƒ½å®ç°è¿­ä»£ï¼ŸSigmoidçš„å…³é”®ä½œç”¨**

**Softmaxï¼ˆä¼ ç»Ÿæ–¹æ³•ï¼‰**ï¼š
```python
# é›¶å’Œåšå¼ˆï¼šä¸“å®¶äº’ç›¸ç«äº‰
scores = softmax([ä¸“å®¶#1å¾—åˆ†, ä¸“å®¶#2å¾—åˆ†, ...])
# æ‰€æœ‰æ¦‚ç‡ä¹‹å’Œ = 1
# å¦‚æœä¸“å®¶#é™¤æ³•å¾—åˆ†é«˜ â†’ å…¶ä»–ä¸“å®¶å¾—åˆ†å¿…ç„¶é™ä½

é—®é¢˜ï¼šå¾ˆéš¾åœ¨å¤šå±‚é‡å¤ä½¿ç”¨åŒä¸€ä¸ªä¸“å®¶
```

**Sigmoidï¼ˆMoEUTæ–¹æ³•ï¼‰**ï¼š
```python
# ç‹¬ç«‹è¯„åˆ†ï¼šæ¯ä¸ªä¸“å®¶ç‹¬ç«‹åˆ¤æ–­"æˆ‘èƒ½å¤„ç†è¿™ä¸ªå—"
scores = sigmoid([ä¸“å®¶#1å¾—åˆ†, ä¸“å®¶#2å¾—åˆ†, ...])
# æ¯ä¸ªæ¦‚ç‡ âˆˆ [0,1]ï¼Œäº’ä¸å½±å“
# ä¸“å®¶#é™¤æ³•å¯ä»¥åœ¨å¤šå±‚éƒ½ä¿æŒé«˜åˆ†

ä¼˜åŠ¿ï¼šåŒä¸€ä¸“å®¶å¯ä»¥åœ¨ä¸åŒå±‚è¢«å¤šæ¬¡è°ƒç”¨
```

##### **å®Œæ•´çš„é˜²åç¼©ç­–ç•¥**

```python
# 1. ç†µæ­£åˆ™åŒ–ï¼ˆä¸»è¦æ‰‹æ®µï¼‰
loss += 0.01 * FFN_ç†µæ­£åˆ™åŒ–
loss += 0.001 * æ³¨æ„åŠ›_ç†µæ­£åˆ™åŒ–

# 2. Expert Dropoutï¼ˆè¾…åŠ©æ‰‹æ®µï¼‰
if training and expert_dropout > 0:
    # éšæœºå±è”½éƒ¨åˆ†ä¸“å®¶
    mask = torch.rand_like(sel) < expert_dropout
    sel = sel.masked_fill(mask, float('-inf'))

# ç»„åˆæ•ˆæœï¼š
# - ç†µæ­£åˆ™åŒ–ï¼šé¼“åŠ±å‘˜å·¥å¤šæ ·åŒ–æ¥å•
# - Expert Dropoutï¼šéšæœºè®©çƒ­é—¨å‘˜å·¥è¯·å‡ï¼Œå¼ºåˆ¶å¯ç”¨å…¶ä»–å‘˜å·¥
```

##### **æ€»ç»“å¯¹æ¯”è¡¨**

| ç»´åº¦ | ä¼ ç»ŸLoad Balancing | MoEUTç†µæ­£åˆ™åŒ– |
|------|-------------------|---------------|
| **ç›®æ ‡** | æ¯ä¸ªä¸“å®¶å¤„ç†**ç›¸åŒæ•°é‡**token | é€‰æ‹©åˆ†å¸ƒ**å¤šæ ·æ€§**æœ€å¤§ |
| **çº¦æŸ** | ç¡¬çº¦æŸï¼ˆå¼ºåˆ¶å¹³å‡ï¼‰ | è½¯çº¦æŸï¼ˆé¼“åŠ±å¤šæ ·ï¼‰ |
| **é€‚åº”æ€§** | å¯èƒ½åˆ†é…ä¸åˆé€‚ä»»åŠ¡ | æ ¹æ®éœ€è¦çµæ´»é€‰æ‹© |
| **è¿­ä»£æ”¯æŒ** | âŒ éš¾ä»¥é‡ç”¨ä¸“å®¶ | âœ… å¯ä»¥å¤šæ¬¡è°ƒç”¨åŒä¸€ä¸“å®¶ |
| **è·¯ç”±æœºåˆ¶** | é€šå¸¸é…åˆSoftmax | é…åˆSigmoidï¼ˆç‹¬ç«‹è¯„åˆ†ï¼‰ |
| **å®ç°** | `Î£\|load_i - avg\|` | `-Î£ p*log(p)` |
| **ç±»æ¯”** | å¼ºåˆ¶æ¯äººæ¥ç›¸åŒé¡¹ç›®æ•° | é¼“åŠ±å¤šæ ·åŒ–ï¼Œä½†å…è®¸ä¸“å®¶å¤šæ¬¡å‚ä¸ |

**MoEUTçš„å…³é”®åˆ›æ–°**ï¼šé€šè¿‡**ç†µæ­£åˆ™åŒ– + Sigmoidè·¯ç”±**ï¼Œæ—¢ä¿è¯äº†ä¸“å®¶å¤šæ ·æ€§ï¼Œåˆå…è®¸è¿­ä»£æ¨ç†ä¸­é‡å¤ä½¿ç”¨å…³é”®ä¸“å®¶ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå®ƒèƒ½å®ç°"æ½œç©ºé—´CoT"çš„æ ¹æœ¬åŸå› ï¼

---

#### **2. ä¼˜åŒ–å™¨é…ç½®**

##### **AdamW + å‚æ•°åˆ†ç»„**

**ä»£ç **ï¼š`simple_task.py:247-264`

```python
def create_optimizer(self):
    # å‚æ•°åˆ†ç»„ï¼šåŒºåˆ†éœ€è¦å’Œä¸éœ€è¦æƒé‡è¡°å‡çš„å‚æ•°
    decay_params = [p for p in params if p.dim() >= 2]      # æƒé‡çŸ©é˜µ
    nodecay_params = [p for p in params if p.dim() < 2]     # bias, layernorm

    param_groups = [
        {'params': decay_params, 'weight_decay': 0.01},
        {'params': nodecay_params, 'weight_decay': 0.0}
    ]

    # åˆ†å¸ƒå¼è®­ç»ƒï¼šä½¿ç”¨ ZeroRedundancyOptimizer
    if self.helper.dist_env.is_distributed:
        self.optimizer = torch.distributed.optim.ZeroRedundancyOptimizer(
            param_groups,
            optimizer_class=torch.optim.AdamW,
            lr=0.00025,
            betas=(0.9, 0.999),
            eps=1e-8
        )
    else:
        self.optimizer = torch.optim.AdamW(param_groups, ...)
```

**å…³é”®å‚æ•°**ï¼ˆ`dm_math_moeut_big_lossonall_vs_noloss.yaml`ï¼‰ï¼š
- å­¦ä¹ ç‡ï¼š`lr = 0.00025`
- æƒé‡è¡°å‡ï¼š`wd = 0.01`
- AdamW betasï¼š`(0.9, 0.999)`
- Epsilonï¼š`1e-8`

**ZeroRedundancyOptimizer**ï¼š
- æ¯ä¸ª GPU åªå­˜å‚¨éƒ¨åˆ†ä¼˜åŒ–å™¨çŠ¶æ€
- èŠ‚çœå†…å­˜ï¼Œæ”¯æŒæ›´å¤§æ¨¡å‹
- è®­ç»ƒæ—¶è‡ªåŠ¨åŒæ­¥

---

##### **æ¢¯åº¦è£å‰ª**

**ä»£ç **ï¼š`simple_task.py:458-462`

```python
if self.helper.args.grad_clip:
    grad_norm = torch.nn.utils.clip_grad_norm_(
        [p for model in self.models.values()
         for p in model.parameters() if p.grad is not None],
        self.helper.args.grad_clip  # 0.25
    )
```

**è®¾ç½®**ï¼š
- MoEUTï¼š`grad_clip = 0.25`
- æ ‡å‡† Transformerï¼š`grad_clip = 0.25`
- ä¸¤è€…ç›¸åŒï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

---

#### **3. å­¦ä¹ ç‡è°ƒåº¦**

##### **Cosine Annealing + Linear Warmup**

**ä»£ç **ï¼š`task.py:65-78, 169-186`

```python
# åˆ›å»ºè°ƒåº¦å™¨
def create_lr_scheduler(self):
    if self.args.lr_sched.type == "cos":
        self.lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=self.args.stop_after,      # 100000
            eta_min=self.args.lr * 0.1       # 0.000025
        )

# æ¯æ­¥æ›´æ–°
def set_lr(self):
    # Warmup é˜¶æ®µï¼ˆå‰4000æ­¥ï¼‰
    if self.state.iter < self.args.lr_warmup:
        lr = self.args.lr / self.args.lr_warmup * (self.state.iter + 1)
        self.set_optimizer_lr(lr)

    # Cosine Annealing é˜¶æ®µ
    else:
        self.lr_scheduler.step(self.state.iter - self.args.lr_warmup)
```

**å­¦ä¹ ç‡æ›²çº¿**ï¼š

```
æ­¥æ•° 0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4000 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 100000
     â”‚              â”‚                              â”‚
     â”‚  Warmup      â”‚   Cosine Annealing          â”‚
     â”‚              â”‚                              â”‚
LR:  0 â”€â”€çº¿æ€§å¢é•¿â†’ 0.00025 â”€â”€ä½™å¼¦è¡°å‡â†’ 0.000025
```

**å‚æ•°**ï¼š
- åŸºç¡€å­¦ä¹ ç‡ï¼š`0.00025`
- Warmup æ­¥æ•°ï¼š`4000`
- æ€»è®­ç»ƒæ­¥æ•°ï¼š`100000`
- æœ€å°å­¦ä¹ ç‡ï¼š`0.00025 Ã— 0.1 = 0.000025`

---

#### **4. æ··åˆç²¾åº¦è®­ç»ƒ**

##### **BFloat16 è‡ªåŠ¨æ··åˆç²¾åº¦**

**ä»£ç **ï¼š`simple_task.py:86-87, 345-346`

```python
# æ£€æµ‹ BF16 æ”¯æŒï¼ˆéœ€è¦ Ampere+ GPUï¼‰
self.bf16_enabled = (
    self.args.bfloat16 and
    torch.cuda.is_available() and
    torch.cuda.get_device_properties(0).major >= 8 and
    torch.cuda.is_bf16_supported()
)

# å‰å‘ä¼ æ’­æ—¶è‡ªåŠ¨è½¬æ¢
with torch.amp.autocast(
    device_type='cuda',
    enabled=self.amp_enabled,
    dtype=torch.bfloat16 if self.bf16_enabled else None
):
    res, plots = self.run_model(data, ubatch)
```

**é…ç½®**ï¼š
- **MoEUT**ï¼š`amp=1, bfloat16=True` â†’ **BFloat16**
- **æ ‡å‡† Transformer**ï¼š`amp=1, bfloat16=False` â†’ FP16

**BFloat16 ä¼˜åŠ¿**ï¼š
- åŠ¨æ€èŒƒå›´æ›´å¤§ï¼ˆä¸ FP32 ç›¸åŒçš„æŒ‡æ•°ä½ï¼‰
- æ•°å€¼ç¨³å®šæ€§æ›´å¥½
- ä¸éœ€è¦ GradScaler
- é¿å…æ¢¯åº¦ä¸‹æº¢/ä¸Šæº¢

---

#### **5. å¾®æ‰¹æ¬¡ç´¯ç§¯ï¼ˆGradient Accumulationï¼‰**

##### **ä¸ºä»€ä¹ˆ MoEUT éœ€è¦å¾®æ‰¹æ¬¡ï¼Ÿ**

**å†…å­˜åˆ†æ**ï¼š
```python
# æ ‡å‡† Transformer (18å±‚, d=1024):
å‚æ•°é‡_per_layer â‰ˆ 4 Ã— dÂ² = 4MB
æ€»å‚æ•° â‰ˆ 18 Ã— 4MB = 72MB

# MoEUT (18å±‚, 387ä¸ªä¸“å®¶):
ä¸“å®¶å‚æ•°_per_layer = 387 Ã— 128 Ã— 1024 â‰ˆ 50MB
æ€»å‚æ•° â‰ˆ 18/2 Ã— 50MB = 450MB  # å‚æ•°å…±äº«å‡åŠ

# åŠ ä¸Šæ¿€æ´»ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€ â†’ éœ€è¦å¾®æ‰¹æ¬¡
```

**å®ç°**ï¼š`simple_task.py:428-441`

```python
def train_step(self):
    # 1. è·å–æ•°æ®å¹¶è‡ªåŠ¨åˆ†å—
    data, d_chunks = self.fetcher.get()
    # d_chunks: [chunk1(32), chunk2(32)] if n_microbatch=2

    total_batch_size = self.get_batch_size(data)  # 64

    optimizer.zero_grad()

    # 2. å¯¹æ¯ä¸ªå¾®æ‰¹æ¬¡
    for ubatch, chunk in enumerate(d_chunks):
        # å‰å‘ä¼ æ’­
        res, plots = self.run_ubatch(chunk, ubatch, total_batch_size)
        # run_ubatch å†…éƒ¨ä¼šè°ƒç”¨ backward()

    # 3. ä¸€æ¬¡æ€§æ›´æ–°ï¼ˆç´¯ç§¯çš„æ¢¯åº¦ï¼‰
    optimizer.step()
```

**run_ubatch è¯¦ç»†æµç¨‹**ï¼š

```python
def run_ubatch(self, data, ubatch, total_batch_size):
    # è®¡ç®—æƒé‡ï¼ˆç”¨äºå¹³å‡ï¼‰
    weight = len(data) / total_batch_size  # 32/64 = 0.5

    # å‰å‘ä¼ æ’­
    with torch.amp.autocast(...):
        res = self.run_model(data)
        base_loss = cross_entropy(res, targets)
        reg_loss = self.get_regularizers()
        total_loss = (base_loss + reg_loss) * loss_scaling

    # åå‘ä¼ æ’­ï¼ˆæ¢¯åº¦ç´¯ç§¯ï¼‰
    (total_loss * weight).backward()

    return res, weight
```

**é…ç½®**ï¼š
- **MoEUT**ï¼š`n_microbatch = 2` ï¼ˆ64 = 2Ã—32ï¼‰
- **æ ‡å‡† Transformer**ï¼š`n_microbatch = 1` ï¼ˆä¸éœ€è¦ï¼‰

---

#### **6. è®­ç»ƒé…ç½®å®Œæ•´å¯¹æ¯”**

| é…ç½®é¡¹ | MoEUT | æ ‡å‡† Transformer | è¯´æ˜ |
|--------|--------|------------------|------|
| **æ¨¡å‹å‚æ•°** | | | |
| éšè—ç»´åº¦ `state_size` | 1024 | 1024 | ç›¸åŒ |
| å±‚æ•° `n_layers` | 18 | 18 | ç›¸åŒ |
| æ³¨æ„åŠ›å¤´ `n_heads` | 4 | 16 | MoEUT æ›´å°‘ |
| å¤´æŠ•å½±ç»´åº¦ | 128 | 64 (1024/16) | MoEUT æ›´å¤§ |
| | | | |
| **MoE ä¸“ç”¨** | | | |
| FFN ä¸“å®¶æ•° | 387 | N/A | MoEUT ç‰¹æœ‰ |
| ä¸“å®¶å¤§å° | 128 | N/A | |
| FFN Top-K | 16 | N/A | æ¯æ¬¡æ¿€æ´» 16/387 |
| æ³¨æ„åŠ›ä¸“å®¶æ•° | 10 | N/A | |
| æ³¨æ„åŠ› Top-K | 2 | N/A | |
| FF Multiplier | N/A | 4.014 | Transformer ç”¨ |
| | | | |
| **å‚æ•°å…±äº«** | | | |
| `group_size` | 2 | N/A | **å…³é”®å·®å¼‚** |
| ç‰©ç†å±‚æ•° | 9 (18/2) | 18 | MoEUT å°‘ä¸€åŠ |
| é€»è¾‘å±‚æ•° | 18 | 18 | éƒ½æ˜¯ 18 å±‚ |
| | | | |
| **è®­ç»ƒè¶…å‚** | | | |
| å­¦ä¹ ç‡ `lr` | 0.00025 | 0.00025 | ç›¸åŒ |
| æƒé‡è¡°å‡ `wd` | 0.01 | 0.01 | ç›¸åŒ |
| æ‰¹æ¬¡å¤§å° `batch_size` | 64 | 64 | ç›¸åŒ |
| æ¢¯åº¦è£å‰ª `grad_clip` | 0.25 | 0.25 | ç›¸åŒ |
| LR è°ƒåº¦ | Cosine | Cosine | ç›¸åŒ |
| Warmup æ­¥æ•° | 4000 | 4000 | ç›¸åŒ |
| æ€»è®­ç»ƒæ­¥æ•° | 100000 | 100000 | ç›¸åŒ |
| æœ€å° LR å€æ•° | 0.1 | 0.1 | ç›¸åŒ |
| | | | |
| **ç‰¹æ®Šé…ç½®** | | | |
| å¾®æ‰¹æ¬¡æ•° `n_microbatch` | **2** | 1 | **MoEUT éœ€è¦** |
| æ··åˆç²¾åº¦ | **BFloat16** | FP16 | **MoEUT æ›´ç¨³å®š** |
| ç†µæ­£åˆ™åŒ– | **âœ… 0.01/0.001** | âŒ | **MoEUT ç‰¹æœ‰** |
| Torch Compile | âŒ | âœ… | Triton å†²çª |
| åºåˆ—é•¿åº¦ `lm.unroll` | 1024 | 1024 | ç›¸åŒ |
| | | | |
| **å®éªŒå˜é‡** | | | |
| `loss_on_target_only` | 0 æˆ– 1 | 0 æˆ– 1 | éƒ½æµ‹è¯•ä¸¤ç§ |
| `dm_math.filter` | "arithmetic_.*" | "arithmetic_.*" | ç›¸åŒæ•°æ®é›† |

**å‚æ•°é‡å¯¹æ¯”**ï¼š
```python
# æ ‡å‡† Transformer
params = 18 Ã— (4 Ã— dÂ² + 4.014 Ã— 2 Ã— dÂ²) â‰ˆ 244M

# MoEUT (group_size=2)
params = 9 Ã— (4 Ã— dÂ² + 387 Ã— 128 Ã— d + ...) â‰ˆ 244M

# å‚æ•°é‡æ¥è¿‘ï¼Œä½† MoEUT ä½¿ç”¨å‚æ•°å…±äº«å’Œç¨€ç–æ¿€æ´»
```

---

#### **7. å®Œæ•´è®­ç»ƒæµç¨‹å¯¹æ¯”**

##### **æ ‡å‡† Transformer è®­ç»ƒæ­¥éª¤**ï¼š

```python
for iter in range(100000):
    # 1. æ•°æ®åŠ è½½
    batch = dataloader.get_batch(batch_size=64)

    # 2. å‰å‘ä¼ æ’­
    with torch.amp.autocast(dtype=torch.float16):
        logits = model(batch)
        loss = cross_entropy(logits, targets)

    # 3. åå‘ä¼ æ’­
    scaler.scale(loss).backward()

    # 4. æ¢¯åº¦å¤„ç†
    scaler.unscale_(optimizer)
    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)

    # 5. å‚æ•°æ›´æ–°
    scaler.step(optimizer)
    scaler.update()
    optimizer.zero_grad()

    # 6. å­¦ä¹ ç‡è°ƒåº¦
    lr_scheduler.step()
```

---

##### **MoEUT è®­ç»ƒæ­¥éª¤**ï¼š

```python
for iter in range(100000):
    # 1. æ•°æ®åŠ è½½å¹¶åˆ†å—
    batch = dataloader.get_batch(batch_size=64)
    chunks = split_to_microbatch(batch, n=2)  # [32, 32]

    optimizer.zero_grad()

    # 2. å¾®æ‰¹æ¬¡å¾ªç¯
    for chunk in chunks:
        with torch.amp.autocast(dtype=torch.bfloat16):
            # 2.1 å‰å‘ä¼ æ’­ï¼ˆå‚æ•°å…±äº« + åŠ¨æ€è·¯ç”±ï¼‰
            logits = model(chunk)
            # model å†…éƒ¨ï¼š
            #   for r in range(9):  # n_repeats
            #       for layer in shared_layers:  # group_size=2
            #           x = layer(x)  # æ¯å±‚åŠ¨æ€é€‰æ‹©ä¸“å®¶

            # 2.2 åŸºç¡€æŸå¤±
            base_loss = cross_entropy(logits, targets)

            # 2.3 ç†µæ­£åˆ™åŒ–æŸå¤±ï¼ˆå…³é”®ï¼ï¼‰
            ffn_entropy = 0
            att_entropy = 0
            for layer in model.modules():
                if isinstance(layer, SigmaMoE):
                    ffn_entropy += layer.get_reg_loss()
                elif isinstance(layer, SwitchHeadCore):
                    att_entropy += layer.get_reg_loss()

            reg_loss = 0.01 * ffn_entropy + 0.001 * att_entropy

            # 2.4 æ€»æŸå¤±ï¼ˆå¸¦æƒé‡ï¼‰
            weight = len(chunk) / 64  # 0.5
            total_loss = (base_loss + reg_loss) * weight

        # 2.5 åå‘ä¼ æ’­ï¼ˆæ¢¯åº¦ç´¯ç§¯ï¼‰
        total_loss.backward()

    # 3. æ¢¯åº¦åŒæ­¥ï¼ˆåˆ†å¸ƒå¼ï¼‰
    if distributed:
        for p in model.parameters():
            if p.grad is not None:
                torch.distributed.all_reduce(p.grad)

    # 4. æ¢¯åº¦è£å‰ª
    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)

    # 5. å‚æ•°æ›´æ–°
    optimizer.step()

    # 6. å­¦ä¹ ç‡è°ƒåº¦
    lr_scheduler.step()

    # 7. æ¸…ç©ºé€‰æ‹©å†å²ï¼ˆä¸ºä¸‹æ¬¡ç†µè®¡ç®—å‡†å¤‡ï¼‰
    for layer in model.modules():
        if isinstance(layer, (SigmaMoE, SwitchHeadCore)):
            layer.sel_hist = []
```

---

#### **8. è®­ç»ƒæ—¶çš„æ ¸å¿ƒå·®å¼‚**

| ç»´åº¦ | æ ‡å‡† Transformer | MoEUT | å½±å“ |
|------|------------------|-------|------|
| **æŸå¤±å‡½æ•°** | äº¤å‰ç†µ | äº¤å‰ç†µ + ç†µæ­£åˆ™ | è´Ÿè½½å‡è¡¡ |
| **ä¼˜åŒ–å™¨** | AdamW | AdamW + ZeRO | ç›¸åŒç®—æ³• |
| **å­¦ä¹ ç‡** | Cosine + Warmup | Cosine + Warmup | ç›¸åŒç­–ç•¥ |
| **æ¢¯åº¦è£å‰ª** | 0.25 | 0.25 | ç›¸åŒé˜ˆå€¼ |
| **æ··åˆç²¾åº¦** | FP16 + Scaler | **BFloat16** | æ›´ç¨³å®š |
| **å¾®æ‰¹æ¬¡** | 1 | **2** | èŠ‚çœå†…å­˜ |
| **å‚æ•°æ›´æ–°** | æ¯å±‚ç‹¬ç«‹æ›´æ–° | **å…±äº«å±‚å¤šæ¬¡æ›´æ–°** | æ›´æ–°æ›´é›†ä¸­ |
| **æ¢¯åº¦æ¨¡å¼** | å¯†é›†æ¢¯åº¦ | **ç¨€ç–æ¢¯åº¦** | åªæ›´æ–°é€‰ä¸­ä¸“å®¶ |
| **å†…å­˜å ç”¨** | è¾ƒå° | è¾ƒå¤§ | éœ€è¦å¾®æ‰¹æ¬¡ |
| **è®­ç»ƒç¨³å®šæ€§** | ç¨³å®š | **éœ€è¦ç†µæ­£åˆ™** | é˜²æ­¢ä¸“å®¶åç¼© |
| **ç¼–è¯‘ä¼˜åŒ–** | âœ… `torch.compile` | âŒ Triton å†²çª | é€Ÿåº¦å·®å¼‚ |
| **å‚æ•°æ•ˆç‡** | ä½ | **é«˜ï¼ˆå…±äº«ï¼‰** | å‚æ•°å°‘ä½†æ€§èƒ½å¥½ |

---

#### **9. è®­ç»ƒæ—¶çš„æŠ€æœ¯æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ**

##### **æŒ‘æˆ˜ 1ï¼šä¸“å®¶åç¼©é—®é¢˜**

**ç°è±¡**ï¼š
```python
# è®­ç»ƒåˆæœŸå¯èƒ½å‘ç”Ÿï¼š
æ‰€æœ‰ token â†’ ä¸“å®¶ #3, #7, #15  # åªç”¨ 3 ä¸ª
å…¶ä»– 384 ä¸ªä¸“å®¶ â†’ ä»ä¸è¢«é€‰ä¸­    # èµ„æºæµªè´¹ï¼

# å¯¼è‡´ï¼š
- å¤§éƒ¨åˆ†ä¸“å®¶å‚æ•°å¾—ä¸åˆ°è®­ç»ƒ
- æ¨¡å‹å®¹é‡ä¸¥é‡æµªè´¹
- æ€§èƒ½ä¸‹é™
```

**è§£å†³æ–¹æ¡ˆ 1ï¼šç†µæ­£åˆ™åŒ–**
```python
# å¼ºåˆ¶ä¸“å®¶å¤šæ ·æ€§
loss += 0.01 * entropy_reg(expert_selections)

# ç†µæ­£åˆ™åŒ–é¼“åŠ±ï¼š
# - ä¸åŒ token ä½¿ç”¨ä¸åŒä¸“å®¶
# - ä¸åŒå±‚æ¿€æ´»ä¸åŒä¸“å®¶
# - æ‰€æœ‰ä¸“å®¶éƒ½æœ‰æœºä¼šè¢«ä½¿ç”¨
```

**è§£å†³æ–¹æ¡ˆ 2ï¼šExpert Dropout**
```python
# è®­ç»ƒæ—¶éšæœºå±è”½çƒ­é—¨ä¸“å®¶
if training and expert_dropout > 0:
    mask = torch.rand_like(sel) < expert_dropout
    sel = sel.masked_fill(mask, float('-inf'))

# å¼ºåˆ¶æ¨¡å‹å­¦ä¹ ä½¿ç”¨å¤šæ ·åŒ–çš„ä¸“å®¶
```

---

##### **æŒ‘æˆ˜ 2ï¼šå†…å­˜çˆ†ç‚¸é—®é¢˜**

**é—®é¢˜åˆ†æ**ï¼š
```python
# MoEUT å†…å­˜éœ€æ±‚ >> æ ‡å‡† Transformer

# å‚æ•°å†…å­˜ï¼š
387 ä¸ªä¸“å®¶ Ã— 128 ç»´ Ã— 1024 éšè—ç»´ â‰ˆ 200MB/å±‚
18 å±‚ Ã— 200MB = 3.6GBï¼ˆä»…ä¸“å®¶å‚æ•°ï¼‰

# æ¿€æ´»å†…å­˜ï¼š
batch=64, seq=1024, 18å±‚ â†’ æ•°GB

# æ¢¯åº¦ + ä¼˜åŒ–å™¨çŠ¶æ€ â†’ æ€»å…± 15-20GB
# å•å¡ 24GB æ˜¾å­˜å¯èƒ½ OOMï¼
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. å¾®æ‰¹æ¬¡ç´¯ç§¯
batch_64 â†’ [batch_32, batch_32]
å³°å€¼å†…å­˜ â‰ˆ åŸæ¥çš„ 60%

# 2. BFloat16
å†…å­˜å‡å°‘ 50%ï¼ˆç›¸æ¯” FP32ï¼‰

# 3. ZeroRedundancyOptimizerï¼ˆåˆ†å¸ƒå¼ï¼‰
ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡å­˜å‚¨
æ¯å¡åªå­˜ 1/N

# 4. å‚æ•°å…±äº«
18 å±‚ â†’ 9 ä¸ªç‰©ç†å±‚
å‚æ•°å†…å­˜å‡å°‘ 50%

# ç»„åˆæ•ˆæœï¼š
åŸæœ¬éœ€è¦ 40GB â†’ å®é™… 12GB
å¯ä»¥åœ¨ 24GB å¡ä¸Šè®­ç»ƒï¼
```

---

##### **æŒ‘æˆ˜ 3ï¼šæ¢¯åº¦ç¨€ç–æ€§é—®é¢˜**

**é—®é¢˜**ï¼š
```python
# æ¯æ¬¡åªæœ‰ k=16 ä¸ªä¸“å®¶è¢«æ¿€æ´»
# å…¶ä»– 387-16=371 ä¸ªä¸“å®¶çš„æ¢¯åº¦ä¸ºé›¶

# å¯èƒ½åæœï¼š
- æŸäº›ä¸“å®¶å¾ˆå°‘è¢«æ›´æ–°
- è®­ç»ƒä¸å……åˆ†
- å®¹é‡æµªè´¹
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. ç†µæ­£åˆ™åŒ–ï¼šå¼ºåˆ¶è½®æ¢
æ‰€æœ‰ä¸“å®¶éƒ½æœ‰æœºä¼šè¢«é€‰ä¸­ â†’ éƒ½èƒ½å¾—åˆ°æ¢¯åº¦

# 2. Expert Dropoutï¼šéšæœºå±è”½
å³ä½¿æŸä¸“å®¶å¾ˆçƒ­é—¨ï¼Œä¹Ÿä¼šè¢«éšæœºå±è”½
â†’ å…¶ä»–ä¸“å®¶æœ‰æœºä¼šè¢«è®­ç»ƒ

# 3. è¶³å¤Ÿé•¿çš„è®­ç»ƒ
100K æ­¥ Ã— 64 batch Ã— 1024 tokens â‰ˆ 6.5B tokens
æ¯ä¸ªä¸“å®¶éƒ½æœ‰è¶³å¤Ÿçš„è®­ç»ƒæœºä¼š
```

---

##### **æŒ‘æˆ˜ 4ï¼šå‚æ•°å…±äº«çš„æ¢¯åº¦å¤„ç†**

**é—®é¢˜**ï¼š
```python
# Universal Transformerï¼šåŒä¸€å±‚è¢«ä½¿ç”¨ 9 æ¬¡
# æ¯æ¬¡éƒ½ä¼šäº§ç”Ÿæ¢¯åº¦

Layer_0 åœ¨ç¬¬ 1 æ¬¡ä½¿ç”¨ï¼šgrad_1
Layer_0 åœ¨ç¬¬ 2 æ¬¡ä½¿ç”¨ï¼šgrad_2
...
Layer_0 åœ¨ç¬¬ 9 æ¬¡ä½¿ç”¨ï¼šgrad_9

# å¦‚ä½•èšåˆè¿™äº›æ¢¯åº¦ï¼Ÿ
```

**PyTorch è‡ªåŠ¨å¤„ç†**ï¼š
```python
# PyTorch è‡ªåŠ¨ç´¯åŠ åŒä¸€å‚æ•°çš„å¤šæ¬¡æ¢¯åº¦
layer.weight.grad = grad_1 + grad_2 + ... + grad_9

# ç­‰ä»·äºï¼š
# - è¯¥å±‚æ¥æ”¶äº† 9 å€çš„æ¢¯åº¦ä¿¡å·
# - ä½†ç»è¿‡å½’ä¸€åŒ–åæ˜¯åˆç†çš„
```

**åˆå§‹åŒ–è¡¥å¿**ï¼š
```python
# è€ƒè™‘é‡å¤æ¬¡æ•°çš„å‚æ•°åˆå§‹åŒ–
scale = sqrt(2 / (n_repeats * len(layers)))
      = sqrt(2 / (9 * 2))
      = sqrt(2 / 18)
      â‰ˆ 0.33

# æ¯”æ ‡å‡† Transformer çš„åˆå§‹åŒ–æ›´å°
# è¡¥å¿å¤šæ¬¡æ¢¯åº¦ç´¯ç§¯çš„æ•ˆæœ
```

---

#### **10. è®­ç»ƒæ•ˆæœä¸æ€§èƒ½æ•°æ®**

æ ¹æ®è®ºæ–‡å®éªŒç»“æœï¼š

##### **æ”¶æ•›é€Ÿåº¦**ï¼š
```
è®­ç»ƒæ­¥æ•°: 0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 50K â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 100K
                  â”‚              â”‚
Transformer:     70%            80%  å‡†ç¡®ç‡
MoEUT:           75%            82%  å‡†ç¡®ç‡

MoEUT æ”¶æ•›ç¨å¿«ï¼Œæœ€ç»ˆæ€§èƒ½ç¨å¥½
```

##### **å¤–æ¨èƒ½åŠ›å¯¹æ¯”**ï¼ˆè®ºæ–‡ Table 1ï¼‰ï¼š

| è®­ç»ƒæ¨¡å¼ | æ ‡å‡† Transformer | MoEUT | æå‡ |
|----------|------------------|-------|------|
| Q+A è®­ç»ƒ | 41% | 36% | -5% |
| **Answer-only è®­ç»ƒ** | 48% | **63%** | **+15%** ğŸ”¥ |

**å…³é”®å‘ç°**ï¼š
- MoEUT åœ¨ **answer-only** æ¨¡å¼ä¸‹è¡¨ç°æä½³
- è¯´æ˜ MoEUT çœŸæ­£å­¦ä¼šäº†**è¿­ä»£æ¨ç†**
- ä¸éœ€è¦çœ‹é—®é¢˜ï¼Œå°±èƒ½åšæ·±å±‚è®¡ç®—

##### **è®­ç»ƒèµ„æºå¯¹æ¯”**ï¼š

| æŒ‡æ ‡ | æ ‡å‡† Transformer | MoEUT |
|------|------------------|-------|
| è®­ç»ƒé€Ÿåº¦ | å¿«ï¼ˆå¯ç¼–è¯‘ï¼‰| ç¨æ…¢ï¼ˆçº¦ 0.8Ã—ï¼‰|
| å†…å­˜å ç”¨ | 12GB | 18GBï¼ˆå¾®æ‰¹æ¬¡åï¼‰|
| GPU åˆ©ç”¨ç‡ | 95% | 85%ï¼ˆç¨€ç–è®¡ç®—ï¼‰|
| å‚æ•°é‡ | 244M | 244Mï¼ˆç›¸è¿‘ï¼‰|
| **æœ‰æ•ˆå‚æ•°** | 244M | **çº¦ 130M**ï¼ˆå…±äº«ï¼‰|

**ç»“è®º**ï¼š
- MoEUT ç”¨**æ›´å°‘çš„æœ‰æ•ˆå‚æ•°**
- è¾¾åˆ°**æ›´å¥½çš„å¤–æ¨æ€§èƒ½**
- è¯æ˜å‚æ•°å…±äº« + è¿­ä»£æ¨ç†æ˜¯æœ‰æ•ˆçš„

---

## å››ã€å¤§è„‘ä¸Transformerçš„ç±»æ¯”

### 4.1 æ¨ç†æ—¶çš„å¯¹æ¯”

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
äººè„‘ï¼ˆæ¨ç†æ—¶ï¼‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

æ„Ÿå®˜è¾“å…¥ â†’ ç¥ç»ç”µä¿¡å·
  â†“
Layer 1: è§†è§‰çš®å±‚V1
  - ç”µä¿¡å·æµè¿‡ âœ…
  - çªè§¦æƒé‡å¾®è°ƒ âš¡ (çŸ­æœŸå¯å¡‘æ€§)
  - æ¿€æ´»æ¨¡å¼æ”¹å˜ âœ…

Layer 2: è§†è§‰çš®å±‚V2
  - ç”µä¿¡å·æµè¿‡ âœ…
  - çªè§¦æƒé‡å¾®è°ƒ âš¡
  - æ¿€æ´»æ¨¡å¼æ”¹å˜ âœ…

ç‰¹ç‚¹ï¼š
âœ… æ¿€æ´»æ”¹å˜ï¼ˆç”µä¿¡å·ï¼‰
âœ… å‚æ•°ä¹Ÿåœ¨æ”¹å˜ï¼ˆçªè§¦å¯å¡‘æ€§ï¼‰
âœ… æ¨ç†å’Œå­¦ä¹ åŒæ—¶å‘ç”Ÿ

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Transformerï¼ˆæ¨ç†æ—¶ï¼‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

æ–‡æœ¬è¾“å…¥ â†’ å‘é‡
  â†“
Layer 1: Attention + MLP
  - å‘é‡æµè¿‡ âœ…
  - å‚æ•°å®Œå…¨ä¸å˜ ğŸ”’ (å†»ç»“)
  - å‘é‡æ”¹å˜ âœ…

Layer 2: Attention + MLP
  - å‘é‡æµè¿‡ âœ…
  - å‚æ•°å®Œå…¨ä¸å˜ ğŸ”’
  - å‘é‡æ”¹å˜ âœ…

ç‰¹ç‚¹ï¼š
âœ… æ¿€æ´»æ”¹å˜ï¼ˆå‘é‡ï¼‰
âŒ å‚æ•°å®Œå…¨å›ºå®š
âŒ æ¨ç†å’Œå­¦ä¹ åˆ†ç¦»
```

---

### 4.2 è®­ç»ƒæ—¶çš„å¯¹æ¯”

```
è®­ç»ƒæ—¶çš„Transformerï¼š

è¾“å…¥ â†’ å‘é‡
  â†“
Layer 1:
  - å‘é‡æµè¿‡ âœ…
  - è®¡ç®—æ¢¯åº¦
  - å‚æ•°æ›´æ–° âš¡ W â† W - lr*grad

Layer 2:
  - å‘é‡æµè¿‡ âœ…
  - è®¡ç®—æ¢¯åº¦
  - å‚æ•°æ›´æ–° âš¡

è¿™æ—¶ç±»æ¯”å°±å¯¹äº†ï¼
âœ… æ¿€æ´»æ”¹å˜
âœ… å‚æ•°æ”¹å˜
âœ… "ä¿¡å·"å½±å“"è¿æ¥"
```

---

### 4.3 å…³é”®å·®å¼‚æ€»ç»“

| ç»´åº¦ | äººè„‘ | Transformeræ¨ç† | Transformerè®­ç»ƒ |
|------|------|-----------------|-----------------|
| è¾“å…¥â†’ä¿¡å· | âœ… | âœ… | âœ… |
| ä¿¡å·æµåŠ¨ | âœ… | âœ… | âœ… |
| ä¿¡å·æ”¹å˜çŠ¶æ€ | âœ… | âœ… | âœ… |
| å‚æ•°æ”¹å˜ | âœ… å®æ—¶ | âŒ å†»ç»“ | âœ… æ‰¹é‡ |
| å­¦ä¹ æ–¹å¼ | æŒç»­å­¦ä¹  | åªè¯» | æ¢¯åº¦ä¸‹é™ |

**æ ¸å¿ƒå·®å¼‚**ï¼š
- å¤§è„‘ï¼šæ¨ç†å’Œå­¦ä¹ åŒæ—¶å‘ç”Ÿ
- Transformerï¼šæ¨ç†å’Œå­¦ä¹ å®Œå…¨åˆ†ç¦»

---

### 4.4 ç¼©å°å·®è·çš„ç ”ç©¶

#### **1. In-Context Learning**
```python
# è™½ç„¶å‚æ•°ä¸å˜ï¼Œä½†"è¡Œä¸º"åœ¨å˜

è¾“å…¥: "catâ†’gato, dogâ†’?"
è¾“å‡º: "perro"  # ä»context"å­¦ä¹ "äº†

ç±»ä¼¼å¤§è„‘çš„å·¥ä½œè®°å¿†
```

#### **2. Retrieval-Augmented Generation**
```python
å›ºå®šå‚æ•° + å¯æ›´æ–°å¤–éƒ¨çŸ¥è¯†åº“

ç±»ä¼¼ï¼š
- å›ºå®šå‚æ•° = å›ºæœ‰ç»“æ„
- å¤–éƒ¨çŸ¥è¯† = å¯æŸ¥ç¬”è®°æœ¬
```

#### **3. æœªæ¥çš„å¯å¡‘Transformerï¼Ÿ**
```python
class PlasticTransformer:
    def forward(self, x):
        h = x
        for layer in self.layers:
            h_new = layer(h)

            # æ ¹æ®æ¿€æ´»å¾®è°ƒå‚æ•°ï¼
            layer.hebbian_update(h, h_new)

            h = h_new
        return h
```

---

### 4.5 MoEUT ä¸å¤§è„‘çš„ç²¾ç¡®ç±»æ¯”

> **æ ¸å¿ƒæ´å¯Ÿ**ï¼šMoEUT æ›´åƒ"å¤§è„‘ä¸æ–­è°ƒç”¨ä¸åŒè„‘åŒºå¤„ç†åŒä¸€ä¸ªé—®é¢˜"

#### **ç±»æ¯”å…³ç³»**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             MoEUT (å•å±‚å…±äº«)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ ä¸“å®¶ 1   â”‚  â”‚ ä¸“å®¶ 2   â”‚  â”‚ ä¸“å®¶ 3   â”‚  ...      â”‚
â”‚  â”‚ (æ•°å­¦)   â”‚  â”‚ (è¯­è¨€)   â”‚  â”‚ (é€»è¾‘)   â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“               â†‘
  [å‘é‡è¡¨ç¤º] â”€â”€â”€â”€â†’ [å¤„ç†] â”€â”€â”€â”€â†’ [å‘é‡è¡¨ç¤º]
       â”‚                          â”‚
       â””â”€â”€â”€â”€â”€ è¿­ä»£16æ¬¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                â‰ˆ â‰ˆ â‰ˆ

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                æ•´ä¸ªå¤§è„‘                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ è§†è§‰çš®å±‚ â”‚  â”‚ è¯­è¨€åŒº   â”‚  â”‚ é€»è¾‘åŒº   â”‚  ...      â”‚
â”‚  â”‚ (ç©ºé—´)   â”‚  â”‚ (ç¬¦å·)   â”‚  â”‚ (æ¨ç†)   â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“               â†‘
  [ç¥ç»ç”µä¿¡å·] â”€â”€â†’ [ç¥ç»å…ƒ] â”€â”€â†’ [ç¥ç»ç”µä¿¡å·]
       â”‚                          â”‚
       â””â”€â”€â”€â”€â”€ æŒç»­æµåŠ¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### **å¯¹åº”å…³ç³»è¯¦è§£**

**1. å•å±‚ MoE = æ•´ä¸ªå¤§è„‘åŠŸèƒ½æ¨¡å—é›†åˆ**

```python
# MoEUT
shared_layer = MoEUTLayer(
    experts = [æ•°å­¦ä¸“å®¶, è¯­è¨€ä¸“å®¶, é€»è¾‘ä¸“å®¶, ...]
)
for step in range(16):
    vector = shared_layer(vector)  # é‡å¤ä½¿ç”¨åŒä¸€ç»„ä¸“å®¶

# å¤§è„‘
å¤§è„‘ = {è§†è§‰çš®å±‚, è¯­è¨€åŒº, é€»è¾‘åŒº, ...}
for æ—¶é—´æ­¥ in æ€è€ƒè¿‡ç¨‹:
    ç”µä¿¡å· = å¤§è„‘.æ¿€æ´»ç›¸å…³è„‘åŒº(ç”µä¿¡å·)  # é‡å¤æ¿€æ´»ä¸åŒè„‘åŒº
```

**ä¸æ˜¯**ï¼š"ç¬¬1å±‚ = åˆçº§è§†è§‰çš®å±‚ï¼Œç¬¬2å±‚ = é«˜çº§è§†è§‰çš®å±‚"
**è€Œæ˜¯**ï¼š"åŒä¸€ç»„åŠŸèƒ½åŒºï¼Œåå¤ä½¿ç”¨ï¼Œæ¯æ¬¡æ¿€æ´»ä¸åŒç»„åˆ"

---

**2. æ¯ä¸ªä¸“å®¶ â‰ˆ æŸä¸ªä¸“é—¨è„‘åŒº**

```
ä¸“å®¶1 (æ•°å­—è¯†åˆ«) â‰ˆ é¡¶å¶æ•°å­¦åŒº
ä¸“å®¶2 (è¯­æ³•å¤„ç†) â‰ˆ å¸ƒç½—å¡åŒº
ä¸“å®¶3 (è¯­ä¹‰ç†è§£) â‰ˆ å¨å°”å°¼å…‹åŒº
ä¸“å®¶4 (é€»è¾‘æ¨ç†) â‰ˆ å‰é¢å¶
ä¸“å®¶5 (ç©ºé—´å…³ç³») â‰ˆ é¡¶å¶ç©ºé—´åŒº
...
```

**å…³é”®ç‰¹æ€§**ï¼š
- âœ… **ä¸“ä¸šåŒ–**ï¼šæ¯ä¸ªä¸“å®¶/è„‘åŒºæ“…é•¿ç‰¹å®šä»»åŠ¡
- âœ… **é€‰æ‹©æ€§æ¿€æ´»**ï¼šæ ¹æ®ä»»åŠ¡é€‰æ‹©ç›¸å…³ä¸“å®¶/è„‘åŒº
- âœ… **å¯é‡ç”¨**ï¼šåŒä¸€ä¸“å®¶/è„‘åŒºå¯è¢«å¤šæ¬¡è°ƒç”¨

---

**3. å‘é‡ = ç”µä¿¡å·**

```python
# MoEUT
v = [0.2, -0.5, 0.8, ...]  # 512ç»´å‘é‡
# è¡¨ç¤ºå½“å‰çš„"æ€è€ƒçŠ¶æ€"

# å¤§è„‘
ç”µä¿¡å·æ¨¡å¼ = [ç¥ç»å…ƒ1æ¿€æ´», ç¥ç»å…ƒ2æŠ‘åˆ¶, ç¥ç»å…ƒ3æ¿€æ´», ...]
# è¡¨ç¤ºå½“å‰çš„"æ€è€ƒå†…å®¹"
```

**ä¸æ˜¯** tokenï¼ˆ"apple"è¿™ä¸ªç¬¦å·ï¼‰
**è€Œæ˜¯** å‘é‡è¡¨ç¤ºï¼ˆ"è‹¹æœ"çš„è¯­ä¹‰ã€é¢œè‰²ã€å½¢çŠ¶ç­‰çš„ç»„åˆè¡¨ç¤ºï¼‰

---

**4. é‡å¤ä½¿ç”¨ = è¿­ä»£æ€è€ƒ**

**MoEUT å¤„ç†æ•°å­¦é¢˜çš„è¿‡ç¨‹**ï¼š
```
é—®é¢˜ï¼š"What is ((14)/(-6))/(1162/(-4980))?"

ç¬¬1-2å±‚ï¼ˆç†è§£é—®é¢˜ï¼‰ï¼š
  æ¿€æ´»ä¸“å®¶ï¼š[è¯­è¨€ç†è§£, æ•°å­—è¯†åˆ«, è¿ç®—ç¬¦è¯†åˆ«]
  å‘é‡å˜åŒ–ï¼šæ–‡æœ¬ â†’ æ•°å­¦ç»“æ„è¡¨ç¤º

ç¬¬3-5å±‚ï¼ˆè®¡ç®—ç¬¬ä¸€ä¸ªåˆ†æ•°ï¼‰ï¼š
  æ¿€æ´»ä¸“å®¶ï¼š[é™¤æ³•è¿ç®—, ç¬¦å·å¤„ç†, åˆ†æ•°åŒ–ç®€]
  å‘é‡å˜åŒ–ï¼š14/(-6) â†’ -7/3

ç¬¬6-8å±‚ï¼ˆè®¡ç®—ç¬¬äºŒä¸ªåˆ†æ•°ï¼‰ï¼š
  æ¿€æ´»ä¸“å®¶ï¼š[é™¤æ³•è¿ç®—, ç¬¦å·å¤„ç†, åˆ†æ•°åŒ–ç®€]  # é‡ç”¨é™¤æ³•ä¸“å®¶ï¼
  å‘é‡å˜åŒ–ï¼š1162/(-4980) â†’ -581/2490

ç¬¬9-12å±‚ï¼ˆç»„åˆç»“æœï¼‰ï¼š
  æ¿€æ´»ä¸“å®¶ï¼š[å¤åˆè¿ç®—, åˆ†æ•°ä¹˜é™¤, ç»“æœåŒ–ç®€]
  å‘é‡å˜åŒ–ï¼š(-7/3) / (-581/2490) â†’ æœ€ç»ˆç­”æ¡ˆ

ç¬¬13-16å±‚ï¼ˆéªŒè¯è¾“å‡ºï¼‰ï¼š
  æ¿€æ´»ä¸“å®¶ï¼š[ç»“æœéªŒè¯, è¾“å‡ºæ ¼å¼åŒ–]
  å‘é‡å˜åŒ–ï¼šå†…éƒ¨è¡¨ç¤º â†’ tokenåºåˆ—
```

**å¤§è„‘è§£å†³åŒæ ·é—®é¢˜**ï¼š
```
æ—¶åˆ»1-2ï¼ˆç†è§£é—®é¢˜ï¼‰ï¼š
  æ¿€æ´»è„‘åŒºï¼š[è¯­è¨€åŒº, è§†è§‰æ•°å­—åŒº]
  ç”µä¿¡å·ï¼šæ–‡æœ¬æ„ŸçŸ¥ â†’ é—®é¢˜ç†è§£

æ—¶åˆ»3-5ï¼ˆæå–å’Œè®¡ç®—ï¼‰ï¼š
  æ¿€æ´»è„‘åŒºï¼š[æ•°å­¦åŒº, å·¥ä½œè®°å¿†]
  ç”µä¿¡å·ï¼šè¯†åˆ«14, -6, é™¤æ³•

æ—¶åˆ»6-8ï¼ˆç»§ç»­è®¡ç®—ï¼‰ï¼š
  æ¿€æ´»è„‘åŒºï¼š[æ•°å­¦åŒº, å·¥ä½œè®°å¿†]  # é‡ç”¨æ•°å­¦åŒºï¼
  ç”µä¿¡å·ï¼šè¯†åˆ«1162, -4980, é™¤æ³•

æ—¶åˆ»9-12ï¼ˆæ•´åˆï¼‰ï¼š
  æ¿€æ´»è„‘åŒºï¼š[é€»è¾‘åŒº, æ•°å­¦åŒº, å·¥ä½œè®°å¿†]
  ç”µä¿¡å·ï¼šç»„åˆä¸­é—´ç»“æœ

æ—¶åˆ»13-16ï¼ˆè¾“å‡ºï¼‰ï¼š
  æ¿€æ´»è„‘åŒºï¼š[è¯­è¨€åŒº, è¿åŠ¨çš®å±‚]
  ç”µä¿¡å·ï¼šå½¢æˆç­”æ¡ˆè¡¨è¿°
```

---

#### **å…³é”®ç›¸ä¼¼ç‚¹**

**âœ… 1. åŠ¨æ€è·¯ç”±**
```python
# MoEUT
router_weights = sigmoid(linear(vector, expert_sel))
selected = topk(router_weights, k=8)

# å¤§è„‘
æ¿€æ´»å¼ºåº¦ = æ ¹æ®å½“å‰ç”µä¿¡å·å’Œä»»åŠ¡éœ€æ±‚å†³å®š
é€‰ä¸­è„‘åŒº = æ¿€æ´»å¼ºåº¦æœ€é«˜çš„åŒºåŸŸ
```

**âœ… 2. ä¸“ä¸šåŒ–åˆ†å·¥**
- MoEUTï¼š128ä¸ªä¸“å®¶ï¼Œæ¯ä¸ªæ“…é•¿ä¸åŒæ¨¡å¼
- å¤§è„‘ï¼šä¸åŒè„‘åŒºï¼ŒåŠŸèƒ½ä¸“é—¨åŒ–

**âœ… 3. å‚æ•°å…±äº« = å›ºå®šè„‘ç»“æ„**
```python
# MoEUTï¼š16å±‚ç”¨åŒä¸€ç»„ä¸“å®¶å‚æ•°
for step in range(16):
    use_same_experts(vector)

# å¤§è„‘ï¼šæ€è€ƒè¿‡ç¨‹ä¸­è„‘ç»“æ„ä¸å˜
for æ—¶é—´æ­¥ in æ€è€ƒ:
    use_same_brain_regions(ç”µä¿¡å·)
```

**âœ… 4. æ®‹å·®è¿æ¥ = ä¿¡æ¯ç´¯ç§¯**
```python
# MoEUT
v = v + expert_output  # ç´¯åŠ è€Œéæ›¿æ¢

# å¤§è„‘
æ–°çŠ¶æ€ = æ—§çŠ¶æ€ + æ–°ä¿¡æ¯  # é€æ­¥æ•´åˆä¿¡æ¯
```

---

#### **æ ¸å¿ƒå·®å¼‚**

**âŒ å‚æ•°å¯å¡‘æ€§**

```
MoEUTï¼š
- æ¨ç†æ—¶ï¼šå‚æ•°å®Œå…¨å†»ç»“ ğŸ”’
- è®­ç»ƒæ—¶ï¼šå‚æ•°æ‰¹é‡æ›´æ–° âš¡

å¤§è„‘ï¼š
- æ¨ç†æ—¶ï¼šçªè§¦å®æ—¶å¾®è°ƒ âš¡
- å­¦ä¹ æ—¶ï¼šçªè§¦æŒç»­å˜åŒ– âš¡âš¡
```

---

#### **ä¸ºä»€ä¹ˆè¿™ä¸ªç±»æ¯”å‡†ç¡®ï¼Ÿ**

**ä¼ ç»Ÿè¯¯è§£**ï¼š
```
âŒ ç¬¬1å±‚ = æµ…å±‚è„‘åŒºï¼Œç¬¬2å±‚ = æ›´æ·±å±‚è„‘åŒº
âŒ ä¿¡å·å•å‘æµåŠ¨ï¼šè¾“å…¥ â†’ å±‚1 â†’ å±‚2 â†’ ... â†’ è¾“å‡º
```

**MoEUT æ­ç¤ºçš„çœŸç›¸**ï¼š
```
âœ… æ‰€æœ‰"ä¸“å®¶"éƒ½æ˜¯å¹³ç­‰çš„åŠŸèƒ½æ¨¡å—
âœ… ä¿¡å·åœ¨åŒä¸€ç»„æ¨¡å—é—´è¿­ä»£æµåŠ¨
âœ… æ¯æ¬¡è¿­ä»£é€‰æ‹©ä¸åŒçš„ä¸“å®¶ç»„åˆ
âœ… ç±»ä¼¼å¤§è„‘"åœ¨ä¸åŒè„‘åŒºé—´è¿­ä»£æ€è€ƒ"
```

---

#### **ä»£ç éªŒè¯è¿™ä¸ªç±»æ¯”**

```python
# moeut.py:38-55 - åˆ›å»ºå…±äº«å±‚
self.layers = [create_layer() for _ in range(group_size)]

# universal_transformer.py:38-42 - é‡å¤ä½¿ç”¨
for r in range(n_repeats):
    for layer in self.layers:
        x = layer(x)  # åŒä¸€å±‚ï¼Œåå¤ä½¿ç”¨

# moeut.py:94-107 - åŠ¨æ€é€‰æ‹©ä¸“å®¶
sel = sigmoid(linear(x, expert_sel))
experts_to_use = topk(sel, k=8)
output = only_compute_selected_experts(x, experts_to_use)
```

**ç±»æ¯”å¤§è„‘**ï¼š
```
å¤§è„‘ç»“æ„ = å›ºå®šçš„è„‘åŒºé›†åˆ
æ€è€ƒè¿‡ç¨‹ = åå¤æ¿€æ´»ä¸åŒè„‘åŒº
æ¯æ—¶åˆ» = æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©æ¿€æ´»å“ªäº›è„‘åŒº
è¾“å‡º = è¿­ä»£å¤šæ¬¡åçš„æœ€ç»ˆçŠ¶æ€
```

---

#### **æ€»ç»“**

**MoEUT = "å¤§è„‘çš„æ½œç©ºé—´è¿­ä»£æ€è€ƒ"**

| ç»´åº¦ | MoEUT | å¤§è„‘ |
|------|-------|------|
| åŸºç¡€ç»“æ„ | ä¸€ç»„å…±äº«ä¸“å®¶ | åŠŸèƒ½è„‘åŒºé›†åˆ |
| ä¿¡å·è¡¨ç¤º | è¿ç»­å‘é‡ | ç¥ç»ç”µä¿¡å·æ¨¡å¼ |
| å¤„ç†æ–¹å¼ | è¿­ä»£16æ¬¡ | æŒç»­è¿­ä»£ |
| é€‰æ‹©æœºåˆ¶ | Sigmoid + TopK | æ¿€æ´»é˜ˆå€¼ + ç«äº‰ |
| ä¸“ä¸šåŒ– | æ¯ä¸ªä¸“å®¶ä¸åŒåŠŸèƒ½ | æ¯ä¸ªè„‘åŒºä¸åŒåŠŸèƒ½ |
| å‚æ•°å˜åŒ– | âŒ æ¨ç†æ—¶å†»ç»“ | âœ… å®æ—¶å¯å¡‘ |
| å¯è§æ€§ | âŒ é»‘ç›’ï¼ˆæ½œç©ºé—´ï¼‰ | âš ï¸ éƒ¨åˆ†å¯æµ‹ï¼ˆfMRIï¼‰ |

**æœ€ç²¾ç¡®çš„æè¿°**ï¼š
> MoEUT æ˜¯åœ¨**æ½œç©ºé—´**ï¼ˆå‘é‡ç©ºé—´ï¼‰ä¸­ï¼Œé€šè¿‡**é‡å¤è°ƒç”¨åŒä¸€ç»„ä¸“å®¶**ï¼ˆç±»ä¼¼é‡å¤æ¿€æ´»ç›¸åŒè„‘åŒºï¼‰ï¼Œå®ç°**è¿­ä»£æ¨ç†**ï¼ˆç±»ä¼¼å¤§è„‘çš„å†…éƒ¨ç‹¬ç™½ï¼‰çš„æ¶æ„ã€‚

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå®ƒå« **"æ½œç©ºé—´ CoT"** æˆ– **"æ½œåœ¨è¿­ä»£æ¨ç†"**ï¼

---

## äº”ã€æ ¸å¿ƒæ´å¯Ÿä¸å¯ç¤º

### 5.1 å…³é”®å‘ç°

1. **æ·±åº¦â‰ æœ‰æ•ˆæ·±åº¦**
   - ç‰©ç†å±‚æ•° â‰  æœ‰æ•ˆè®¡ç®—æ­¥éª¤æ•°
   - å½“å‰LLMï¼šç‰©ç†å±‚æ•°80ï¼Œæœ‰æ•ˆæ·±åº¦â‰ˆ40

2. **å›ºå®šæ·±åº¦çš„é—®é¢˜**
   - ç®€å•é—®é¢˜ï¼šæµªè´¹ååŠéƒ¨åˆ†å±‚
   - å¤æ‚é—®é¢˜ï¼šå¯èƒ½æ·±åº¦ä¸å¤Ÿ
   - æ— æ³•è‡ªé€‚åº”è°ƒæ•´

3. **å‚æ•°å…±äº«çš„ä»·å€¼**
   - å¯ä»¥å®ç°è¿­ä»£è®¡ç®—
   - æ›´å¥½åœ°åˆ©ç”¨æ·±åº¦
   - MoEUTæ˜¯ä¸€ä¸ªå¥½æ–¹å‘

4. **æ¨ç†æ–¹å¼çš„æ ¹æœ¬å·®å¼‚**
   - ç†æƒ³ï¼šæ·±åº¦ä¼˜å…ˆï¼ˆå±‚å±‚é€’è¿›ï¼‰
   - å®é™…ï¼šå¹¿åº¦ä¼˜å…ˆï¼ˆå¹¶è¡Œå¾®è°ƒï¼‰

---

### 5.2 å¯¹ä¸šç•Œçš„å½±å“

#### **è§£é‡Šäº†ç°æœ‰ç°è±¡**ï¼š
- âœ… ä¸ºä»€ä¹ˆScaling Lawé¥±å’Œ
- âœ… ä¸ºä»€ä¹ˆå±‚å‰ªææ•ˆæœä¸é”™
- âœ… ä¸ºä»€ä¹ˆCoTè¿™ä¹ˆæœ‰æ•ˆ
- âœ… ä¸ºä»€ä¹ˆå¢åŠ æ·±åº¦æ”¶ç›Šé€’å‡

#### **å¯¹æœªæ¥ç ”ç©¶çš„å¯ç¤º**ï¼š

**1. æ¶æ„æ”¹è¿›æ–¹å‘**
- å‚æ•°å…±äº«æœºåˆ¶ï¼ˆUniversal Transformerï¼‰
- ä¸“å®¶æ··åˆï¼ˆMoEï¼‰
- è‡ªé€‚åº”æ·±åº¦
- æ›´å¥½çš„æ®‹å·®è¿æ¥

**2. è®­ç»ƒç›®æ ‡ä¼˜åŒ–**
- é¼“åŠ±æ·±å±‚è®¡ç®—çš„æŸå¤±å‡½æ•°
- ä¸å»ºæ¨¡é«˜ç†µéƒ¨åˆ†
- æ·±åº¦çš„æ­£åˆ™åŒ–

**3. å®½åº¦vsæ·±åº¦æƒè¡¡**
- æ®‹å·®æµå®½åº¦å¯èƒ½æ˜¯ç“¶é¢ˆ
- å¢åŠ å®½åº¦å¯èƒ½æ¯”æ·±åº¦æ›´æœ‰æ•ˆ

**4. å¯¹Latent Thinkingçš„è´¨ç–‘**
- å¦‚æœæ˜¯é¢„è®­ç»ƒç›®æ ‡é—®é¢˜ â†’ æœ‰å¸Œæœ›
- å¦‚æœæ˜¯æ¶æ„é—®é¢˜ â†’ éœ€è¦æ–°æ–¹æ¡ˆ

---

### 5.3 æœªè§£å†³çš„é—®é¢˜

1. **ä¸ºä»€ä¹ˆä¼šè¿™æ ·ï¼Ÿ**
   - æ˜¯é¢„è®­ç»ƒç›®æ ‡å¯¼è‡´çš„ï¼Ÿ
   - è¿˜æ˜¯Transformeræ¶æ„çš„æ ¹æœ¬é™åˆ¶ï¼Ÿ

2. **å¦‚ä½•æ”¹è¿›ï¼Ÿ**
   - MoEUTèƒ½å¦æ‰©å±•åˆ°100B+è§„æ¨¡ï¼Ÿ
   - æœ‰æ²¡æœ‰å…¶ä»–æ¶æ„æ–¹æ¡ˆï¼Ÿ

3. **å¤§è„‘çš„å¯ç¤º**
   - èƒ½å¦å®ç°"å¯å¡‘çš„Transformer"ï¼Ÿ
   - æ¨ç†å’Œå­¦ä¹ èƒ½å¦ç»Ÿä¸€ï¼Ÿ

---

## å…­ã€è®¨è®ºä¸­çš„ç²¾å½©ç±»æ¯”

### 6.1 æµæ°´çº¿ vs æŠ€èƒ½å·¥äºº

```
æ ‡å‡†Transformer = æµæ°´çº¿å·¥å‚
- å·¥ä½1: è½¦å‰Šï¼ˆä¸èƒ½é‡å¤ï¼‰
- å·¥ä½2: é“£å‰Šï¼ˆä¸èƒ½é‡å¤ï¼‰
- å·¥ä½3: æŠ›å…‰ï¼ˆä¸èƒ½é‡å¤ï¼‰
- æ·±åº¦å›ºå®šï¼Œä¸èƒ½è¿­ä»£

MoEUT = æŠ€èƒ½å·¥äººæ± 
- å·¥äºº1: è½¦å‰ŠæŠ€èƒ½
- å·¥äºº2: é“£å‰ŠæŠ€èƒ½
- å·¥äºº3: æŠ›å…‰æŠ€èƒ½
- å¯ä»¥åå¤è°ƒç”¨åŒä¸€å·¥äºº
- å¯ä»¥è¿­ä»£ï¼Œæ·±åº¦è‡ªé€‚åº”
```

---

### 6.2 çº¸ä¸Šåˆ—è‰ç¨¿ vs å¿ƒç®—

```
CoT = åœ¨çº¸ä¸Šåˆ—è‰ç¨¿
- æ¯æ­¥éƒ½å†™ä¸‹æ¥
- åˆ«äººèƒ½çœ‹æ‡‚
- ä½†å ç”¨ç©ºé—´ï¼ˆcontextï¼‰

MoEUT = åœ¨è„‘ä¸­å¿ƒç®—
- åå¤ä½¿ç”¨"å¿ƒç®—æŠ€å·§"
- åˆ«äººçœ‹ä¸åˆ°è¿‡ç¨‹
- ä½†æ›´å¿«ï¼Œä¸å é¢å¤–ç©ºé—´
```

---

### 6.3 ç†è§£æ–‡ç«  vs æ¶¦è‰²æ–‡ç« 

```
å‰åŠéƒ¨åˆ† = ç†è§£å’Œæ¨ç†
- ä¸æ–­ä¿®æ­£ç†è§£
- å‘ç°"åŸæ¥æ˜¯è¿™ä¸ªæ„æ€"
- å»ºç«‹æ·±å±‚è¯­ä¹‰

ååŠéƒ¨åˆ† = æ¶¦è‰²æªè¾
- åå¤ä¿®æ”¹è¡¨è¾¾
- ä½†ä¸æ”¹å˜å†…å®¹
- å¾®è°ƒæ¦‚ç‡åˆ†å¸ƒ
```

---

## ä¸ƒã€å‚è€ƒèµ„æ–™

### è®ºæ–‡
- åŸæ–‡PDFï¼š`Do Language Models Use Their Depth Efficiently.pdf`
- arXiv: 2505.13898v3
- ä»£ç ï¼šhttps://github.com/robertcsordas/llm_effective_depth

### ç›¸å…³æ¨¡å‹
- Llama 3.1 ç³»åˆ—ï¼ˆ8B, 70B, 405Bï¼‰
- Qwen 3 ç³»åˆ—ï¼ˆ8B, 14B, 32Bï¼‰
- OLMo 2 ç³»åˆ—ï¼ˆ7B, 13B, 32Bï¼‰
- MoEUTï¼ˆ244Mï¼Œå®éªŒæ€§ï¼‰

### ç›¸å…³æ¦‚å¿µ
- Universal Transformer
- Mixture of Experts (MoE)
- Chain of Thought (CoT)
- Logitlens
- Residual Stream Analysis

---

## å…«ã€è®¨è®ºæ€»ç»“

æœ¬æ¬¡è®¨è®ºæ·±å…¥æ¢è®¨äº†ï¼š

1. **è®ºæ–‡æ ¸å¿ƒå‘ç°**
   - æ·±å±‚æ¨¡å‹çš„ååŠéƒ¨åˆ†åˆ©ç”¨ä¸è¶³
   - è®¡ç®—æ·±åº¦ä¸é—®é¢˜å¤æ‚åº¦æ— å…³
   - æ·±å±‚æ¨¡å‹åªæ˜¯"æ‹‰ä¼¸"äº†æµ…å±‚è®¡ç®—
   - æœ‰æ•ˆæ·±åº¦è¿œå°äºç‰©ç†æ·±åº¦

2. **Transformerå·¥ä½œåŸç†**
   - æ¯å±‚éƒ½æ˜¯å‘é‡å˜æ¢ï¼Œä¸æ˜¯tokenå˜æ¢
   - å¯ä»¥é€šè¿‡Logitlens"å·çœ‹"ä»»æ„å±‚çš„è¾“å‡º
   - å‰åŠéƒ¨åˆ†æ¨ç†ï¼ŒååŠéƒ¨åˆ†å¾®è°ƒ
   - æ®‹å·®è¿æ¥å¯¼è‡´ä¿¡æ¯ç´¯ç§¯è€Œéæ›¿æ¢

3. **MoEUTçš„åˆ›æ–°**
   - å‚æ•°å…±äº« + ä¸“å®¶æ··åˆ
   - å®ç°äº†"æ½œç©ºé—´CoT"
   - é€šè¿‡é‡ç”¨ä¸“å®¶å¢åŠ æœ‰æ•ˆæ·±åº¦
   - æœ¬è´¨ï¼šå•å±‚MoE + æ½œåœ¨è¿­ä»£æ¨ç†

4. **MoEUT ä»£ç å®ç°æ·±åº¦åˆ†æ**
   - **UniversalTransformer**ï¼šå‚æ•°å…±äº«æœºåˆ¶ï¼Œ2ä¸ªç‰©ç†å±‚é‡å¤8æ¬¡ = 16ä¸ªé€»è¾‘å±‚
   - **SigmaMoE**ï¼šMoE FFN å®ç°ï¼ŒSigmoidè·¯ç”± + TopKé€‰æ‹©
   - **SwitchHeadRope**ï¼šæ³¨æ„åŠ›ä¹Ÿç”¨MoEï¼ŒVå’ŒOæŠ•å½±éƒ½æœ‰å¤šä¸“å®¶
   - **cvmm**ï¼šæ¡ä»¶çŸ©é˜µä¹˜æ³•ï¼Œåªè®¡ç®—é€‰ä¸­ä¸“å®¶ï¼ŒèŠ‚çœ16å€è®¡ç®—
   - **ç†µæ­£åˆ™åŒ–**ï¼šé˜²æ­¢ä¸“å®¶åç¼©ï¼Œè´Ÿè½½å‡è¡¡
   - **æ ¸å¿ƒåˆ›æ–°**ï¼š
     - Triton JITç¼–è¯‘çš„é«˜æ•ˆGPU kernelï¼ˆæœ€å…³é”®ï¼‰
     - åŒé‡MoEï¼ˆæ³¨æ„åŠ›+FFNéƒ½ç”¨ä¸“å®¶ï¼‰
     - Sigmoidè€ŒéSoftmaxï¼ˆéé›¶å’Œåšå¼ˆï¼‰
     - ç¨€ç–è®¡ç®—ä¼˜åŒ–ï¼ˆå‰å‘å’Œåå‘éƒ½æ˜¯ç¨€ç–çš„ï¼‰

5. **MoEUT è®­ç»ƒè¯¦è§£**
   - **æŸå¤±å‡½æ•°è®¾è®¡**ï¼š
     - åŸºç¡€æŸå¤±ï¼šäº¤å‰ç†µ
     - **ä¸¤ç§è®­ç»ƒæ¨¡å¼**ï¼ˆé‡è¦æ¾„æ¸…ï¼‰ï¼š
       - `loss_on_target_only=False`ï¼šæ•´ä¸ªåºåˆ—éƒ½è®¡ç®—æŸå¤±ï¼ˆQ+Aéƒ½ç®—ï¼‰
       - `loss_on_target_only=True`ï¼š**åªåœ¨ç­”æ¡ˆéƒ¨åˆ†è®¡ç®—æŸå¤±**
       - âš ï¸ **ä¸æ˜¯**"æ¯å±‚æŸå¤± vs æœ€ç»ˆæŸå¤±"ï¼Œè€Œæ˜¯**åºåˆ—çš„å“ªéƒ¨åˆ†è®¡ç®—æŸå¤±**
       - Answer-onlyæ¨¡å¼æ­ç¤ºæ·±å±‚æ¨ç†èƒ½åŠ›ï¼šæ ‡å‡†Transformer 48% â†’ MoEUT 63% (+15%ï¼‰ğŸ”¥
     - **ç†µæ­£åˆ™åŒ–æŸå¤±**ï¼ˆMoEUTç‰¹æœ‰ï¼‰ï¼š
       - FFNç†µæ­£åˆ™ï¼šæƒé‡0.01
       - æ³¨æ„åŠ›ç†µæ­£åˆ™ï¼šæƒé‡0.001
       - **ç›®æ ‡**ï¼šæœ€å¤§åŒ–ä¸“å®¶é€‰æ‹©åˆ†å¸ƒçš„ç†µï¼ˆå¤šæ ·æ€§ï¼‰
       - **ä¸ä¼ ç»ŸLoad Balancingçš„å·®å¼‚**ï¼š
         - ä¼ ç»Ÿï¼šç¡¬çº¦æŸï¼Œå¼ºåˆ¶æ¯ä¸ªä¸“å®¶å¤„ç†ç›¸åŒæ•°é‡tokenï¼ˆå¯èƒ½åˆ†é…ä¸åˆé€‚ï¼‰
         - MoEUTï¼šè½¯çº¦æŸï¼Œé¼“åŠ±å¤šæ ·æ€§ä½†å…è®¸çµæ´»é€‰æ‹©ï¼ˆæ”¯æŒè¿­ä»£é‡ç”¨ï¼‰
       - **å…³é”®ä½œç”¨**ï¼šé˜²æ­¢ä¸“å®¶åç¼© + æ”¯æŒåŒä¸€ä¸“å®¶å¤šæ¬¡è°ƒç”¨ï¼ˆé…åˆSigmoidè·¯ç”±ï¼‰
   - **ä¼˜åŒ–å™¨**ï¼šAdamW + ZeroRedundancyOptimizerï¼ˆåˆ†å¸ƒå¼ï¼‰
   - **å­¦ä¹ ç‡**ï¼šCosine Annealing + 4000æ­¥ Warmup
   - **æ··åˆç²¾åº¦**ï¼šBFloat16ï¼ˆæ¯”FP16æ›´ç¨³å®šï¼‰
   - **å¾®æ‰¹æ¬¡**ï¼š2ä¸ªå¾®æ‰¹æ¬¡ç´¯ç§¯ï¼ˆèŠ‚çœå†…å­˜ï¼‰
   - **å…³é”®å·®å¼‚**ï¼š
     - ç†µæ­£åˆ™åŒ–é˜²æ­¢ä¸“å®¶åç¼©ï¼ˆvs ä¼ ç»ŸLoad Balancingï¼‰
     - Sigmoidè·¯ç”±å…è®¸è¿­ä»£é‡ç”¨ä¸“å®¶ï¼ˆvs Softmaxé›¶å’Œåšå¼ˆï¼‰
     - ç¨€ç–æ¢¯åº¦ï¼ˆåªæ›´æ–°é€‰ä¸­ä¸“å®¶ï¼‰
     - å‚æ•°å…±äº«å¯¼è‡´æ¢¯åº¦å¤šæ¬¡ç´¯ç§¯
     - éœ€è¦å¾®æ‰¹æ¬¡å¤„ç†å¤§é‡ä¸“å®¶å‚æ•°
   - **è®­ç»ƒæ•ˆæœ**ï¼š
     - Answer-only æ¨¡å¼ï¼š63% vs 48%ï¼ˆ+15%ï¼‰
     - è¯æ˜çœŸæ­£å­¦ä¼šäº†"æ½œç©ºé—´è¿­ä»£æ¨ç†"
     - ç”¨æ›´å°‘æœ‰æ•ˆå‚æ•°è¾¾åˆ°æ›´å¥½æ€§èƒ½

6. **å¤§è„‘ç±»æ¯”çš„ä¿®æ­£**
   - **ä¼ ç»Ÿè¯¯è§£**ï¼šæ¯å±‚Transformer = ä¸åŒå±‚æ¬¡çš„è„‘åŒº
   - **æ­£ç¡®ç†è§£**ï¼šå•å±‚MoE = æ•´ä¸ªå¤§è„‘åŠŸèƒ½æ¨¡å—é›†åˆ
   - **ç²¾ç¡®ç±»æ¯”**ï¼š
     - æ¯ä¸ªä¸“å®¶ â‰ˆ æŸä¸ªä¸“é—¨è„‘åŒºï¼ˆæ•°å­¦åŒºã€è¯­è¨€åŒºã€é€»è¾‘åŒºç­‰ï¼‰
     - å‘é‡ = ç”µä¿¡å·æ¨¡å¼ï¼ˆä¸æ˜¯tokenï¼‰
     - é‡å¤ä½¿ç”¨ = è¿­ä»£æ€è€ƒï¼ˆåå¤æ¿€æ´»ä¸åŒè„‘åŒºï¼‰
     - åŠ¨æ€è·¯ç”± = æ ¹æ®ä»»åŠ¡é€‰æ‹©æ¿€æ´»å“ªäº›è„‘åŒº
   - **æ ¸å¿ƒæ´å¯Ÿ**ï¼šMoEUTæ˜¯"å¤§è„‘åœ¨ä¸æ–­è°ƒç”¨ä¸åŒè„‘åŒºå¤„ç†åŒä¸€ä¸ªé—®é¢˜"
   - **å…³é”®å·®å¼‚**ï¼šæ¨ç†æ—¶å‚æ•°æ˜¯å¦æ”¹å˜ï¼ˆTransformerå†»ç»“ï¼Œå¤§è„‘å¯å¡‘ï¼‰

7. **å®ç°ç»†èŠ‚çš„æ·±åˆ»ç†è§£**
   - **ä¸ºä»€ä¹ˆå¿«**ï¼šcvmmåªè®¡ç®—é€‰ä¸­çš„kä¸ªä¸“å®¶ï¼ˆk=8, æ€»å…±128ä¸ªï¼‰
   - **å¦‚ä½•é€‰æ‹©**ï¼šSigmoidç‹¬ç«‹è¯„åˆ† + TopKï¼Œå…è®¸åŒä¸€ä¸“å®¶è¢«å¤šæ¬¡è°ƒç”¨
   - **å¦‚ä½•å¹³è¡¡**ï¼šç†µæ­£åˆ™åŒ–é¼“åŠ±ä¸åŒtoken/å±‚ä½¿ç”¨ä¸åŒä¸“å®¶
   - **å¦‚ä½•å…±äº«**ï¼šgroup_sizeæ§åˆ¶ç‰©ç†å±‚æ•°ï¼Œn_repeatsæ§åˆ¶é‡å¤æ¬¡æ•°
   - **å¦‚ä½•è®­ç»ƒ**ï¼š
     - å¾®æ‰¹æ¬¡ç´¯ç§¯èŠ‚çœå†…å­˜
     - BFloat16æé«˜ç¨³å®šæ€§
     - ç†µæ­£åˆ™åŒ–é˜²æ­¢åç¼©
     - ç¨€ç–æ¢¯åº¦åªæ›´æ–°é€‰ä¸­ä¸“å®¶
   - **ä»£ç éªŒè¯**ï¼š
     ```python
     # å‚æ•°å…±äº«
     self.layers = [create_layer() for _ in range(2)]  # åª2å±‚
     for r in range(8):  # é‡å¤8æ¬¡
         for layer in self.layers:
             x = layer(x)  # åŒä¸€å±‚ï¼Œä¸åŒä¸“å®¶ç»„åˆ
     ```

**æœ€ç»ˆæ´å¯Ÿ**ï¼š
> æ·±åº¦ä¸ç­‰äºå±‚æ•°ï¼Œè€Œæ˜¯æœ‰æ•ˆè®¡ç®—æ­¥éª¤æ•°ã€‚å½“å‰æ ‡å‡†Transformeråœ¨"æ‹‰ä¼¸"è®¡ç®—è€Œé"æ·±åŒ–"è®¡ç®—ã€‚MoEUTé€šè¿‡å‚æ•°å…±äº«+ä¸“å®¶æ··åˆï¼Œå®ç°äº†çœŸæ­£çš„è¿­ä»£æ¨ç†ï¼Œç±»ä¼¼å¤§è„‘åœ¨æ½œç©ºé—´ä¸­åå¤è°ƒç”¨ä¸åŒåŠŸèƒ½åŒºå¤„ç†åŒä¸€é—®é¢˜ã€‚è®­ç»ƒæ—¶é€šè¿‡ç†µæ­£åˆ™åŒ–å’Œç¨€ç–æ¢¯åº¦ï¼Œç¡®ä¿æ‰€æœ‰ä¸“å®¶éƒ½èƒ½å¾—åˆ°æœ‰æ•ˆè®­ç»ƒï¼Œæœ€ç»ˆç”¨æ›´å°‘çš„æœ‰æ•ˆå‚æ•°è¾¾åˆ°æ›´å¥½çš„å¤–æ¨æ€§èƒ½ã€‚

**æŠ€æœ¯çªç ´ç‚¹**ï¼š
1. **cvmmç¨€ç–è®¡ç®—**æ˜¯MoEUTé«˜æ•ˆçš„æ ¹æœ¬åŸå› ï¼ˆ16å€åŠ é€Ÿï¼‰
2. **Sigmoidè·¯ç”±**å…è®¸è¿­ä»£æ¨ç†ï¼ˆåŒä¸€ä¸“å®¶å¯è¢«å¤šæ¬¡è°ƒç”¨ï¼‰
3. **å‚æ•°å…±äº«**å®ç°äº†"æ½œç©ºé—´CoT"ï¼ˆå‘é‡ç©ºé—´çš„æ€ç»´é“¾ï¼‰
4. **åŒé‡MoE**æä¾›äº†æ›´ç»†ç²’åº¦çš„ä¸“ä¸šåŒ–ï¼ˆæ³¨æ„åŠ›å’ŒFFNéƒ½ç”¨ä¸“å®¶ï¼‰
5. **ç†µæ­£åˆ™åŒ–**æ˜¯è®­ç»ƒç¨³å®šçš„å…³é”®ï¼ˆé˜²æ­¢ä¸“å®¶åç¼©ï¼‰
6. **BFloat16 + å¾®æ‰¹æ¬¡**è§£å†³äº†å†…å­˜çˆ†ç‚¸é—®é¢˜
7. **ç¨€ç–æ¢¯åº¦ + è¶³å¤Ÿè®­ç»ƒ**ç¡®ä¿æ‰€æœ‰ä¸“å®¶éƒ½å¾—åˆ°å……åˆ†è®­ç»ƒ

**å®éªŒéªŒè¯**ï¼ˆè®ºæ–‡æ ¸å¿ƒç»“æœï¼‰ï¼š
```
è®­ç»ƒæ¨¡å¼ï¼šAnswer-onlyï¼ˆåªåœ¨ç­”æ¡ˆéƒ¨åˆ†è®¡ç®—æŸå¤±ï¼‰
æ ‡å‡† Transformerï¼š48% å¤–æ¨å‡†ç¡®ç‡
MoEUTï¼š63% å¤–æ¨å‡†ç¡®ç‡ï¼ˆ+15%ï¼‰ğŸ”¥

è¯´æ˜ï¼šMoEUT çœŸæ­£å­¦ä¼šäº†è¿­ä»£æ¨ç†
ä¸éœ€è¦çœ‹é—®é¢˜ï¼Œå°±èƒ½åœ¨æ½œç©ºé—´ä¸­è¿›è¡Œæ·±å±‚è®¡ç®—
```

---

*æ–‡æ¡£åˆ›å»ºæ—¶é—´ï¼š2025-12-15*
*æ–‡æ¡£æ›´æ–°æ—¶é—´ï¼š2025-12-15*
*æ›´æ–°å†…å®¹ï¼š*
- *æ·»åŠ  MoEUT ä»£ç å®ç°è¯¦è§£ï¼ˆ3.7èŠ‚ï¼‰*
- *æ·»åŠ  MoEUT è®­ç»ƒè¯¦è§£ï¼ˆ3.8èŠ‚ï¼‰*
- *æ·»åŠ ç²¾ç¡®çš„å¤§è„‘ç±»æ¯”ï¼ˆ4.5èŠ‚ï¼‰*
- *æ·»åŠ è®­ç»ƒæŸå¤±å‡½æ•°çš„ä¸¤ç§æ¨¡å¼è¯¦ç»†è§£é‡Šï¼ˆ3.8.1èŠ‚ï¼‰*
- *æ·»åŠ ç†µæ­£åˆ™åŒ–çš„å½¢è±¡è§£é‡Šå’Œä¸ä¼ ç»Ÿæ–¹æ³•çš„å¯¹æ¯”ï¼ˆ3.8.1èŠ‚ï¼‰*
- *å®Œå–„è®¨è®ºæ€»ç»“ï¼ˆç¬¬8èŠ‚ï¼‰*

**å…³é”®æ¾„æ¸…**ï¼š
1. **ä¸¤ç§è®­ç»ƒæ¨¡å¼**ä¸æ˜¯"æ¯å±‚æŸå¤± vs æœ€ç»ˆæŸå¤±"ï¼Œè€Œæ˜¯"åœ¨åºåˆ—çš„å“ªéƒ¨åˆ†è®¡ç®—æŸå¤±"ï¼ˆQuestion+Answer vs Answer-onlyï¼‰
2. **ç†µæ­£åˆ™åŒ–**ä¸åŒäºä¼ ç»ŸLoad Balancingï¼šæ˜¯è½¯çº¦æŸè€Œéç¡¬çº¦æŸï¼Œé…åˆSigmoidè·¯ç”±å…è®¸è¿­ä»£é‡ç”¨åŒä¸€ä¸“å®¶
3. **Answer-onlyæ¨¡å¼çš„é‡è¦æ€§**ï¼šè¯æ˜MoEUTçœŸæ­£å­¦ä¼šäº†"æ½œç©ºé—´è¿­ä»£æ¨ç†"ï¼Œè€Œéä¾èµ–å¤–åŒ–çš„æ¨ç†è¿‡ç¨‹

*è®¨è®ºå‚ä¸è€…ï¼šç”¨æˆ· & Claude (Sonnet 4.5)*

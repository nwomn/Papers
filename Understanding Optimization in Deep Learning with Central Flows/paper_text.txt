Understanding Optimization in Deep Learning with Central Flows
Jeremy Cohen*
Carnegie Mellon and Flatiron Institute
jmcohen.github.io
Ameet Talwalkar
Carnegie Mellon University

Alex Damian*
Princeton University
alex-damian.github.io
Jason D. Lee
Princeton University

J. Zico Kolter
Carnegie Mellon University

Traditional theories of optimization cannot describe the dynamics of optimization in deep learning, even in the
simple setting of deterministic training. The challenge is that optimizers typically operate in a complex, oscillatory
regime called the edge of stability. In this paper, we develop theory that can describe the dynamics of optimization
in this regime. Our key insight is that while the exact trajectory of an oscillatory optimizer may be challenging to
analyze, the time-averaged (i.e. smoothed) trajectory is often much more tractable. To analyze an optimizer, we
derive a differential equation called a central flow that characterizes this time-averaged trajectory. We empirically
show that these central flows can predict long-term optimization trajectories for generic neural networks with a high
degree of numerical accuracy. By interpreting these central flows, we are able to understand how gradient descent
makes progress even as the loss sometimes goes up; how adaptive optimizers “adapt” to the local loss landscape;
and how adaptive optimizers implicitly navigate towards regions where they can take larger steps. Our results
suggest that central flows can be a valuable theoretical tool for reasoning about optimization in deep learning.

1

Introduction

While there is a rich body of work on the theory of optimization, few works attempt to analyze optimization in “real”
deep learning settings. Instead, even works motivated by deep learning often rely on unrealistic assumptions such as
convexity, or restrict their analyses to simplified models. Practitioners cannot use such theories to reason directly
about their optimization problems. Our goal in this paper is to develop optimization theory that applies directly to
deep learning problems. This is a difficult task: prior research has shown that, even in the seemingly simple setting of
deterministic (i.e. full-batch) training, optimization typically operates in a complex, oscillatory regime called the edge
of stability (EOS) (Xing et al., 2018; Wu et al., 2018; Jastrz˛ebski et al., 2019, 2020; Cohen et al., 2021, 2022). The
dynamics of optimization in this regime cannot be captured by traditional optimization theory.

Gradient Descent on ResNet
train loss

arXiv:2410.24206v2 [cs.LG] 25 Sep 2025

Abstract

= 2/200
= 2/150
= 2/100
central flows

1.2
1.0
0.8
0.6
0.4
0.2

2.5

Gradient Descent on Transformer
= 2/200
= 2/150
= 2/100
central flows

2.0
1.5
1.0
0.5

0

500

1000

step

1500

2000

0.0

0

1000

2000

step

3000

4000

1.4
1.2
1.0
0.8
0.6
0.4
0.2

RMSProp on Mamba
= 7e-06
= 1e-05
= 2e-05
central flows

0

1000

2000

3000

step

4000

5000

Figure 1: Our theory accurately predicts long-term optimization trajectories of practical neural networks. We
hold our theory to the high standard of rendering accurate numerical predictions about the optimization of practical
(i.e. non-toy) neural networks. For example, this figure shows that our central flows can accurately predict the
time-averaged (smoothed) loss curves of gradient descent and RMSProp on various practical architectures.
*
Equal contribution; author ordering determined by coin flip over a Zoom call (Kingma and Ba, 2015). Please direct correspondence to
both the first authors (see websites for latest contact information). Alex Damian is now at Harvard and Jason D. Lee is now at U.C. Berkeley.
This is the full version of a paper that was published at ICLR 2025.

1

Figure 2: The central flow models the time-averaged (i.e. smoothed) trajectory of the oscillatory optimizer. In
this illustrative cartoon of the weight-space dynamics, gradient descent (blue) takes an oscillatory path through weight
space. The central flow (black) is a smooth curve that characterizes this trajectory, whereas gradient flow (red) takes
a different path. As illustrated in the inset, an oscillatory optimizer can be visualized as moving through a “valley”
while bouncing between the “valley walls” (Xing et al., 2018; Cohen et al., 2021; Wen et al., 2025).
In this paper, we devise a methodology for analyzing these oscillatory deep learning dynamics. Our key insight is
that while the fine-grained trajectory of an oscillatory optimizer may be challenging to analyze, the time-averaged
(i.e. locally smoothed) trajectory is often much more tractable. To analyze an optimization algorithm, we derive a
differential equation called a central flow which explicitly captures this time-averaged trajectory (Figure 2). Being a
smooth curve, the central flow is a simpler object than the original oscillatory trajectory. Hence, by interpreting the
central flow, we can reason more easily about the original optimizer.
We start in Section 3 by analyzing gradient descent, the simplest optimizer. We first explain why traditional analyses
cannot capture the typical dynamics of gradient descent in deep learning, and we then present a new analysis that does
capture these dynamics. The product of this analysis is a central flow. We use this central flow to understand various
aspects of gradient descent’s behavior, such as how the train loss can behave non-monotonically over the short-term
while nevertheless decreasing over the long term. We then examine a simple adaptive optimizer in Section 4, before
turning to RMSProp (i.e. Adam without momentum) in Section 5. We show that much of the behavior of these
optimizers is actually implicit in their oscillatory dynamics, and we render such behaviors explicit via our central flow
analysis. In particular, our central flows reveal how these adaptive optimizers: (1) implicitly adapt their step size(s) to
the local curvature, and (2) implicitly steer towards lower-curvature regions where they can take larger steps.
We focus in this paper on the simple, idealized setting of deterministic (i.e. full-batch) training. However, we
emphasize that similar optimization dynamics have been observed in the practical stochastic setting (Jastrz˛ebski et al.,
2019, 2020; Andreyev and Beneventano, 2024). We view our analysis of deterministic optimization as a necessary
stepping stone to a subsequent analysis of stochastic optimization.
While we derive each central flow using informal mathematical reasoning, we show that these flows can accurately
predict long-term optimization trajectories in a variety of deep learning settings — a high standard of empirical proof.
Thus, we believe that central flows hold promise as a framework for analyzing, reasoning about, and perhaps even
inventing, deep learning optimizers.
We strongly encourage readers to look at the companion blog version of this paper for an interactive
exposition with animations, as well as the accompanying code we used to simulate the central flows.1

1

The blog version is centralflows.github.io and the code is at github.com/centralflows/centralflows.

2

Table of Contents
1

Introduction

1

2

Related Work

4

3

Gradient Descent
3.1 The Dynamics of Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Deriving the Gradient Descent Central Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 Understanding Gradient Descent via its Central Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5
5
9
18

4

Scalar RMSProp
4.1 The Dynamics of Scalar RMSProp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Deriving the Scalar RMSProp Central Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3 Understanding Scalar RMSProp via its Central Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

20
21
21
22

5

RMSProp
5.1 The Dynamics of RMSProp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Deriving the RMSProp Central Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.3 Understanding RMSProp via its Central Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

25
26
26
28

6

Experiments
6.1 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

32
32

7

Discussion
7.1 Modeling decisions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.2 Takeaways from our analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

34
34
34

8

Limitations

35

9

Conclusion

35

A Central Flow Derivations
A.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.2 Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.3 Scalar RMSProp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.4 RMSProp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.5 General Class of Adaptive Preconditioned Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.6 Differential Complementarity Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.7 Continuous-time approximation to an EMA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.8 Miscellaneous math . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

44
44
48
57
64
71
75
79
80

B Experimental Details
B.1 Implementation details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.2 Architecture details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.3 Dataset details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

82
82
84
85

C Miscellaneous
C.1 Implicit gradient regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
C.2 Failure mode: higher-order terms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

86
86
89

D Supplementary Figures

91

E Bulk Experimental Data
109
E.1 Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
E.2 Scalar RMSProp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
E.3 RMSProp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148

3

2

Related Work

Edge of stability The dynamics of optimization in deep learning remain poorly understood, even in the seemingly
simple setting of deterministic (i.e. full-batch) training. Indeed, recent research showed that gradient descent on
neural networks typically operates in a regime termed the “edge of stability” (EOS) in which (1) the largest Hessian
eigenvalue equillibrates around the critical threshold 2/η, and (2) the algorithm oscillates along high-curvature
directions without diverging (Xing et al., 2018; Wu et al., 2018; Jastrz˛ebski et al., 2019, 2020; Cohen et al., 2021).
These dynamics could not be explained by existing optimization theory, which led Cohen et al. (2021) to observe that
there was no explanation for how or why gradient descent can function properly in deep learning.
Subsequently, several studies sought to theoretically explain EOS dynamics. Some works rigorously analyzed EOS
dynamics on specific objective functions (Agarwala et al., 2023; Ahn et al., 2024; Chen and Bruna, 2023; Even et al.,
2024; Kreisler et al., 2023; Song and Yun, 2023; Li et al., 2022b; Wu et al., 2024; Zhu et al., 2023), while other works
(Arora et al., 2022; Lyu et al., 2022; Damian et al., 2023) gave generic analyses based on a local third-order Taylor
expansion of the loss, which is one order higher than is normally used in the theoretical analysis of gradient descent.
Similar arguments were first used by Blanc et al. (2019) to study implicit regularization in SGD. Our work is most
directly inspired by Damian et al. (2023), as their analysis applies to generic objective functions, and holds throughout
training, not just near convergence. However, whereas they analyze the fine-grained oscillatory dynamics, we argue
that analyzing the time-averaged dynamics is simpler, and is sufficient for most purposes.
Continuous-time models for optimization The standard continuous-time model for gradient descent is the gradient
flow. Barrett and Dherin (2021); Smith et al. (2021) argued that gradient descent is, instead, better approximated
by a modified gradient flow that is augmented with a penalty on the squared gradient norm. We find that on deep
learning objectives, this modified flow improves slightly over gradient flow in the stable regime, but fails in the
edge of stability regime, where most of the discrepancy between gradient descent and gradient flow originates (see
Section C.1). Rosca et al. (2023) proposed a flow that can model oscillations by using complex numbers. However,
this flow still cannot track the long-term trajectory of gradient descent in EOS regime.
Many works propose to model the dynamics of stochastic optimizers using stochastic differential equations (SDEs) (Li
et al., 2017; Li et al., 2021, Malladi et al., 2022; Compagnoni et al., 2023, 2025). In the full batch limit, where SGD
reduces to gradient descent, these SDEs reduce to gradient flow, which is a poor approximation to gradient descent
at the edge of stability. Thus, these SDEs cannot be accurate in all hyperparameter regimes. Further, even when
these SDEs do well-approximate the real optimizer trajectory, the SDE trajectories are themselves oscillatory, and
accordingly can possess behaviors that are implicit in the oscillatory dynamics. Our central flows, by contrast, average
out the oscillations and render all such behaviors explicit. Developing an analogue of the central flow for stochastic
optimization is an interesting open question (see Section 7). While some works do aim to explicitly characterize the
time-averaged trajectory of SGD (Blanc et al., 2019; Damian et al., 2021; Li et al., 2022a), existing analyses only
apply in limiting regimes (e.g. η → 0), and only when the loss is already near zero.
Understanding adaptive optimizers Ma et al. (2022) observed that RMSProp and Adam oscillate, and Cohen
et al. (2022) showed that such dynamics can be viewed as an adaptive version of the edge of stability.Khaled et al.
(2023) and Mishkin et al. (2024) observed that on quadratic functions, certain adaptive optimizers implicitly adapt
their effective step size to the maximum stable step size; we show this holds more generally, beyond quadratics. The
phenomenon we call “acceleration via regularization.” explains experiments in Roulet et al. (2024) and Wang et al.
(2024d). Many works have also conducted rigorous convergence analyses of adaptive optimizers, generally focused
on deriving rates of convergence to a global minimizer or stationary point (Duchi et al., 2011; Reddi et al., 2018; Chen
et al., 2019a,b; Zaheer et al., 2018; Zou et al., 2019; Défossez et al., 2022; Li and Lin, 2024; Chen et al., 2022; Wang
et al., 2024a; Yang et al., 2024; Guo et al., 2021; Shi et al., 2021; Zhang et al., 2022; Crawshaw et al., 2022; Li et al.,
2024; Wang et al., 2024b; Hong and Lin, 2024; Zhang et al., 2025; Wang et al., 2024c; Hübler et al., 2024).
Dynamical systems Our work likely has rich connections to various topics from the theory of dynamical systems
such as the method of averaging and slow-fast systems (e.g., Guckenheimer and Holmes, 1983). We hope that these
connections can be explored by future work.
4

3

Gradient Descent

The simplest first-order optimizer is deterministic gradient descent with a fixed learning rate η:
wt+1 = wt − η∇L(wt ).

(1)

Perhaps surprisingly, Cohen et al. (2021) showed that traditional optimization analyses cannot capture the typical
dynamics of gradient descent in deep learning. We now present a new analysis that does capture these dynamics.
• In Section 3.1, we describe the typical dynamics of gradient descent in deep learning, and we explain why these
oscillatory edge of stability dynamics cannot be captured by traditional optimization theory.
• In Section 3.2, we show that while the exact oscillatory trajectory may be hard to analyze, the time-averaged
trajectory is more tractable. We derive a central flow that characterizes this time-averaged trajectory.
• In Section 3.3 we use this central flow to understand the behavior of gradient descent. For example, we show
that while gradient descent’s loss curve is non-monotonic, it can be viewed as the superposition of the loss
along the central flow, plus a contribution from the oscillations. The central flow loss is a smoothly varying
quantity that monotonically decreases, and therefore constitutes a hidden progress metric for gradient descent.
Our analysis of gradient descent will set the stage for subsequent analyses of more complex optimizers.

3.1

The Dynamics of Gradient Descent

To understand the oscillatory dynamics of gradient descent in deep learning, it is instructive to first consider the case of
quadratic objective functions. On quadratic functions, gradient descent oscillates if the curvature (i.e. Hessian) is too
large relative to the learning rate. For example, consider a one-dimensional quadratic objective L(x) = 21 Sx2 , which
has global curvature S. Under gradient descent with learning rate η, the iterates {xt } evolve via xt+1 = (1 − ηS)xt . If
S exceeds the critical threshold 2/η, then (1 − ηS) < −1, so the iterate xt flips signs and grows in magnitude at each
step, i.e. gradient descent oscillates with exponentially growing magnitude, as shown in Figure 3. More generally, on
a quadratic objective in multiple dimensions, the curvature is quantified by the Hessian matrix, and gradient descent
oscillates with exponentially growing magnitude along Hessian eigenvectors with eigenvalues exceeding 2/η.2
Of course, deep learning objectives L(w) are not globally quadratic. Still, at any point w in weight space, the objective
can be locally approximated by a quadratic Taylor expansion around w. The dynamics of gradient descent on this
quadratic are controlled by the largest eigenvalue of the Hessian H(w), which we call the sharpness S(w):
S(w) := λ1 (H(w)).

(2)

If the sharpness S(w) exceeds 2/η, then gradient descent on the quadratic Taylor approximation would oscillate with
exponentially growing magnitude along the top Hessian eigenvector(s). This argument suggests that gradient descent
cannot function properly in regions of weight space where the sharpness S(w) exceeds 2/η.

stable

unstable

Figure 3: Gradient descent on a quadratic function. Consider gradient descent with learning rate η on a quadratic
function 21 Sx2 , with sharpness S. If S > 2/η, gradient descent oscillates with exponentially growing magnitude.
2

For an explicit derivation of this well-known fact, see Cohen et al. (2021, Proposition 1). Note that an exception is if the initial iterate has
exactly zero alignment with these Hessian eigenvectors. However, this event has probability zero under any typical random initialization.

5

(a) Expectation: gradient descent stays throughout
training inside the stable region (gray), the subset of
weight space where the sharpness is bounded by 2/η.

(b) Reality: gradient descent frequently exits the stable region, but dynamically steers itself back inside.

Figure 4: Why does gradient descent converge in deep learning? The reality (right) is dramatically different from
the picture suggested by traditional theory (left).
In light of this discussion, why does gradient descent converge in deep learning? The natural explanation is that the
sharpness remains below 2/η throughout training. In other words, if we define the “stable region” {w : S(w) ≤ 2/η}
as the subset of weight space where the sharpness is bounded by 2/η, then one might suppose that gradient descent
remains inside the stable region throughout training, as depicted in the cartoon Figure 4(a). This is the picture
suggested by traditional analyses of gradient descent.3
Yet, Cohen et al. (2021) observed a very different reality when training neural networks using gradient descent.
Figure 5 depicts a typical gradient descent trajectory, with important events annotated a - g . Initially, the sharpness
rises a .4 Indeed, it is a robust empirical phenomenon, dubbed progressive sharpening, that the sharpness tends to
rise when training neural networks.56 Soon enough, the sharpness rises past the critical threshold 2/η. Once this
happens, gradient descent begins to oscillate with growing magnitude along the top Hessian eigenvector,7 just as
one would predict from a quadratic Taylor approximation b . These oscillations grow large enough that the train
loss starts to go up rather than down c . Yet, gradient descent does not diverge. Instead, something odd happens:
as if “by magic,” the sharpness rapidly drops d . Indeed, it drops all the way below the critical threshold 2/η,
after which point the oscillations start to shrink in magnitude e , as one would expect from a new quadratic Taylor
approximation. The unexplained rapid drop in the sharpness has conveniently prevented gradient descent from
diverging.8 Similar dynamics recur throughout the rest of training: gradient descent oscillates without diverging along
the highest-curvature direction(s),9 as the sharpness stays dynamically regulated around the critical threshold 2/η f .
Meanwhile, the train loss decreases over the long run, but behaves non-monotonically over the short run g .
Intuitively, whereas the traditional theory implies that gradient descent remains inside the stable region throughout
training, as in Figure 4(a), in reality gradient descent is frequently exiting the stable region, but is somehow steering
itself back inside, as in Figure 4(b). Cohen et al. (2021) dubbed these dynamics edge of stability (EOS), and noted
that they could not be explained by traditional optimization theory.
3

We are referring to analyses which assume L-smoothness, i.e. Lipschitzness of the gradient / boundedness of the Hessian spectral norm.
This assumption is usually stated as a global condition, but analyses generally only require it to hold locally, in the vicinity of the trajectory.
4
Throughout this paper, we report the Hessian eigenvalues measured not at the iterates themselves, but rather at the second-order midpoints
between the iterates (see Section B.1). This results in plots that are slightly crisper, while retaining all essential features.
5
Progressive sharpening remains theoretically unexplained. Our goal in this paper is not to understand the origin of progressive sharpening
in deep learning, but rather to understand the dynamics of gradient descent on objective functions which may or may not possess this property.
6
In the literature, progressive sharpening has also been called a “narrowing valley” (Liu et al., 2025a,b) and “lower loss as sharper” loss
landscape structure (Li et al., 2023; Bai et al., 2025).
7
To compute “displacement along top Hessian eigenvector” for Figure 5, we let t0 be the first step of the figure (i.e. step 2990), we let u be
the top Hessian eigenvector computed at step t0 , and we report u⊤ (wt − wt0 ).
8
This process is similar to the “catapult” phenomenon observed in Lewkowycz et al. (2020) at initialization. Indeed, the EOS dynamics
with one unstable eigenvalue resemble a sequential series of catapults. However, the dynamics with >1 unstable eigenvalues are more complex.
9
As gradient descent oscillates along the high-curvature directions, it can be visualized as moving through a “valley” while bouncing
between the “walls” of the valley (Xing et al., 2018; Cohen et al., 2021; Wen et al., 2025).

6

Complete trajectory

f
a
g

Zoom-in on start of EOS (steps 2990-3090)

e

b

d

c

Figure 5: A typical gradient descent trajectory in deep learning. We train a neural network using gradient
descent with step size η = 0.01. The top row shows the long-term trajectory, while the bottom row zooms in on a
particular time segment. (a) The sharpness rises, reaching the critical threshold 2/η around step 2900. (b) Once the
sharpness crosses the critical threshold 2/η, gradient descent oscillates with growing magnitude along the top Hessian
eigenvector. (c) These oscillations cause the train loss to go up rather than down. (d) However, gradient descent
does not diverge; instead, as if “by magic”, the sharpness decreases until falling below 2/η. (e) Once the sharpness
is below 2/η, the oscillations shrink. Throughout the rest of training: (f) the sharpness stays regulated around the
critical threshold 2/η and (g) the train loss behaves non-monotonically over short timescales, while decreasing over
long timescales. Details: the network is a Vision Transformer trained on a subset of CIFAR-10 using MSE loss.

Damian et al. (2023) showed that the key for understanding these surprising dynamics is to Taylor-expand the objective
to third order, which is one order higher than traditionally used in analyses of gradient descent. A third-order Taylor
expansion reveals the crucial ingredient missing from traditional optimization theory:
Oscillations along the top Hessian eigenvector automatically trigger reduction of the top Hessian eigenvalue.
Let us informally sketch this argument. Suppose that gradient descent is oscillating around a reference point w, along the top Hessian
eigenvector u, with current magnitude x, so that (illustration on right):
w = w + xu.

w
xu
w

(3)

Due to the oscillation, the optimizer follows the gradient at w rather than the gradient at w. How do the two relate? A
Taylor expansion of ∇L around w yields:
first term

second term

∇L(w + xu) =∇L(w) + xH(w)u +O(x2 ).
| {z }
= xS(w)u

7

(4)

Since u is an eigenvector of H(w) with eigenvalue S(w), we recognize the second term as xS(w)u. This term causes
a negative gradient step computed at w + xu to move in the −u direction. In other words, this term is causing gradient
descent to oscillate back and forth along the top Hessian eigenvector u, as predicted by the traditional theory. The
“magic” comes from the next term, which arises from third-order terms in the Taylor expansion of the loss:
first term

second term

third term
1 2
∇L(w + xu) =∇L(w) + xS(w)u + 2 x ∇w [uT H(w)u] +O(x3 ).

|

{z

(5)

}

1
= 2 x2 ∇S(w)

Since uT H(w)u = S(w), we recognize this term as 12 x2 ∇S(w), where ∇S is none other than the gradient of the
sharpness.10 Thus, a negative gradient step computed at w + xu implicitly takes a negative gradient step on the
sharpness with step size 12 ηx2 . This is the key ingredient missing from the traditional theory. When gradient descent
exits the stable region, it oscillates along the top Hessian eigenvector, just as the traditional theory predicts; but what
the traditional theory fails to anticipate is that these oscillations in turn perform gradient descent on the sharpness,
thereby steering the trajectory back into the stable region automatically.
Note that traditional optimization theory fails to capture the basic causal structure of the optimization process:
gradient descent converges not because the sharpness is “already” small, but rather due to an automatic negative
feedback mechanism that keeps the sharpness small.
Damian et al. (2023) analyzed the EOS dynamics in the special case where only one Hessian eigenvalue has crossed
the critical threshold 2/η, as in steps ~2900-3600 in Figure 6. In this setting, the dynamics consist of consecutive
cycles in which: (1) the sharpness rises above 2/η; (2) this triggers growing oscillations along the top Hessian
eigenvector; (3) such oscillations reduce sharpness via eq. (5), pushing it below 2/η; (4) the oscillations consequently
shrink in magnitude.11 However, a more common situation is when multiple Hessian eigenvalues have reached 2/η,
as in steps ~3600-4000 in Figure 6. Here, gradient descent oscillates simultaneously along all the corresponding
eigenvectors,12 and these oscillations cause all such eigenvalues to remain dynamically regulated around 2/η.
Unfortunately, analyzing EOS dynamics in fine-grained detail is challenging (Damian et al., 2023). The difficulty
arises from the need to account for the mutual interactions between the oscillations and the curvature. Even in the

0.34

stable

train loss
1 unstable

2 unstable

0.32

250

top 3 Hessian eigenvalues

stable

1 unstable

200

2 unstable

gradient norm2 along top eigenvectors
0.5

stable

1 unstable

2 unstable

0.4
eigenvector 1
eigenvector 2
eigenvector 3

0.30

150

0.28

100

0.26

50

0.24
2500 2750 3000 3250 3500 3750 4000
step

0.0
0
2500 2750 3000 3250 3500 3750 4000 2500 2750 3000 3250 3500 3750 4000
step
step

0.3
eigenvalue 1
eigenvalue 2
eigenvalue 3

0.2
0.1

Figure 6: Multiple Hessian eigenvalues can be at the edge of stability. From steps ~2900-3600, one Hessian
eigenvalue is at the edge of stability, and gradient descent oscillates along the top Hessian eigenvector. From steps
~3600-4000, two Hessian eigenvalues are at the edge of stability, and gradient descent oscillates simultaneously along
both the corresponding eigenvectors. The number of oscillating directions can be easily read off from the right plot,
which shows the squared norm of the gradient when projected onto each of the top 3 Hessian eigenvectors.
Technically, equating ∇w [uT H(w)u] = ∇w S(w) requires invoking Danskin’s theorem. This is made precise in Section A.8, Fact 1.
Notice that the drop in the sharpness is rapid, yielding a sawtooth-like plot for the evolution of the sharpness. This is because the strength
of the sharpness reduction effect is proportional to x2 . When x is small, the sharpness-reduction effect is negligible, but when x grows larger,
the effect quickly becomes strong. See Damian et al. (2023) for a simplified ODE model of the joint dynamics between x and sharpness.
12
With k > 1 unstable eigenvalues, the corresponding eigenvectors are not individually identifiable; instead, one should think of gradient
descent as oscillating within the k-dimensional eigenspace spanned by the k eigenvectors at the edge of stability.
10
11

8

train loss

0.35

gradient descent
gradient flow
central flow

0.30

300

0.25

200

0.20

top Hessian eigenvalue
(sharpness)

400

0.30
0.25
0.20
0.15
0.10

100

0.15
0

2000 4000 6000 8000
step / time

0

network output on
arbitrary test example

0

2000 4000 6000 8000
step / time

Figure 7: What macroscopic path does gradient descent take?
Gradient descent (blue) is well-approximated by gradient flow
(red) so long as the sharpness is below 2/η. However, once
gradient descent reaches the edge of stability, it takes a different
path. Our central flow (black) approximates gradient descent
even at the edge of stability. The plots on top present data from
an experiment (same as Figure 5); the drawing on the right is a
cartoon of the underlying weight-space dynamics.

distance to gradient
descent trajectory

1.25
1.00
0.75
0.50
0.25

0

2000 4000 6000 8000
step / time

0.00

0

2000 4000 6000 8000
step / time

{w : S(w) ≤ 2/η}

special case of one unstable eigenvalue, these dynamics are nonlinear and highly sensitive to initial conditions. The
more typical case of multiple unstable eigenvalues is even harder to analyze: the dynamics with k unstable eigenvalues
do not decouple into k independent systems, and instead involve O(k 2 ) mutually interacting quantities, yielding
complex and often chaotic behavior.
Our key insight in this paper is that a fine-grained analysis of the EOS dynamics may not be necessary. Rather,
we argue that the more important question is: what macroscopic (i.e. long-term) trajectory does gradient descent
take through weight space? In the next section, we will use a heuristic time-averaging argument to characterize this
macroscopic trajectory. Our analysis will not only recover the main finding of Damian et al. (2023) for a single
unstable eigenvalue, but will also readily generalize to the challenging setting of multiple unstable eigenvalues.

3.2

Deriving the Gradient Descent Central Flow

The standard continuous-time approximation to gradient descent is the gradient flow:13
dw
= −η∇L(w).
dt

(6)

Cohen et al. (2021) observed that trajectory of gradient descent is well-approximated1415 by that of gradient flow so
long as training is stable, i.e. so long as the sharpness S(w) remains below 2/η. However, once the sharpness reaches
2/η and the dynamics enter the EOS regime, gradient descent departs from the gradient flow trajectory and takes a
different path, as illustrated in Figure 7.16
13

We fold η into the definition of gradient flow so that there is a correspondence between step t of gradient descent and time t of gradient
flow. This will especially be useful when analyzing adaptive optimizers where the effective step size is a dynamic quantity.
14
Barrett and Dherin (2021) argued that the accuracy of the gradient flow approximation can be improved by adding a penalty on the
squared gradient norm. However, their modified flow does not hold in the EOS regime, and in the stable regime, we found that the accuracy
improvement it brings is relatively small (Section C.1). Therefore, for simplicity, we leave out any such term from our flows.
15
It remains theoretically unexplained why gradient flow is such a good fit to gradient descent. Existing bounds for the distance between
gradient descent and gradient flow increase exponentially with time, with an exponent determined by the most negative Hessian eigenvalue
(Elkabetz and Cohen, 2021). Empirically, such bounds are overly conservative.
16
Even in the simplest setting of one unstable eigenvalue, capturing the EOS dynamics necessarily requires three variables: one for the
oscillations along the top Hessian eigenvector, one for the top Hessian eigenvalue (sharpness), and one for the remaining directions. Since
visualizing three-dimensional dynamics is difficult, we will frequently resort to two-dimensional cartoons (e.g. Figure 7). Such a “projection”
will necessarily drop information. Accordingly, Figure 7 captures sharpness and remaining directions, but leaves out the back-and-forth
oscillations along the top Hessian eigenvector. Figure 2, by contrast, captures these back-and-forth oscillations but leaves out the sharpness.

9

We now derive a more general differential equation, which we call a central flow, that approximates the trajectory of
gradient descent even at the edge of stability. The central flow directly models the time-averaged (i.e. smoothed)
trajectory of the oscillatory optimizer. In other words, the central flow averages out the oscillations while retaining
their lasting effect on the macroscopic trajectory. We will derive the central flow using a heuristic time-averaging
argument, and we will empirically demonstrate that it can accurately predict long-term optimization trajectories on a
variety of neural networks with a high degree of numerical accuracy, as illustrated in Figure 7.
We will abuse notation and use E to denote “local time-averages” of deterministic quantities — see Section A.1.5
for discussion. The gradient descent central flow is intended to model the time-averaged trajectory E[wt ]. To
simplify notation, we will also use wt := E[wt ] to denote the time-averaged trajectory.
3.2.1

Warm-up: the Special Case of One Unstable Eigenvalue

We will introduce our time-averaging methodology by analyzing the special case when only the largest Hessian
eigenvalue has crossed the critical threshold 2/η, and gradient descent oscillates along a single direction — the
corresponding eigenvector. Our fully general analysis, given later in Section 3.2.2, will allow for an arbitrary number
of eigenvalues to be at the edge of stability, and for eigenvalues to enter and leave the edge of stability.
Thus, in this section, we start our analysis at the instant when the sharpness S(w) first reaches 2/η. From this point
onward, we will model the gradient descent trajectory by:
wt = wt + xt ut ,

(7)

where wt is the gradient descent iterate, wt is the time-averaged iterate, ut is the top Hessian eigenvector at wt , and
xt denotes the displacement between wt and wt along the ut direction.17 Note that by definition, E[xt ] = 0, i.e. the
time-averaged displacement is zero. To track the evolution of the time-averaged iterate wt , we time-average both
sides of the gradient descent update eq. (1):
wt+1 = wt − η E[∇L(wt )] .
| {z }

(8)

time-averaged gradient

That is, the time-averaged iterates follow the (negative) time-averaged gradient. To approximate the time-averaged
gradient, we first Taylor-expand the gradient ∇L around the time-averaged iterate wt :
∇L(wt ) = ∇L(wt ) + xt S(wt )ut + 12 x2t ∇S(wt ) + O(x3 ).
| {z }
| {z }
|
{z
}
gradient at wt

oscillation

(9)

sharpness reduction

We then take the time average of both sides, averaging over the x oscillations. This reflects an implicit assumption
that the x oscillations are happening fast relative to the remaining training dynamics:18
2
1
((
E[∇L(wt )] ≈ ∇L(wt ) + (
E[x
]S(w
(t(
t )ut + 2 E[xt ]∇S(w t ) .
|
{z
}
|
{z
}

(
(

0 because E[xt ]=0

(10)

implicit sharpness penalty

This calculation shows that the time-averaged gradient E[∇L(wt )] is equal to the gradient at the time-averaged iterate
∇L(wt ), plus an implicit sharpness penalty whose strength is proportional to E[x2t ], the variance of the oscillations
at step t. Substituting eq. (10) into eq. (8) and switching to continuous time, we therefore model the time-averaged
iterates wt by the sharpness-penalized gradient flow w(t) defined by:
h
i
dw
= −η ∇L(w) + 21 σ 2 (t)∇S(w) .
(11)
dt
|
{z
}
implicit sharpness penalty

17

This is a simplification. In reality, we know that gradient descent is displaced from w in at least two directions: the u direction and
the ∇S(w) direction, with the latter responsible for the fluctuations in the sharpness. However, when modeling the time-averaged gradient,
we will only account for the displacement in the u direction. This is analogous to Damian et al. (2023, Assumption 5). The success of our
experiments validates this simplification.
18
When time-averaging eq. (9), we assume that the eigenvector u changes slowly relative to the displacement x so that E[xt ut ] ≈ E[xt ]ut .

10

top Hessian eigenvalue (sharpness)
220

gradient descent
central flow

210
200
190
180

2800

3000

3200 3400
step / time

3600

0.00030
0.00025
0.00020
0.00015
0.00010
0.00005
0.00000

variance of oscillations along
top Hessian eigenvector
displacement2 along top
Hessian eigenvector
its time-average
central flow 2(t)

3000

3200
3400
step / time

3600

0.150
0.125
0.100
0.075
0.050
0.025
0.000

distance between gradient
descent and both flows
gradient flow
central flow

2800

3000

3200 3400
step / time

3600

Figure 8: Illustrating the central flow with one unstable eigenvalue. Left: the sharpness (top Hessian eigenvalue)
cycles around 2/η under gradient descent, and stays capped at 2/η under the central flow. Center: the central flow’s
σ 2 (t) accurately predicts the variance of the oscillations. In light blue, we plot (u(t)⊤ (wt − w(t)))2 , the squared
displacement between gradient descent wt and the central flow w(t) along u(t), the top Hessian eigenvector at w(t).
In dark blue, using Gaussian smoothing, we plot its time average, i.e. the empirical variance of the oscillations.
Observe that this is well-predicted by the central flow’s σ 2 (t), eq. (14), plotted in black. Right: the Euclidean distance
∥wt − w(t)∥ between gradient descent and the central flow (black) stays small over time, indicating that the central
flow accurately predicts the long-term trajectory of gradient descent. In contrast, the distance (red) between gradient
descent and the gradient flow eq. (6) grows large over time. This figure depicts the same ViT trajectory as Figure 5.
Here, σ 2 (t) is a still-unknown quantity intended to model E[x2t ], the instantaneous variance of the oscillations at time
t. This quantity also controls the strength of the implicit sharpness penalty. To determine σ 2 (t), we argue that only
one value is consistent with the observed behavior of gradient descent. Empirically, once the sharpness reaches the
critical threshold 2/η, it does not continue to rise indefinitely; rather, it remains dynamically regulated around 2/η.
Thus, we will enforce that the central flow never increases the sharpness S(w(t)) past 2/η. The time derivative of the
sharpness under a flow of the form eq. (11) can be easily computed using the chain rule:


dw
dS(w)
= ∇S(w),
= η ⟨∇S(w), −∇L(w)⟩ − 21 ησ 2 (t)∥∇S(w)∥2 .
(12)
dt
dt
|
{z
}
{z
}
|
change in sharpness
under gradient flow

sharpness reduction
from oscillations

When the first term, the change in sharpness under the gradient flow, is negative, gradient descent will leave the edge
of stability and will once again follow gradient flow — this is made precise in Section 3.2.2. Therefore, we focus on
the case where this first term is positive, i.e. where progressive sharpening holds. As the sharpness is currently at
dS(w)
2
2/η and must remain at 2/η, we must have that dS(w)
dt = 0. Since dt is affine in σ (t), we can easily solve for the
unique value of σ 2 (t) that ensures dS(w)
dt = 0:
σ 2 (t) =

2 ⟨∇S(w), −∇L(w)⟩
.
∥∇S(w)∥2

(13)

Intuitively, this is the unique σ 2 (t) for which the downward force of oscillation-induced sharpness reduction “cancels
out” the upwards force of progressive sharpening so the sharpness remains locked at 2/η. The central flow for a
single unstable eigenvalue is given by substituting this σ 2 (t) into eq. (11):
h
i
dw
2 ⟨∇S(w), −∇L(w)⟩
= −η ∇L(w) + 21 σ 2 (t) ∇S(w) where σ 2 (t) =
.
(14)
dt
∥∇S(w)∥2
Figure 8 demonstrates this flow in action. We run gradient flow until the sharpness hits 2/η, and then switch to
eq. (14) at that time. (The complete central flow, defined in the next section, will handle such switches automatically.)
Observe that the distance in weight space between gradient descent and the central flow remains small over time,19
19
Observant readers might notice that the distance between gradient descent and the central flow starts to grow once the sharpness hits 2/η
and is actually initially larger than the distance between gradient descent and the gradient flow. This is because the central flow has already
started to apply sharpness regularization, but the discrete gradient descent trajectory has not yet done so.

11

verifying that the central flow accurately predicts the long-term trajectory of gradient descent. Moreover, observe that
σ 2 (t) from eq. (13) accurately predicts the empirical variance of the oscillations along the top Hessian eigenvector,
further demonstrating that our time-averaging argument is accurately capturing gradient descent’s behavior.
Intuitively, whereas gradient descent reduces sharpness in impulse-like spurts which are triggered whenever the
oscillations grow large, the central flow applies a sharpness-reduction force continuously, with the same average
strength over time. That these two processes stay close over long timescales implies that the oscillations are only
affecting the long-term gradient descent trajectory via their variance rather than via their fine-grained details (e.g the
precise shape of the light blue line in Figure 8, center). This is good news: while the fine-grained oscillations may
be challenging to analyze, we have shown that their variance is easy to analyze, as there is only one possible value
that is consistent with the observed edge of stability equilibrium. In this way, we have successfully used a heuristic
argument to solve for the time-averaged trajectory of gradient descent.
Interpretation as Projection While we have derived the central flow as a sharpness-penalized gradient flow, it can
be equivalently interpreted as a projected gradient flow.
In particular, simplifying eq. (14) gives:


dw
∇S(w)∇S(w)⊤
= −η I −
∇L(w) = −ηΠ⊥
∇S(w) ∇L(w),
2
dt
∥∇S(w)∥

(15)

T

vv
where Π⊥
v := I − ∥v∥2 denotes the projection matrix onto the orthogonal
complement of v. This flow projects out the ∇S direction from the gradient ∇L
to keep the sharpness fixed at 2/η, as shown in the cartoon on the right.

Previously, Damian et al. (2023) proved that under certain conditions, gradient Figure 9: The central flow projects
descent at the edge of stability implicitly follows the trajectory of projected gra- out the ∇S(w) direction from the
dient descent constrained to the stable region. We have thus nearly20 rederived loss gradient ∇L(w).
their result in a simpler, albeit non-rigorous, manner.
The projection interpretation will be useful below for reasoning about gradient descent’s behavior.
3.2.2

The Fully General Case

We will now derive the complete central flow, which applies in the fully general setting where any number of
eigenvalues can be at the edge of stability, including zero. When no eigenvalues are at the edge of stability, the central
flow will automatically reduce to the gradient flow. As above, we decompose the gradient descent trajectory as:
wt = w t + δ t ,

(16)

where wt is the gradient descent iterate, wt := E[wt ] is the time-averaged iterate, and δt denotes the displacement
between wt and wt , i.e. the oscillation. Because gradient descent oscillates along the Hessian eigenvectors that are at
the edge of stability, we model δt as lying within the span of these eigenvectors.21 For example, in the case where
only one direction is at the edge of stability, taking δt = xt ut recovers the analysis in Section 3.2.1. Note that by
definition of wt , we have that E[δt ] = 0. As before, the time-averaged iterates follow the time-averaged gradient
E[∇L(wt )]. To compute the time-averaged gradient, we first Taylor-expand the gradient around wt :
∇L(wt ) =

∇L(wt ) + H(wt )δt + 21 ∇wt δtT H(wt )δt + O(∥δt ∥3 ).
| {z }
| {z }
|
{z
}

gradient at w

oscillation

20

(17)

implicit curvature penalty

The flow eq. (15) actually differs slightly from the constrained trajectory in Damian et al. (2023), beyond being continuous rather than
discrete. In Damian et al. (2023), the stable region was defined as the set where S(w) ≤ 2/η and the the loss is directionally minimized along
the top Hessian eigenvector. The latter condition prevents their theory from applying to certain models (e.g. Kreisler et al., 2023, Appendix A).
Our central flow does not use the latter condition and hence does not suffer from such restrictions.
21
Similar to footnote 17, this neglects the motion in the top Hessian eigenvalues. The success of our experiments justifies this simplification.

12

top Hessian eigenvalues

225
200
175
150

gradient descent
central flow

125

2000 2500 3000 3500 4000 4500 5000
step / time

3

rank of (t)

0.25

central flow

distance between gradient
descent and both flows
gradient flow
central flow

0.20

2

0.15

1

0.10
0.05

0

2000 2500 3000 3500 4000 4500 5000
step / time

0.00

2000 2500 3000 3500 4000 4500 5000
step / time

Figure 10: Illustrating the central flow (general case). Left: whenever a Hessian eigenvalue rises to 2/η, the central
flow prevents it from increasing further. Center: since Σ(t) models the covariance of the oscillations, its rank is
always equal to the number of Hessian eigenvalues at the edge of stability. We show in Figure 11 that Σ(t) accurately
predicts the covariance of oscillations. Right: the Euclidean distance between gradient descent wt and the central flow
w(t) stays small over time, indicating that the central flow accurately predicts the long-term trajectory of gradient
descent. In contrast, the distance between gradient descent and the gradient flow eq. (6) grows large over time. This
figure depicts the same ViT trajectory as Figure 5.
The third term in this Taylor expansion reveals that the negative gradient at the iterate wt implicitly acts to decrease
the directional curvature in the direction δt . Time-averaging both sides and rearranging the third term yields:
T
1

E[∇L(wt )] ≈ ∇L(wt ) + 
H(w
t ) E[δt ] + 2 ∇wt H(w t ), E[δt δt ] ,

{z
}
|
|
{z
}



0 because E[δt ]=0

(18)

implicit curvature penalty

where we use ⟨·, ·⟩ to denote the Frobenius inner product between two matrices, equivalent to flattening the matrices
into vectors and taking the dot product. Thus, we see that the time-averaged gradient is the gradient at the timeaveraged iterate, plus an implicit curvature penalty whose strength and direction are determined by the covariance of
the oscillations E[δt δtT ]. Substituting eq. (18) into the time-averaged gradient descent update (eq. 8) and switching to
continuous time, we model the time-averaged iterates wt by a differential equation of the form:
h
i
dw
= −η ∇L(w) + 12 ∇w ⟨H(w), Σ(t)⟩ .
(19)
|
{z
}
dt
implicit curvature penalty

Here, Σ(t) is a still-unknown quantity intended to model E[δt δtT ], the instantaneous covariance of the oscillations at time t. This matrix also controls an implicit curvature penalty which penalizes the Σ-weighted Hessian
⟨Σ(t), H(w)⟩.22 Similar as before, to determine Σ(t), we impose three conditions:
1. Since Hessian eigenvalues which reach the critical threshold 2/η do not continue to rise further, we impose the
condition that Σ(t) should not allow any Hessian eigenvalues to rise beyond 2/η.
2. Since gradient descent oscillates within the span of the unstable eigenvectors, we impose the condition that
Σ(t), which models the covariance of these oscillations, should be supported23 within the span of the Hessian
eigenvectors whose eigenvalue is equal to 2/η.24
3. Since Σ(t) models a covariance matrix, we impose the condition that Σ(t) should be positive semidefinite.
These three conditions turn out to imply a unique value of Σ(t). In particular, we detail in Section A.2 that Σ(t) must
be the unique solution to a type of convex program known as a semidefinite complementarity problem (SDCP), which
22

This is a weighted sum of all entries in the Hessian matrix, where each entry is weighted by the corresponding entry of Σ(t).
We mean that span[Σ] ⊆ U, where U is the span of the Hessian eigenvectors with eigenvalue 2/η. Equivalently, we mean that Σ can be
written as Σ = U XU T where the k columns of U form a basis for the k-dimensional subspace U, and X is a k × k symmetric matrix.
24
For gradient descent, the unstable eigenvectors have eigenvalues which fluctuate around 2/η. However, for the central flow, the unstable
eigenvectors will have eigenvalues which are exactly equal to 2/η.
23

13

eigenvector 1

0.020

(t) eigenvalue
displacement2 along
(t) eigenvector
its time-average

0.015
0.010
0.005
0.000

4600

4800

5000 5200
step / time

5400

eigenvector 2

0.020
0.015

0.015

0.010

0.010

0.005

0.005

0.000

4600

4800

5000 5200
step / time

eigenvector 3

0.020

5400

0.000

4600

4800

5000 5200
step / time

5400

Figure 11: Central flow’s Σ(t) accurately predicts covariance of oscillations. We show that the central flow’s Σ(t)
accurately predicts the covariance of gradient descent’s oscillations. For this stretch of training, there are 3 Hessian
eigenvalues at the edge of stability, so Σ(t) has 3 nonzero eigenvalues (subpanels). In black, we plot each eigenvalue
of Σ(t); in colors, we plot the squared magnitude of gradient descent’s displacement from the central flow along the
corresponding eigenvectors (light = raw values, dark = time average using Gaussian smoothing). Observe that each
eigenvalue of Σ(t) accurately predicts the instantaneous variance of oscillations along the corresponding eigenvector,
even though these oscillations might initially appear erratic. This figure depicts the same ViT trajectory as Figure 5.
are described in Section A.1.4.25 The central flow is defined as eq. (19) with this Σ(t):
h
i
dw
= −η ∇L(w) + 12 ∇w ⟨H(w), Σ(t)⟩
dt

where

Σ(t) solves the SDCP in eq. (70).

(20)

A formal definition for the central flow is given in Section A.2, Definition 4. We note that Σ(t) can be efficiently
represented numerically as it is a low rank matrix, with rank at most the number of unstable eigenvalues.
We now elaborate on the behavior of the central flow:
1. Stable regime: If all Hessian eigenvalues are below 2/η, then the SDCP returns Σ(t) = 0, and the central flow
reduces to the gradient flow.
2. One unstable eigenvalue: If one Hessian eigenvalue is at 2/η, and if the gradient flow would increase this
eigenvalue above 2/η, then our analysis reduces to that of Section 3.2.1. In particular, the SDCP returns a
rank-one matrix of the form Σ(t) = σ 2 u u⊤ where u is the top Hessian eigenvector at w, and σ 2 is defined in
eq. (14). On the other hand, if the gradient flow would decrease this eigenvalue below 2/η, then Σ(t) = 0, and
the central flow will follow the gradient flow out of the edge of stability.
3. Multiple unstable eigenvalues: In general, the SDCP returns a Σ(t) which constrains all Hessian eigenvalues
currently at 2/η from rising above that value. Often, this Σ(t) causes all Hessian eigenvalues currently at 2/η
to remain fixed at 2/η.26 . However, it also allows for eigenvalues to leave EOS when appropriate.
Figure 10 demonstrates the central flow in action. Initially, all Hessian eigenvalues are below 2/η, so Σ(t) = 0 and
the central flow reduces to the gradient flow. Once the top Hessian eigenvalue reaches 2/η around step 2900, Σ(t)
becomes a rank-one matrix, and the central flow keeps the top Hessian eigenvalue locked at 2/η, as it mimics the
effects of oscillating along the the top eigenvector direction. Once the second Hessian eigenvalue also reaches 2/η
around step 3600, Σ(t) becomes a rank-two matrix, and the central flow keeps the top two eigenvalues both locked
at 2/η, as it mimics the effects of oscillating simultaneously along the top two eigenvector directions. Throughout,
the Euclidean distance between gradient descent’s wt and the central flow’s w(t) stays small over time (right plot),
25

Interestingly, complementarity problems arise frequently in the the study of contact mechanics. EOS can be interpreted as the gradient
descent trajectory making “contact” with the boundary of the stable region, and then sliding along the boundary.
26
There is a unique Σ that causes all Hessian eigenvalues currently at 2/η to remain fixed at 2/η, and it can be found by solving a linear
inverse, generalizing eq. (13). The solution to the SDCP coincides with this Σ at almost all times. However, this Σ cannot be used to define the
central flow, as it would never allow an eigenvalue to leave the edge of stability, and it is not necessarily PSD.

14

indicating that the central flow accurately tracks the long-term trajectory of gradient descent. In contrast, the distance
between gradient descent and the gradient flow eq. (6) grows large over time.
In Figure 11, we show that the central flow’s Σ(t) accurately predicts the covariance with which gradient descent
is oscillating around the central flow. In particular, we show that each eigenvalue of Σ(t) accurately predicts the
instantaneous variance of oscillations along the corresponding eigenvector of Σ(t). We find it striking that our theory
is able to accurately predict the covariance of these oscillations. While the oscillations are erratic and might appear
unpredictable, our findings reveal that a certain statistic — their covariance — is predictable after all. Moreover,
predicting this covariance seems to be sufficient to predict the long-term trajectory of gradient descent.
Interpretation as projection The projection interpretation in Section 3.2.1 generalizes to the case of an arbitrary
number of unstable eigenvalues. In particular, the central flow eq. (20) can be written as a flow which orthogonally
projects the negative gradient onto the so-called tangent cone of the stable region S = {w : S(w) ≤ 2/η}, which is
the set of directions in which one can move while still staying, to first order, within the stable region:27
dw
= η projTw S [−∇L(w)]
dt
|
{z
}

where

S = {w : S(w) ≤ 2/η} .
|
{z
}

(21)

stable region S

project negative gradient onto
tangent cone Tw S of set S

A formal definition is given in Definition 5. This projection interpretation will be used in Section 3.3 to show that the
loss along the central flow decreases monotonically.
Where does deep learning come in?
Our principal claim is that, if initialized stably, gradient descent will
approximately follow the central flow over the long term. In the case where the sharpness does not rise to 2/η (e.g. on
a quadratic objective, where the sharpness is constant), then the central flow reduces to the gradient flow, and so our
claim reduces to the somewhat “uninteresting” claim that gradient descent will approximately follow the gradient flow.
The central flow only becomes nontrivial, and our claim only becomes “interesting,” in the event that the sharpness
rises to 2/η. This empirically tends to happen on deep learning objectives. However, we suspect that the central flow
might also hold on other kinds of objectives where the sharpness rises to 2/η during gradient descent.
3.2.3

Understanding the train loss curve and more

Figure 12 shows that the loss along the actual gradient descent trajectory is consistently higher than the loss along the
central flow. The intuitive explanation is that when gradient descent oscillates along the top Hessian eigenvector(s),
it can be visualized as “bouncing between valley walls” (Xing et al., 2018; Cohen et al., 2021; Wen et al., 2025),
gradient norm2

train loss

0.25

gradient descent (raw)
gradient descent (smoothed)
central flow
central flow prediction
for time-averaged loss

0.24
0.23
0.22

10

Gradient
descent

5

0.21
0.20

gradient descent (raw)
gradient descent (smoothed)
central flow
central flow prediction
for time-average

15

5000

5100

5200
5300
step / time

5400

5500

0

5000

5100

5200
5300
step / time

5400

prog
ress

5500

Central ow

direc
tio

n

fl

Figure 12: The train loss and gradient norm2 are larger along the raw gradient descent trajectory (light blue) than
along the central flow (solid black). The intuitive reason is that an oscillatory optimizer can be visualized as oscillating
between the walls of a “valley” (see cartoon on right). Because the central flow models the covariance Σ(t) of the
oscillations, it can render predictions for the time-averaged values of the loss and gradient norm2 along the gradient
descent trajectory, via eq. (23) and eq. (74) respectively (dashed black). These predictions are close to the empirical
time-averaged values, computed via Gaussian smoothing (dark blue). This figure depicts the same ViT as Figure 5.
27

In the interior of the stable region (i.e. S(w) < 2/η), the tangent cone is the entire space, so this projection is a no-op and the central flow
reduces to the gradient flow. When exactly one eigenvalue is at the edge of stability, the tangent cone is the half-space {v : ∇S(w)T v ≤ 0}.

15

whereas the time-averaged iterates run nearly along the “valley floor” (called a “river” in Wen et al. (2025)), as
illustrated on the right of Figure 12. The loss is higher on the valley walls, where the actual iterates are located, than
on the valley floor, where the time-averaged iterates are located.28
Fortunately, the central flow framework will still let us reason about the loss along the actual gradient descent
trajectory. Recall that the central flow predicts not just the time-averaged iterates w(t), but also the covariance of the
oscillations Σ(t). In particular, the central flow models the gradient descent trajectory {wt } as:29
wt = w(t) + δt ,

where

E[δt ] = 0

and

E[δt δtT ] = Σ(t).

Thus, for any quantity f (w) derived from the weights (e.g. loss or gradient norm), we can predict its time-averaged
value E f (wt ) along the gradient descent trajectory wt by taking a quadratic Taylor expansion along the central flow
w(t), and time-averaging over δt :
≈

E[f (wt )]
| {z }

time-averaged
value along trajectory

f (w(t))
| {z }

value along central flow

+ 12 ∇2 f (w(t)), Σ(t) .
{z
}
|

(22)

contribution from oscillations

For example, if f is the loss L, then because Σ(t) is supported on the Hessian eigenvectors with eigenvalue 2/η:
E[L(wt )]
| {z }

time-averaged
loss along trajectory

≈

L(w(t))
| {z }

+

loss along central flow

1
η tr Σ(t)
| {z }

:= L̄(t).

(23)

contribution from oscillations

See Section A.2.2 for an explicit derivation. Figure 12 shows that this prediction L̄(t) for the time-averaged loss
closely matches the actual time-averaged loss (computed with Gaussian smoothing). Both the central flow loss L(w(t))
and the predicted time-averaged loss L̄(t) model important quantities that are meaningful to DL practice:
• The central flow’s prediction for the time-averaged loss L̄(t) models the smoothed training loss curve, often
monitored in practice by Tensorboard (Abadi et al., 2015) or Weights and Biases (Biewald, 2020).
• The central flow loss L(w(t)) models the loss at the time-averaged iterate. This is similar to the loss at an
exponential moving average of the weights, or the loss after annealing the learning rate (Sandler et al., 2023).
The central flow perspective allows us to quantify both of these loss values and reason about them separately.
A similar point holds for other quantities,30 such as the gradient norm. Figure 12 shows that the squared gradient
norm along the central flow, ∥∇L(w(t))∥2 is much smaller than at the actual iterates, ∥∇L(wt )∥2 . Intuitively, most
of the gradient at the iterates is spent “oscillating across the valley” and cancels out over the long run. The central
flow’s gradient is smaller because it leaves out these oscillations. Yet, because the central flow models the covariance
Σ(t) of the oscillations, it can still predict the time-average of the squared gradient norm at the iterates, using eq. (74)
from Section A.2.2. Figure 12 demonstrates the accuracy of this prediction.
3.2.4

Empirical verification

We empirically find that the central flow can accurately predict the long-term trajectory of gradient descent in a variety
of deep learning settings. For example, Figure 13 shows the central flow on several different deep learning settings
(details in Section 6). Observe that the central flow accurately predicts the weight-space trajectory, the covariance of
the oscillations, and the time-averaged training loss curve. Figures 32(a) and 32(b) show that the central flow can
accurately predict the time-averaged training loss curve at different learning rates across a variety of deep learning
settings. Our full set of gradient descent experiments can be found in Section E.1.
28

This does not mean that the loss landscape is “locally convex” in any deep sense (indeed, the Hessian generally has negative eigenvalues).
It merely reflects that the optimizer is oscillating along directions of positive curvature.
29
We emphasize that the central flow does not model the “distribution” (so to speak) of the oscillations δt . Rather, it only models the second
moment E[δt δtT ], under the theory that the macroscopic trajectory of gradient descent is completely characterized by this second moment.
30
Interestingly, for some quantities (such as the network outputs), we find that the value along the central flow is already an excellent
approximation to the time-averaged value along the discrete optimizer trajectory, and eq. (22) is not necessary.

16

train loss
raw GD loss
smoothed GD loss
central flow loss
time-average prediction

CNN

0.4
0.3
0.2

real vs. predicted
oscillation covariance

top hessian eigenvalues

400

gradient descent
central flow
gradient flow (top 1)

300
200
100

0.1
0

1000

2000 3000
step / time

4000

0

0

1000

2000 3000
step / time

4000

400

ViT

0.35

200

0.25

100

0.20

ResNet

0 1000 2000 3000 4000 5000 6000
step / time
0.35

300

0.30

250

0.25

200

0.20

150

0.15

100

0.10

50
0 1000 2000 3000 4000 5000 6000
step / time

0

0 1000 2000 3000 4000 5000 6000
step / time

0 1000 2000 3000 4000 5000 6000
step / time

40
20

0.1
0 1000 2000 3000 4000 5000 6000
step / time

0

0 1000 2000 3000 4000 5000 6000
step / time

Transformer

150

2000 3000
step / time

4000

0.0

0.010

1.0

0.008

0.8

0.006

0.6

0.004

0.4

0.002

0.2

0.000

0 1000 2000 3000 4000 5000 6000
step / time

0.000

0.0150
0.0125
0.0100
0.0075
0.0050
0.0025
0.0000

0.0

1000

2000 3000
step / time

4000

0.4
0.2
0 1000 2000 3000 4000 5000 6000
step / time

0.0

1.5
1.0
0.5
0 1000 2000 3000 4000 5000 6000
step / time

0.0

50

0.002

0.4

25

0.001

0.2

0.000

0 1000 2000 3000 4000 5000 6000
step / time

2.0

0.6

0 1000 2000 3000 4000 5000 6000
step / time

0 1000 2000 3000 4000 5000 6000
step / time

0.6

0.003

75

0

0

0.8

0.8

100

0 1000 2000 3000 4000 5000 6000
step / time

0.0

0 1000 2000 3000 4000 5000 6000
step / time

0 1000 2000 3000 4000 5000 6000
step / time

800

0.300

0.00150
0.00125
0.00100
400
0.00075
0.00050
200
0.00025
0
0.00000
0 1000 2000 3000 4000 5000 6000
0 1000 2000 3000 4000 5000 6000
step / time
step / time
600

Mamba

0.275
0.250
0.225
0.200
0.175

1000

0.004

125

0 1000 2000 3000 4000 5000 6000
step / time

0

1.0

0.3

0.1

0.2

0.005

0.4

0.2

0.4

0.001

60

0.2

0.6

0.002

80

0.3

central flow
gradient flow

0.8

0.003

100

0.4

LSTM

0

distance to gradient descent

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.012

300

0.30

0.006
0.005
0.004
0.003
0.002
0.001
0.000

0 1000 2000 3000 4000 5000 6000
step / time

0.4
0.3
0.2
0.1
0.0

0 1000 2000 3000 4000 5000 6000
step / time

Figure 13: Verifying the gradient descent central flow across various architectures. Across various architectures,
the central flow accurately predicts the weight-space trajectory, the covariance of the oscillations, and the timeaveraged loss curve. These experiments all use MSE loss. See Section 6 for more experimental details and Section E
for our full set of raw experiments.

17

Nevertheless, our derivation relied on informal mathematical reasoning, and certain factors do empirically affect the
quality of the central flow approximation. First, the central flow tends to become less accurate as the learning rate η
is made increasingly large. Second, on some deep learning problems, higher-order terms cause the central flow to
slightly mispredict Σ(t), causing error to accumulate over the long run. Third, large spikes also can throw off the
central flow. The latter two issues empirically seem to be more common when the loss criterion is cross-entropy rather
than MSE. We discuss these points at greater length in Section 6. We hope that future work can rigorously understand
the conditions under which the central flow does or does not approximate the gradient descent trajectory.

3.3

Understanding Gradient Descent via its Central Flow

We have shown that the central flow is a smooth curve that characterizes the macroscopic trajectory of gradient
descent. We now explain why this makes it a useful theoretical tool for reasoning about optimization.
Averaging out oscillations reveals the underlying order At the
edge of stability, gradient descent’s oscillations lead to wild fluctuations in many training-related quantities, such as the training loss and
the network’s predictions (Rosenfeld and Risteski, 2024). For example, the figure on the right shows the evolution under gradient descent
of the network’s prediction on an example. One can see that along the
actual gradient descent trajectory (blue), training proceeds erratically.
In contrast, the central flow (black) is a more coherent training process
which makes steady, continuous progress over time. By averaging out
the oscillations, the central flow reveals the underlying order hidden
beneath the chaotic oscillatory dynamics.31

wt

w(t)

dw(t)
dt

−η ∇L(wt)

network output on arbitrary example
0.4
0.3
0.2
0.1

gradient descent
central flow

0.0
0.1

5000

5050

5100

5150

step/time

5200

5250

A smooth curve can be analyzed using calculus Because the central
flow is a smooth curve, we can leverage calculus to reason quantitatively
about the dynamics of training. Crucially, along the central flow, the
time derivative dw(t)
dt meaningfully reflects the optimizer’s direction of
motion over the near term (see cartoon on left). In comparison, along the
gradient descent trajectory, the analogous update −η∇L(wt ) is dominated
by oscillations and hence does not meaningfully reflect the direction of
motion over the near term — only over the current step.
For any quantity f (w) derived from the weights w, we can use the chain rule
dw
to compute its rate of change under the central flow: df
dt = ⟨∇f (w), dt ⟩.
We will now use this to reason about the rate of loss decrease.

Reasoning about training loss curve Consider the most basic questrain loss
gradient descent loss
tion one can ask about an optimization algorithm: how fast is the loss 0.32
central flow loss
0.30
going down? For the “raw” gradient descent trajectory, the loss doesn’t
central flow prediction
for time-averaged loss
always go down — instead, the loss behaves non-monotonically over 0.28
slope of gradient flow
short timescales, while only decreasing over long timescales. Thus, rea- 0.26
soning about the rate of loss decrease is challenging. In contrast, under 0.24
the central flow, the loss evolves smoothly, and its rate of decrease can 0.22
be quantified using the chain rule: dL(w)
= ∇L(w), dw
dt
dt . Combining
0.20
this with the projection interpretation (Definition 5), one can easily
2000 2500 3000 3500 4000 4500 5000 5500 6000
prove that the loss along the central flow L(w(t)) is monotonically
step/time
decreasing. In other words, the central flow loss is a valid potential function for the optimization process:
d
Proposition 1. Under the central flow w(t), we have dt
L(w(t)) ≤ 0.
31

Arguably, the central flow can even be viewed as the “true” training process, with the actual gradient descent trajectory being merely a
noisy realization of this idealized trajectory which is computationally cheap to obtain.

18

See Section A.2.3 for the proof. The intuition is that, even after the negative gradient is projected onto the tangent
cone of the stable region, it will still be negatively aligned with the gradient.
While averaging out the oscillations yields a central flow with a smoothly decreasing loss curve, the oscillations
still have an effect on this loss curve through their implicit curvature reduction effect, which can be shown to
slow down training. In particular, whereas the unregularized gradient flow eq. (6) decreases the loss at the speed
dL
2
dt = −η∥∇L(w)∥ , it is straightforward to show that the central flow optimizes at a slower speed:
d
L(w(t)) ≥ −η∥∇L(w(t))∥2 .
Proposition 2. Under the central flow w(t), we have dt

See Section A.2.3 for the proof. The intuition is that because the central flow projects out the components of the loss
gradient that would cause the sharpness to rise above 2/η, it has less gradient available with which to decrease the
loss. This effect is illustrated in the figure on the previous page, which shows that at various points during training,
the slope of the central flow loss curve is less steep than the rate of loss decrease under the gradient flow.
Understanding the effect of hyperparameters A notorious peculiarity
of deep learning is that optimizer hyperparameters affect not just the speed
central ow, small η
stable region,
of training, but also the particular path that the optimizer takes through
small η
weight space (e.g. Keskar et al., 2017; Jastrz˛ebski et al., 2019). As a result,
stable region,
large η
these hyperparameters can affect many properties of the final learned
model, including its robustness and generalization.32 Such effects are
central ow, large η
implicit in the gradient descent update eq. (1). In contrast, the central flow
renders explicit all effects of the learning rate hyperparameter η on the
optimization process, allowing us to disentangle these effects from one
another. Recall from eq. (21) that the central flow is a projected gradient flow with learning rate η that is constrained
to the stable region S = {w : S(w) ≤ 2/η}. From this characterization, we see that the learning rate hyperparameter
η has two distinct effects on the central flow: (1) it acts as a time rescaling, which controls the speed of optimization
without affecting the overall trajectory; and (2) it determines the stable region, which affects the overall trajectory.
Thus, increasing the learning rate constrains gradient descent to a smaller subset of weight space, but also allows it to
traverse this set at a faster speed.33
fl

Having introduced the central flows framework with an analysis of gradient descent, we will now use this methodology
to understand the behavior of two adaptive optimizers.

32

Large learning rates are necessary for obtaining good generalization in some deep learning settings (e.g. Li et al., 2019). However,
obtaining the best generalization performance usually also requires stochastic optimization with a sufficiently small batch size. Since our paper
exclusively studies the deterministic setting, we decided to not focus on generalization in this paper.
33
The learning rate η that is optimal from an optimization perspective (i.e. that will decrease the loss the fastest) will depend on the
trade-off between these two effects. Empirically, we observe that for deterministic gradient descent, larger learning rates usually optimize
faster (provided that training does not diverge), implying that the former effect is stronger.

19

4

Scalar RMSProp

As a stepping stone to the analysis of RMSProp in Section 5, we first study “Scalar RMSProp,” a simplification of
RMSProp which uses one global step size, rather than separate step sizes for each coordinate:3435
νt = β2 νt−1 + (1 − β2 )∥∇L(wt )∥2 ,

η
wt+1 = wt − √ ∇L(wt ).
νt

(24)

The algorithm maintains an exponential moving average (EMA), ν, of the squared gradient norm, and takes gradient
√
steps of size η/ ν, which we call the effective step size.36 The EMA hyperparameter β2 is a knob that interpolates
the algorithm between gradient descent when β2 = 1 and normalized gradient descent (NGD) when β2 = 0.37
While optimizers such as Scalar RMSProp are often said to utilize an “adaptive step size,” it has remained unclear
what precise property of the local landscape the step size is being adapted to (Orabona, 2020). In this section, we will
use the central flows framework to answer this basic question. After describing the dynamics of Scalar RMSProp
in Section 4.1 and deriving a central flow in Section 4.2, we will interpret this flow to understand the optimizer’s
behavior in Section 4.3. In particular:
• In Section 4.3.1, we make precise how Scalar RMSProp adapts its step size to the local loss landscape.
Specifically, we show that the optimizer’s dynamics implicitly set the effective step size to the value 2/S(w),
where S(w) is the current sharpness; this value is the largest stable step size at the current weights w.
• In Section 4.3.2, we show that step size adaptation is not the full story: Scalar RMSProp also implicitly
regularizes curvature throughout training, and in fact, at EOS, the hyperparameters η, β2 only affect the
time-averaged trajectory by modulating the strength of this curvature regularization.
• Bringing it all together, in Section 4.3.3 we describe how the interplay between step size adaptation and
curvature regularization gives rise to a mechanism we call acceleration via regularization, whereby the
optimizer implicitly steers itself towards low-curvature regions where it can take larger steps. We show that this
mechanism is key to the efficacy of Scalar RMSProp and to the function of its hyperparameters.
These points will generalize to RMSProp in Section 5, but are simpler to understand for Scalar RMSProp.

stable

0.35

train loss

1 unstable

top effective Hessian eigenvalues

4 stable

2 unstable

2

0.25

1
0

500

1000 1500
step

2000

2500

0

top Hessian eigenvalues

2 unstable

stable

1 unstable

2 unstable

400

3

0.30
0.20

1 unstable

200
0

500

1000 1500
step

2000

2500

0

500

1000 1500
step

2000

2500

Figure 14: A typical Scalar RMSProp trajectory. We train a Mamba network on a sequence task using Scalar
RMSProp with η = 2/400 and β2 = 0.99. While the top eigenvalues of the “raw” Hessian H(w) evolve freely
√
(right), the top eigenvalue of the effective Hessian ηH(w)/ ν equilibrates at the critical threshold 2 (center).

34

Note that we have re-indexed ν compared to the standard definition of RMSProp (i.e. νt+1 → νt ). This does not affect the trajectory and
just ensures the effective learning rate at step t is determined by νt , rather than νt+1 , which simplifies the notation.
35
This algorithm was also studied by Lyu et al. (2022). However, their analysis only applies along a manifold of global minima, as η → 0.
36
The terms “learning rate” and “step size” are usually interchangeable. In this paper, to avoid ambiguity, we will use the phrase “learning
rate” to denote the hyperparameter, and “step size” or “effective step size” to denote the actual step sizes that are taken.
√
37
When β2 = 1, Scalar RMSProp reduces to gradient descent with learning rate η/ ν0 . Conversely, when β2 = 0, it reduces to normalized
∇L(wt )
gradient descent with learning rate η: wt+1 = wt − η · ∥∇L(w
.
t )∥

20

4.1

The Dynamics of Scalar RMSProp

√
The dynamics of Scalar RMSProp revolve around the effective sharpness, defined as S eff := ηS(w)/ ν.38 First, the
effective sharpness controls the oscillations: when S eff > 2, Scalar RMSProp oscillates with growing magnitude along
high curvature direction(s). Second, such oscillations in turn trigger a reduction of effective sharpness. This occurs
via a combination of two distinct mechanisms. One mechanism, shared with gradient descent, is that oscillations
implicitly reduce sharpness due to eq. (9), thereby decreasing the effective sharpness via its numerator. The other
mechanism, new to Scalar RMSProp, is that oscillations increase the gradient norm and hence ν, thereby decreasing
effective sharpness via its denominator. These dynamics give rise to a negative feedback loop that keeps the effective
sharpness automatically regulated around the value 2, as depicted in Figure 14. The fine-grained dynamics are
complex and challenging to analyze, even in the case of a single unstable eigenvalue. Fortunately, we will see in the
next section that analyzing the time-averaged dynamics is much simpler.

4.2

Deriving the Scalar RMSProp Central Flow

Recall that while gradient descent trains stably, it is well-approximated by gradient flow. One can derive an analogous
“stable flow” for Scalar RMSProp (Ma et al., 2022, cf.):39
dw
η
= − √ ∇L(w),
dt
ν


dν
1 − β2 
=
∥∇L(w)∥2 − ν .
dt
β2

(25)

However, at the edge of stability, the trajectory of Scalar RMSProp deviates from eq. (25). We will now derive a more
general central flow that characterizes the time-averaged trajectory even at EOS. In the main text, we will focus on
the special case where one eigenvalue is, and remains at, the edge of stability. See Section A.3 for our full derivation
which accounts for multiple eigenvalues at EOS and for eigenvalues entering and leaving EOS.
In Section 3.2.1, we derived an approximation for the time-averaged gradient, E[∇L(wt )]. Using the first two terms
of eq. (9), we can also derive a time-averaged approximation for the squared gradient norm E[∥∇L(wt )∥2 ]:
((((

( ) E[x ] + S(w )2 E[x2 ]
E[∥∇L(wt )∥2 ] ≈ ∥∇L(wt )∥2 + 2 ⟨∇L(w
u⟩(S(w
t ),(
t
t
t
t
(((
((

where we again used E[xt ] = 0 to ignore the middle term. This calculation makes clear that larger oscillations (i.e.
higher E[x2t ]) increase the squared gradient norm on average over time. Based on these time averages, we make the
ansatz that the joint dynamics of (wt , νt ) follow a central flow (w(t), ν(t)) of the form:
i
dw
η h
= − √ ∇L(w) + 21 σ 2 (t)∇S(w) ,
dt
ν |
{z
}
E[∇L(wt )]

i
dν
1 − β2 h
=
∥∇L(w)∥2 + S(w)2 σ 2 (t) −ν ,
{z
}
dt
β2 |

(26)

E[∥∇L(wt )∥2 ]

where σ 2 (t) is a still-unknown quantity intended to model E[x2t ], the instantaneous variance of the oscillations. As in
our analysis of gradient descent, there is a unique value of σ 2 (t) that maintains S eff (w, ν) = 2. To compute it, we
eff
eff
eff dw
∂S eff dν
dw dν
dS eff
expand dSdt using the chain rule: dSdt = ⟨ ∂S
∂w , dt ⟩ + ∂ν · dt . Plugging in dt , dt from eq. (26) shows that dt is
eff
linear in σ 2 . Thus, there is a unique value of σ 2 that will ensure dSdt = 0, which is given by:
effect of mean reversion on ν

σ 2 (w; η, β2 ) =

progressive sharpening
}|
{
}|
{
z
z
β2 ⟨−∇L(w), ∇S(w)⟩ +(1−β2 ) S(w)2 /4 − ∥∇L(w)∥2 /η 2

β2 12 ∥∇S(w)∥2 +(1−β2 ) S(w)2 /η 2
| {z }
|
{z
}
sharpness reduction

.

(27)

effect of oscillation on ν

√
While we could have defined effective sharpness as S(w)/ ν so that it would equilibrate at 2/η, this version makes the analysis easier.
39
2
The 1 − β2 → 1−β
correction is necessary for small values of β2 . For example, when β2 = 0 (i.e. normalized gradient descent),
β2
2
νt = ∥∇L(wt )∥ so in the continuous time ODE, ν(t) needs to adapt “instantly” to ∥∇L(w(t))∥2 . See Section A.7 for additional justification
for this correction term.
38

21

train loss
Scalar RMSProp
Scalar RMSProp (smoothed)
central flow
(time-average prediction)
stable flow

0.35
0.30

4

real vs. predicted oscillation covariance

top effective Hessian eigenvalues

1e 5

Scalar RMSProp
central flow
stable flow (top 1)

3

1.5

2

1.0

0.25

1

0.5

0.20

0

0.0

0

500

1000 1500 2000 2500
step / time

0

500

empirical variance
along (t) eigenvectors
(t) eigenvalues

2.0

1000 1500 2000 2500
step / time

distance to Scalar RMSProp

0.25

central flow
stable flow

0.20
0.15
0.10
0.05

0

500

1000 1500 2000 2500
step / time

0.00

0

500

1000 1500 2000 2500
step / time

Figure 15: Central flow for Scalar RMSProp. The central flow (black) accurately models the time-averaged
trajectory of Scalar RMSProp even at the edge of stability, whereas the naive stable flow (red) follows a different
path. As with gradient descent, our analysis can accurately predict the covariance Σ(t) with which Scalar RMSProp
oscillates around the central flow (third panel). The setting is the same as Figure 14.
The central flow for Scalar RMSProp in the special case of one unstable eigenvalue is given by eq. (26) with this value
of σ 2 .40 The fully general central flow is given in Section A.3, Definition 7. Figure 15 illustrates how this central
flow can accurately predict the long-term trajectory of Scalar RMSProp as well as the covariance with which Scalar
RMSProp oscillates around that trajectory. Figures 33(a) and 33(b) show, in a variety of deep learning settings, that
this central flow can accurately predict the loss curve of Scalar RMSProp at different learning rates. Figure 36 shows
that the central flow holds across different values of β2 . See Section E.2 for the full set of raw experiments.
The analysis in this section highlights the potential of our time-averaging methodology. With just a single invocation
of the chain rule, we have characterized the long-term trajectory of a complex dynamical system involving mutual
interactions between the oscillations, the sharpness, and the adaptive step size.

4.3

Understanding Scalar RMSProp via its Central Flow

We now interpret the Scalar RMSProp central flow to shed light on the behavior of the algorithm and the function of
its hyperparameters η and β2 . Because the dynamics usually transition from stable to EOS quite early in training, we
focus on interpreting the central flow in the EOS regime.41
4.3.1

Implicit step size selection

The central flow renders explicit the step size strategy that is implicit
in the oscillatory dynamics of Scalar RMSProp. Recall that while the
√
central flow is at EOS, the effective sharpness S eff := ηS(w)/ ν is
fixed at 2. Indeed, this is the equilibrium condition that is automatically
maintained by the dynamics of optimization. This EOS condition can
be rearranged into a statement about the effective step size:
√
η/ ν = 2/S(w).
(28)

Effective step size
0.0044

/ t (along optimizer)
2 /S(w(t)) (along flow)

0.0040
0.0036

0.0032
2000
2500
3000
3500
4000
That is, at EOS, the effective step size along the central flow is always
step
/
time
equal to the value 2/S(w). Notably, the value 2/S(w) is the largest
stable step size for gradient descent at location w. Thus, while Scalar
RMSProp is at EOS, the oscillatory dynamics continually adapt the effective step size to the current largest
stable step size, even as this value evolves throughout training. This is the precise sense in which Scalar RMSProp
“adapts” its step size to the local loss landscape.
40

The Scalar RMSProp central flow can be interpreted as a projected flow in the augmented space (w, ν) under a certain non-Euclidean
norm. However, because this flow is not a gradient flow, it does not immediately suggest a decreasing potential function for Scalar RMSProp.
41
In the stable regime (S eff < 2), the central flow is given by the stable flow eq. (25). For this flow, dw
is directly proportional to that of
dt
gradient flow, implying these flows traverse the same trajectory, just at a different speed (i.e. with a nonlinear time-rescaling). In this regime,
the effective step size generally increases monotonically, so Scalar RMSProp follows gradient flow with a learning rate warmup.

22

In principle, it would be possible for an optimizer to manually compute the sharpness S(w) at each iteration (e.g. by
using the power method), and to manually set the step size to 2/S(w). However, computing the sharpness would
incur some computational overhead, whereas we have shown that Scalar RMSProp finds the maximum stable step
size of 2/S(w) efficiently, using no more computation than is already used by gradient descent (namely, one gradient
computation per iteration). This rich behavior is implicit in the algorithm’s oscillatory dynamics.
Furthermore, note that even comprehending this behavior requires an appeal to some notion of time-averaging. The
effective step size is usually not exactly at 2/S(w), but rather is fluctuating around 2/S(w). The important point is
that it is 2/S(w) on average over time. The central flow perspective gives a way to reason about this behavior.
4.3.2

Implicit curvature reduction

Understanding the implicit step size strategy employed by Scalar RMSProp is not sufficient to fully characterize
the behavior of the algorithm. To do so, we need to return to the central flow, which additionally accounts for the
curvature regularization induced by oscillations. In general, the Scalar RMSProp central flow is a joint flow over
2
2
√
(w, ν). However, at EOS, because η/ ν = 2/S(w) ⇐⇒ ν = η S(w)
, we can eliminate ν from the expression for
4
dw
dt , and write the central flow in terms of w alone:

dw
2 
1
=−
∇L(w) + σ 2 (w; η, β2 )∇S(w)
dt
S(w)
|2
{z
}
| {z }

(29)

implicit sharpness penalty

effective step size

where σ 2 (w; η, β2 ) is given by eq. (27). In other words, the time-averaged trajectory of Scalar RMSProp at EOS is
essentially equivalent to that of the following simpler-to-understand algorithm:
At each iteration, compute the sharpness S(w), and take a gradient step of size 2/S(w) on a
sharpness-regularized objective, where the strength of the sharpness regularizer is given by eq. (27).
Interestingly, the hyperparameters η, β2 are not used to determine the effective step size 2/S(w). Instead, their only
role is to modulate σ 2 , which controls the strength of the implicit sharpness penalty. The effect of the learning rate
hyperparameter η is to monotonically increase σ 2 — indeed, the numerator of eq. (27) is increasing in η while the
denominator is decreasing in η, which implies the overall expression for σ 2 is increasing in η. The simplest case is
2
that of NGD, i.e. when β2 = 0, for which eq. (27) reduces to σ 2 ≈ η4 (see Section A.3). Meanwhile, the effect of the
hyperparameter β2 is to monotonically interpolate σ 2 between that of normalized gradient descent when β2 = 0 and
that of gradient descent when β2 = 1.42 The interpretations of η, β2 generalize to the setting of multiple unstable
eigenvalues, as detailed in Section A.3, Proposition 5.
4.3.3

Acceleration via regularization

To fully grasp the modus operandi of Scalar RMSProp, it is necessary to consider the link between step size adaptation
and curvature regularization. By regularizing sharpness S(w), Scalar RMSProp is able to steer itself towards regions
where the maximal locally stable step size of 2/S(w) is larger. In such regions, Scalar RMSProp can and does
take larger steps. Thus, by regularizing sharpness, Scalar RMSProp enables itself to take larger steps later in
training. We call this mechanism acceleration via regularization. Our experiments suggest that this mechanism is a
critical component of the algorithm’s effectiveness. In Figure 16, we compare the Scalar RMSProp central flow to
an ablated version which adapts the step size to 2/S(w) but does not regularize sharpness. Over the long term, this
ablated flow optimizes slower than the Scalar RMSProp central flow, because it traverses sharper regions of weight
space in which it is forced to take smaller steps. (See Section D, Figure 40 for more settings.)
42
We note that which of these is larger is situation dependent, so σ 2 can be either monotonically increasing or monotonically decreasing in
β2 . That said, because when β2 = 0, σ 2 (w; η, 0) ≈ η 2 /4 and when β2 = 1, σ 2 (w; η, 1) is independent of η, a general rule is that for small
learning rates, σ 2 is monotonically increasing in β2 , while for large learning rates, σ 2 is monotonically decreasing in β2 .

23

train loss L(w(t))

0.21

sharpness S(w(t))

0.03

1000
800

0.20

400

0.01

200

0.18
0

200

400

step

600

800

1000

Scalar RMSProp, = 0.01
Scalar RMSProp, = 0.02
central flows
ablation w/o
curvature reg

0.02

600

first 50 steps

0.19

effective step size: / (t)

0

200

400

step

600

800

1000

0

200

400

step

600

800

1000

Figure 16: Implicit curvature regularization accelerates optimization for Scalar RMSProp. Starting from the
same initial point, we run Scalar RMSProp at two different learning rates (blue and orange), alongside the corre2
sponding central flows (black). We also run an ablated flow dw
dt = − S(w) ∇L(w) which has curvature regularization
removed (purple). All three flows use the same step size strategy, and differ only in the strength of implicit curvature
regularization. Initially (see inset), the flows with higher curvature regularization optimize slower; however, over the
longer run, they take larger steps and optimize faster. This figure is in the same setting as Figure 15.43
The mechanism of “acceleration via regularization” is also key for understanding the function of the learning rate
hyperparameter η. We have seen that at EOS, the only direct effect of η on the central flow is to modulate the strength
of sharpness regularization, with higher η inducing stronger sharpness regularization. Thus, counterintuitively, the
instantaneous effect of a higher η is often to slow down optimization. However, as we illustrate in Figures 16 and 40,
over longer timescales, higher η steers the trajectory into lower-sharpness regions, in which Scalar RMSProp’s
effective step size will be larger, thereby tending to speed up optimization. Thus, as one would expect of a learning
rate hyperparameter, larger η can accelerate optimization; however they do so through this indirect mechanism.

43

In this figure, for Scalar RMSProp, we show the train loss at the second-order midpoints between iterates (see Section B.1).

24

5

RMSProp

We now study RMSProp (Tieleman and Hinton, 2012), which is equivalent to Adam (Kingma and Ba, 2015) without
momentum. RMSProp maintains an EMA ν of the elementwise squared gradients ∇L(w)⊙2 , and uses per-coordinate
√
effective step sizes of η/ ν:44
νt = β2 νt−1 + (1 − β2 )∇L(wt )⊙2 ,

η
wt+1 = wt − √ ⊙ ∇L(wt ),
νt

(30)

where ⊙ represents the entrywise product. RMSProp can also be viewed as preconditioned gradient descent
√
wt+1 = wt − Pt−1 ∇L(wt ) with the dynamic preconditioner Pt := diag( νt /η).45 While Adam employs the same
dynamic preconditioner and has achieved widespread success in deep learning, it has remained unclear why this
specific preconditioning strategy is so effective (Kunstner et al., 2019; Orabona, 2020; Martens, 2020). A common
folklore belief is that Adam/RMSProp adapts to the local “curvature” (i.e. Hessian). However, it is a priori unclear how
this can be so, since the algorithm uses the (squared) gradient, not the Hessian, to update its preconditioner.
In this section, we use the central flows framework to understand the behavior of RMSProp. We will show that
RMSProp does adapt to the local Hessian after all, but the reason is inextricably tied to its oscillatory dynamics,
which have not been previously studied.
We start by describing the dynamics of RMSProp in Section 5.1. We then derive a central flow in Section 5.2. Finally,
in Section 5.3, we interpret this flow to understand the optimizer’s behavior. In particular:
• In Section 5.3.1, we show that RMSProp’s preconditioner is implicitly determined by the algorithm’s oscillatory
dynamics, and we make this preconditioner explicit for the first time. Specifically, we show that RMSProp
computes its preconditioner by solving a convex program (eq. 35) involving the Hessian. This clarifies that
RMSProp is implicitly a second-order optimizer, despite only accessing the loss through first-order gradients.
• In Section 5.3.2, we show that, like Scalar RMSProp, the success of RMSProp relies not only on this preconditioning strategy, but also on an acceleration via regularization mechanism whereby implicitly regularizing
curvature allows the optimizer to take larger steps later in training.
These basic insights into the functioning of RMSProp are a prerequisite for a similar understanding of Adam, and
may guide us in using these optimizers more effectively and in improving upon them.

1.50
1.25
1.00
0.75
0.50
0.25
0.00

coordinate 1

1e 8

4

coordinate 2

1e 7

1.0
0.8

3

0.6

2

2000

2100

2200 2300
step

2400

2500

gradient2

0.4

1
0

coordinate 3

1e 9

0.2
2000

2100

2200 2300
step

2400

2500

0.0

2000

2100

2200 2300
step

2400

2500

Figure 17: RMSProp ν is determined by oscillations. While training a network using RMSProp, we plot the
squared gradient ∇L(wt )⊙2 (light blue) and its EMA νt (dark blue) at three coordinates (subpanels). Due to the
EOS oscillations, the squared gradient fluctuates, causing the EMA νt to also fluctuate. Since this EMA is used
√
to determine the effective step sizes η/ νt , analyzing these dynamics is necessary for understanding RMSProp’s
adaptivity. This network is a ResNet trained on a subset of CIFAR-10 using η = 2e-5, β2 = 0.99 and MSE loss.

√
√
Our analysis can accommodate both bias correction and an ϵ-dampening (dividing by ν + ϵ rather than ν) which are used by Adam
(see Section A.5). However, to simplify exposition, the main text focuses on this simpler version of RMSProp.
45
Folding η into the preconditioner is unconventional, but will make the analysis clearer.
44

25

train loss

0.5

top effective hessian eigenvalues

RMSProp
RMSProp (smoothed)
central flow
(time-avg prediction)
stable flow

0.4
0.3
0.2
0.1

0

500

1000 1500
step / time

2000

4

RMSProp
central flow
stable flow (top 1)

3

real vs. predicted oscillation covariance
0.00020

empirical variance
along each mode
predicted variances

0.00015

distance to RMSProp
3

2

0.00010

2

1

0.00005

1

0

0

500

1000 1500
step / time

2000

0.00000

0

500

1000 1500
step / time

central flow
stable flow

4

2000

0

0

500

1000 1500
step / time

2000

Figure 18: Central flow for RMSProp. The RMSProp central flow (black) accurately models the macroscopic
trajectory of RMSProp even at EOS, whereas the naive stable flow (red) follows a different path. As for gradient
descent and Scalar RMSProp, we are able to predict the covariance Σ(t) with which RMSProp oscillates around the
central flow (third panel). This figure is in the same setting as Figure 17.

5.1

The Dynamics of RMSProp

To give some intuition into RMSProp’s behavior, Figure 17 plots the dynamics of the squared gradient ∇L(wt )⊙2 and
its EMA νt at several coordinates over a stretch of training. Observe that the entries of the squared gradient fluctuate
√
rapidly, causing their EMA to also fluctuate. Since this EMA ν directly determines the effective step sizes η/ ν,
understanding the origin of this behavior is necessary to understand how RMSProp sets its effective step sizes.
These fluctuations in the gradient arise because RMSProp is operating in an oscillatory edge of stability regime. To
understand why RMSProp oscillates, first consider running preconditioned gradient descent wt+1 = wt −P −1 ∇L(wt )
on a quadratic function with Hessian H. The resulting dynamics are controlled by the effective Hessian P −1 H.
Namely, if any eigenvalues of this matrix exceed the critical threshold 2, then preconditioned GD will oscillate
with exponentially growing magnitude along the corresponding (right) eigenvectors.46 For RMSProp in deep
√
learning, both the Hessian H(wt ) and the preconditioner Pt = diag( νt /η) can vary. However, a local quadratic
Taylor approximation suggests that RMSProp will oscillate if the largest eigenvalue of the current effective Hessian
Pt−1 H(wt ) exceeds the critical threshold 2.47 We refer to this quantity as the effective sharpness S eff (wt , νt ):
S eff (wt , νt ) := λ1 (Pt−1 H(wt )).

(31)

Paralleling the dynamics of gradient descent, Cohen et al. (2022) observed that RMSProp typically operates in an
oscillatory EOS regime that revolves around the effective sharpness eq. (31). On the one hand, oscillations ensue
whenever the effective sharpness rises above the critical threshold 2.48 On the other hand, such oscillations reduce the
effective sharpness, both by inducing implicit regularization of curvature (i.e. shrinking H(wt )), and by growing the
gradient and hence the preconditioner Pt . The net result is that the effective sharpness equilibrates around the value 2
(see Figure 18), as the optimizer oscillates along the top eigenvectors of the effective Hessian.

5.2

Deriving the RMSProp Central Flow

Similar as before, we now derive a central flow (w(t), ν(t)) that jointly models the time-averaged dynamics of wt , νt .
We defer the full details to Section A.4 and sketch the argument here.
If RMSProp is oscillating around its time-averaged trajectory {wt }, so that wt = wt + δt , then the time average of

On a quadratic 12 wT Hw, this algorithm evolves via: wt+1 = (I − P −1 H)wt =⇒ wt = (I − P −1 H)w0 . If P −1 H has any
eigenvalues greater than 2, (I − P −1 H) has eigenvalues less than −1, and the iterates diverge along the corresponding right eigenvectors.
47
With this argument, we are also implicitly assuming that the preconditioner evolves sufficiently slowly that its movement can be neglected.
48
For RMSProp (and Scalar RMSProp), the effective sharpness eq. (31) tends to rise both because the curvature tends to rise (progressive
sharpening) and because the gradient (and hence ν) tends to shrink. Due to the second effect, RMSProp often enters EOS sooner, and for a
large range of learning rates, than gradient descent. Further, RMSProp enters EOS even on quadratics, whereas gradient descent does not.
46

26

1.5

1e 8

gradient2 at one arbitrary coordinate

1.0

RMSProp
central flow

4
3
2

0.5
0.0

at same coordinate

1e 9

RMSProp
RMSProp (smoothed)
central flow
central flow
(time-avg prediction)

1
2000

2100

2200
2300
step / time

2400

2500

2000

2100

2200
2300
step / time

2400

2500

Figure 19: Central flow can successfully predict both the time-averaged gradient2 (left) and the EMA ν (right).
On the left, we show that although the squared gradient is fluctuating erratically (recall Figure 17), its time-average
can be predicted by the central flow using eq. (33). On the right, we show that the central flow’s ν(t) accurately tracks
the macroscopic trajectory of the real EMA νt . This figure is in the same setting as Figure 17.
the elementwise squared gradient is approximately:
E[∇L(wt )⊙2 ] ≈
|
{z
}
time-average of
squared gradient

∇L(wt )⊙2
| {z }

squared gradient at
time-averaged iterate

+ diag[H(wt ) E[δt δtT ]H(wt )] .
|
{z
}

(32)

contribution from oscillations

The first term is the squared gradient at the time-averaged iterate; the second term is the contribution to the squared
gradient that originates from oscillating with covariance E[δt δtT ].
If we further assume then these oscillations are contained within the right eigenspace of the effective Hessian
√
diag(η/ νt )H(wt ) that corresponds to the eigenvalue 2, then the rightmost term simplifies as follows:
E[∇L(wt )⊙2 ] ≈
{z
}
|
time-average of
squared gradient

∇L(wt )⊙2
| {z }

squared gradient at
time-averaged iterate

+

4ν
⊙ diag[E[δt δtT ]]
η2
|
{z
}

(33)

contribution from oscillations

Based on this calculation, and on the time-averaged gradient computed in eq. (18), we make the ansatz that the
time-averaged dynamics of wt , νt follow a central flow (w(t), ν(t)) with the functional form:





dν
dw
η
1 − β2 
4ν




= − √ ⊙ ∇L(w) + 12 ∇⟨Σ(t), H(w)⟩,
=
∇L(w)⊙2 + 2 ⊙ diag[Σ(t)] −ν . (34)
dt
dt
β2 
η

ν
|
{z
}
{z
}
|
E[∇L(wt )]
E[∇L(wt )⊙2 ]

To determine Σ(t), we impose three conditions on this flow, analogous to those from Section 3.2.2. As before, it can
be shown that there is a unique matrix Σ(t) satisfying these three conditions, and this matrix can be characterized as
the solution to a semidefinite complementarity problem. The RMSProp central flow is defined as eq. (34) with this
value of Σ(t). See Section A.4, Definition 9 for a formal statement.
Figure 18 illustrates how this central flow can accurately predict the macroscopic trajectory w(t) of RMSProp, as
well as the covariance Σ(t) with which RMSProp is oscillating around that trajectory.49 Figure 19 shows how the
central flow can accurately predict the time-average of the elementwise squared gradient via eq. (33), as well as
the macroscopic trajectory ν(t) of the EMA. Figure 34(a) and Figure 34(b) in Section D show in a variety of deep
learning settings that the central flow can accurately predict the RMSProp loss curve across different learning rates.
Figure 35 and Figure 37 show that the central flow can accurately predict the RMSProp trajectory at different values
of β2 and ϵ, respectively. The full set of raw RMSProp experiments can be found in Section E.3.
To match the preconditioned geometry of the optimizer, we assess whether each eigenvalue of P (ν(t))1/2 Σ(t) P (ν(t))1/2 accurately
predicts the P -whitened variance of oscillations along the corresponding eigenvector; see eq. (126) in Section A.4.
49

27

cosine between (t) and (w(t))

ten coordinates of (t) vs. (w(t))
10 3

1.000

0.95

coordinates of

cos( (t), (w(t)))

1.00

0.998

0.90
0.85
0.80
500

1000

1500

step

2000

2500

10 5

dots: (t)i
lines: (w(t))i

10 7
10 9
10 11
10 13
500

1000

1500

step

2000

2500

Figure 20: The EMA ν converges to its stationary value. While running the RMSProp central flow, we compare
the actual EMA ν(t) to its stationary value ν(w(t)) w.r.t the current weights w(t). On the left, we plot the cosine
similarity between ν(t) and ν(w(t)); on the right, we compare ten individual coordinates (colors), spaced uniformly
throughout the network. The plots begin when training enters EOS, just before step 500. Observe that after a bit of
time, the cosine similarity between ν(t) and ν(w(t)) reaches high values (near 1), and the individual coordinates
coincide as well. This figure depicts the same setting as Figure 17; see Figures 42(a) to 43(b) for more settings.
As with gradient descent, we find that the central flow approximation tends to become less accurate as the learning
rate η grows; see Section 6 for our general discussion about the accuracy of the central flow. In addition, we observe
that the central flow for RMSProp tends to be a bit less accurate overall than that for gradient descent, at least as
measured by weight-space distance between the flow and the discrete optimizer. Finally, we expect the central flow
for RMSProp to break down when β2 becomes too close to zero (i.e. the sign GD limit), as then RMSProp would no
longer resemble preconditioned gradient descent with a slowly-changing preconditioner.

5.3

Understanding RMSProp via its Central Flow

We now interpret the RMSProp central flow to understand the behavior of RMSProp, including how the algorithm
√
sets its effective step sizes η/ ν. Because the dynamics usually transition from stable to EOS early in training, we
focus on the EOS regime.
5.3.1

The stationary preconditioner

Stationarity of ν Unfortunately, even at the edge of stability, ν(t) cannot be expressed as a closed-form function of
w(t) (as it could for Scalar RMSProp in Section 4), and instead remains an independent variable that must be tracked.
This reflects the fact that for any w, there are potentially many values for ν that could stabilize optimization, and the
actual value used by RMSProp depends on the history. Nevertheless, we will now see that under the RMSProp central
flow, ν often implicitly converges to a value that depends on the current w alone.
Intuitively, the RMSProp central flow eq. (34) involves two simultaneous processes of optimization (the w dynamics)
and preconditioner adaptation (the ν dynamics). Suppose that the ν dynamics of preconditioner adaptation occur
fast relative to the w dynamics of optimization, so that ν reaches a stationary point w.r.t the current weights w. In
Proposition 8 we show that for any w, there is in fact a unique ν that satisfies the stationarity condition dν
dt = 0. We call
this unique ν the stationary ν for the weights w, denoted as ν(w). Empirically, we observe that ν(t) usually starts to
attain its stationary value ν(w(t)) at some point during training (after the dynamics have entered EOS), and continues
to match ν(w(t)) thereafter, even as this value evolves. Indeed, Figure 20 illustrates how ν(t) converges to ν(w(t))
both in cosine similarity (left) and coordinate-wise (right). See Figures 42(a) to 43(b) for more settings.50
The stationarity of ν will allow us to reason about RMSProp’s preconditioning strategy with relative ease, i.e. without
needing to account for the history of ν. At any weights w, we can view the corresponding stationary preconditioner
50
Since the speed of the ν dynamics in eq. (34) is controlled by the β2 hyperparameter, one might suspect that ν(t) will converge quicker to
ν(w(t)) when β2 is smaller, and we confirm this in Figure 44. Nevertheless, we emphasize that the quasistationarity of ν w.r.t w empirically
holds even when β2 is relatively large (e.g. 0.99). In keeping with our attitude throughout this paper, we do not claim to have an explanation.

28

optimization speed
L(w) 2P 1

= 2e-05

= 4e-05

= 0.0001

10 3

10 3

10 3

10 4

10 4

10 4

10 5

10 5

10 5

1000 1500 2000 2500 3000
steps

500 1000 1500 2000 2500 3000
steps

stationary
preconditioner
first term only
GD
0

1000

steps

2000

3000

Figure 21: Optimization speeds for various preconditioners. Along the RMSProp central flow for various learning
rates (columns), we assess the efficacy of three different preconditioners P : the RMSProp stationary preconditioner
eq. (35), in blue; a variant eq. (36) with only the first term, in orange; and the preconditioner corresponding to vanilla
gradient descent with the largest locally stable learning rate, i.e. P −1 = (2/S(w)) I, in green. We assess each
preconditioner P by reporting ∥∇L(w)∥2P −1 := ∇L(w)T P −1 ∇L(w), the instantaneous rate of loss decrease under
preconditioned gradient flow with preconditioner P . Observe that the “first term only” preconditioner (orange) is
much better than the vanilla GD (green) preconditioner, and is also better than the actual stationary preconditioner
(blue). The actual stationary preconditioner (blue) is usually better than vanilla GD (green), but not always, especially
when η is smaller. See Figures 45(a) and 45(b) for more experimental settings.
p
P (w) := diag( ν(w)/η) as “the RMSProp preconditioner” that is implicitly used by RMSProp at weights w. We
will now interpret this preconditioner to gain insight into RMSProp’s preconditioning strategy.
Interpreting
the stationary preconditioner In Proposition 7, we show that this stationary preconditioner P (w) :=
p
diag( ν(w)/η) is, remarkably, the optimal solution to a convex optimization problem over preconditioners:
P (w)

:=

arg min
P diagonal, P ⪰0

tr(P ) + η12 ∥∇L(w)∥2P −1
{z
}
|
optimization speed

such that

H(w) ⪯ 2P .
{z
}
|

(35)

local stability

That is, RMSProp implicitly solves the convex program eq. (35) to compute its preconditioner.51 This is the
precise sense in which RMSProp “adapts” its preconditioner to the local loss landscape.
We can now understand RMSProp’s preconditioning strategy by interpreting the optimization problem eq. (35). The
constraint H(w) ⪯ 2P is equivalent to S eff ≤ 2 and hence stipulates that the preconditioner P should keep RMSProp
locally stable. The first term of the objective, tr(P ), is the sum of the inverse effective step sizes. If this were the
only term in the objective, RMSProp’s preconditioning strategy could be simply summarized as maximizing the
harmonic mean of the effective step sizes while maintaining local stability — a sensible preconditioning strategy.
Indeed, consider a variant of eq. (35) with only the first term:
P̂ (w) := arg min

tr(P )

such that

H(w) ⪯ 2P.

(36)

P diagonal, P ⪰0

Figure 21 demonstrates that this preconditioner is a substantial improvement over vanilla gradient descent. (We
describe in Section A.4.1 how we numerically solve eq. (35) and eq. (36).) Interestingly, if the diagonal constraint in
eq. (36) were removed, and if H(w) were PSD, then the optimization problem eq. (36) would have the closed-form
solution P̂ (w) = 21 H(w). That is, the preconditioner P would be a scaling of the Hessian, and preconditioned
gradient descent would move in the same direction as Newton’s method.52
However, matters are complicated by the presence of the second term in the eq. (35) objective. The quantity
∥∇L(w)∥2P −1 is the instantaneous rate of loss decrease under preconditioned gradient flow with preconditioner P .
51
Interestingly, this SDP is the dual to the max-cut SDP: maxΣ⪰0 ⟨Σ, H⟩ such that Σii = 1 for all i. Thus, this preconditioning strategy
could be described as solving the max-cut SDP with the Hessian as the weight matrix, and using the resulting dual variable as its preconditioner.
52
Even if H(w) were not PSD, a similar point would hold: the optimization problem eq. (36) would have the closed-form solution
P̂ = 12 ΠS+ H(w), where ΠS+ denotes projection onto the cone of positive semidefinite matrices.

29

train loss L(w)
0.2

harmonic mean of
effective step sizes 1/

sharpness S(w)

800
600

0

200 400 600 800 1000 1200
step

i/

)

RMSProp
central flow
ablation w/o
curvature reg

0.5

200
0.0

i

1.0

400

0.1

(

1.5

0

200 400 600 800 1000 1200
step

0.0

0

200 400 600 800 1000 1200
step

Figure 22: Implicit curvature regularization accelerates optimization for RMSProp. Starting from the same
initial point, we compare RMSProp (blue) and its central flow (black) to an ablated flow where the implicit curvature
regularization is disabled (red). Relative to this ablated flow, the central flow takes a lower-curvature trajectory
(middle), in which it takes larger steps (right) and optimizes faster (left). The setting is the same as Figure 17.55
Minimizing this term necessarily acts to slow down optimization.53 Indeed, Figure 21 shows that the stationary
preconditioner eq. (35) underperforms the variant eq. (36) with only the first term.
Since the second term in eq. (35) is proportional to η12 , its influence diminishes as the learning rate hyperparameter
η grows. Indeed, it can be seen in Figure 21 that the performance of the stationary preconditioner tends closer to
that of eq. (36) as the learning rate hyperparameter η is made larger. In the limit of large η, the second term vanishes
entirely, and the stationary preconditioner reduces completely to eq. (36). Interestingly, in this limit, the stationary
preconditioner ceases to depend on η: for example,√doubling η will cause ν to quadruple in scale (due to larger
oscillations), while keeping the effective step sizes η/ ν unchanged. This parallels the situation for Scalar RMSProp
in Section 4, where the effective step size at EOS was 2/S(w), independent of η.
The stationary flow

Substituting P into the central flow, we can obtain a stationary flow over w alone:
h
i
dw
−1
1
∇L(w) + 2 ∇w ⟨Σ, H(w)⟩ ,
= − P (w)
| {z }
dt
|
{z
}
stationary
preconditioner

(37)

implicit curvature penalty

where Σ = Σ(w; η; β2 ) is defined as the solution to a certain semidefinite complementarity problem (Section A.4.1,
Definition 10). This model assumes that the ν dynamics (preconditioner adaptation) happen infinitely fast relative to
the w dynamics (optimization), so that we can treat the preconditioner P as always being fixed at its current stationary
value P (w) (eq. 35). The appeal of this characterization is that it eliminates ν from the picture entirely, and expresses
the time-averaged dynamics of RMSProp as a closed system in w alone. Namely, it suggests that the time-averaged
trajectory of RMSProp is equivalent to that of the following simpler-to-understand algorithm:
At each iteration, compute the preconditioner P (w) using eq. (35) and then take a preconditioned gradient step using this preconditioner on a curvature-penalized objective.
Empirically, we find that the stationary flow eq. (37) is often a reasonable model for the RMSProp trajectory. For
example, Figures 46(a) and 46(b) show that the stationary flow can accurately predict the instantaneous rate of loss
decrease at various points along the central flow trajectory, even though it only has access to w(t) and not ν(t).
Meanwhile, Figures 47(a) and 47(b) show that the stationary flow can tolerably predict the trajectory of RMSProp
over moderate timescales, although we note that its accuracy is not as high as the full central flow.54

That is, for any w, the optimization speed ∥∇L(w)∥2P −1 must necessarily be smaller (worse) for eq. (35) than for eq. (36).
In this figure, for RMSProp, we show the train loss at the second-order midpoints between iterates (see Section B.1).
54
As one might expect, the stationary flow tends to only be an accurate model for RMSProp once ν has reached stationarity.
53

53

30

5.3.2

Acceleration via regularization

As with Scalar RMSProp, we find that RMSProp’s implicit curvature regularization enables it to optimize faster. In
Figure 22, we show that when the curvature regularization is disabled, the RMSProp central flow navigates into increasingly sharp regions, where it takes smaller steps, and optimizes slower (see Figure 41 for more settings).56
Establishing this claim theoretically is more difficult for RMSProp than Scalar RMSProp.57 However, in the limit of
large η and small β2 , it can be argued (Section A.4.1) that the stationary flow eq. (37) reduces to:


η2
dw
−1
∇L(w) + ∇ tr P̂ (w) ,
= −P̂ (w)
(38)
dt
4
where P̂ (w) was defined in eq. (36). This model says that RMSProp implicitly picks the diagonal preconditioner with
minimal trace (equivalently, the preconditioner P where the effective learning rates P −1 have maximal harmonic
mean), and also implicitly moves in a direction in which the trace of this preconditioner will become even smaller.
The strength of the latter effect is controlled by η, and in fact, this is the only means by which the learning rate
hyperparameter η affects the trajectory, since the preconditioner P̂ (w) is independent of η. Thus, as with Scalar
RMSProp, larger learning rates translate to larger steps, but only via this indirect mechanism.

56

To run this ablated flow, we manually set ∇H(w) = 0 both in the expression for β and in the expression for dw
(see Section A.4).
dt
Partly, the difficulty of analysis is due to the independent ν dynamics. However, this analysis is also not easy under the stationary flow,
because the second term in eq. (35) causes the effective step sizes to depend not just on the current Hessian but also on the current gradient.
57

31

6

Experiments

The goal of our experiments is to establish that each central flow accurately approximates the trajectory of its
corresponding optimizer in a variety of deep learning settings, and to understand the circumstances under which
this approximation breaks down. Because it is computationally costly to discretize central flows, we experiment on
small-scale networks and datasets. Note that there is no evidence that scale itself fundamentally affects the dynamics
of optimization in deep learning; for example, EOS dynamics have been observed at both smaller (e.g. CIFAR-10)
and larger (e.g. ImageNet or WMT) scales, without noticeable differences. Therefore, we expect that the central flow
approximation would similarly hold true at larger scales, if such experiments were computationally feasible.
We emphasize that the central flow is a theoretical tool for understanding optimizer behavior, not a practical
optimization method. In practice, maintaining an exponential moving average of the iterates (e.g., Morales-Brotons
et al., 2024) is likely a computational feasible way to estimate the optimizer’s time-averaged trajectory.
Architectures We experiment on a diverse set of six architectures: a convolutional neural network (CNN), a ResNet
(He et al., 2016), a Vision Transformer (ViT) (Dosovitskiy et al., 2021), an LSTM (Hochreiter and Schmidhuber, 1997),
a (sequence) Transformer (Vaswani et al., 2017), and a Mamba sequence model (Gu and Dao, 2024). Architectural
details can be found in Section B.2.
Datasets We test the vision architectures (CNN, ResNet, ViT) on a subset of CIFAR-10 (Krizhevsky, 2009), and
the sequence architectures (LSTM, Transformer, Mamba) on a synthetic sorting task (Karpathy, 2020). Further details
on these datasets can be found in Section B.3. For each architecture and each dataset, we test both cross-entropy loss
and MSE loss. As discussed below, the central flow tends to be somewhat more accurate with MSE loss.
Implementation Discretizing the central flows is somewhat nontrivial, as the flows are non-smooth at points
where there is a change in the number of unstable eigenvalues (e.g. going from 0 to 1). We describe our solution
in Section A.2.4 for gradient descent and in Section A.5.1 for a generic (potentially) adaptive optimizer. The time
complexity of each discretization step scales quadratically with the number of eigenvalues that are at the edge of
stability. Most of the computational cost arises from the need to continually re-estimate the top eigenvectors and
eigenvalues of the (effective) Hessian, and to compute the necessary third derivatives (gradients of these eigenvalues).
Full implementation details can be found in Section B.1.
Our code can be found at: https://github.com/centralflows/centralflows.

6.1

Experimental Results

To assess the accuracy of the central flow approximation, we run both the discrete optimizer and the central flow
simultaneously, starting from the same initialization. As a baseline, we also run the corresponding stable flow (e.g.
for gradient descent, the gradient flow), which we expect to poorly approximate the discrete optimizer when the latter
is at the edge of stability.
Our full experimental results, which can be found in Section E, make clear that the central flow can accurately
approximate the long-term optimization trajectory in a variety of deep learning settings. We find that the weight-space
distance between the discrete optimizer and the central flow generally stays small over time, and is much smaller than
the distance between the discrete optimizer and the stable flow baseline. Meanwhile, the network’s predictions under
the central flow generally match those of the discrete optimizer, whereas the stable flow takes a different path through
function space. The central flow can also accurately predict the time-averaged train loss curve and squared gradient
norm curve, as well as the covariance of the discrete optimizer’s oscillations around the central flow.
That said, the central flow approximation can break down in certain circumstances, which we now describe. An
interesting direction for future work would be to rigorously characterize the conditions under which the central flow
does or does not approximate the real optimizer trajectory.
Sufficiently large learning rates For all three optimizers studied in this paper, we reliably observe that as the
learning rate hyperparameter is made increasingly large, the real optimization trajectory tends to deviate more from
32

training loss
0.4
0.2
0.0

weight space distance between w(t) and wt

network output on test example
= 0.01
= 0.02
= 0.04
central flows

0.2

0.3

0.0

0.2

0.2

0.1

0.4
0

1000

2000
step

3000

4000

0

1000

2000
step

3000

4000

0.0

0

1000

2000
step

3000

4000

Figure 23: Central flow approximation is less accurate at larger learning rates. We run both gradient descent and
its central flow at three learning rates (colors). The larger the learning rate, the faster the growth in the accumulated
approximation error (right). Indeed, at larger learning rates, the network’s output on an arbitrary test example can be
visually seen to be slightly different between the central flow and gradient descent (middle). Nevertheless, the central
flow approximation is still accurate enough here to accurately capture the train loss curves (left). Details: a CNN is
trained on CIFAR-10 using MSE loss.
the central flow, as illustrated in Figure 23. (As an extreme example, even if the optimizer is initialized stably, very
large learning rates sometimes cause the real optimizer to explosively diverge in the middle of training, whereas this
never happens under the central flow.) We do not know whether some corrected version of the central flow would
be more successful at capturing the real optimization trajectory in these scenarios, or whether the real trajectory is
simply too chaotic to be captured by any flow.
Higher-order terms Sometimes, the local curvature is not well-modeled by the cubic Taylor approximation, as is
assumed by our theory. This leads the central flow to mispredict Σ(t), causing error to accumulate over the long run.
We elaborate on this failure mode in Section C.2.
Smoothness of architecture The smoothness of the architecture seems to affect the accuracy of the central flow
approximation; non-smooth components such as ReLU or max pooling often cause the quality of the approximation
to break down. For example, in Figure 38, we show that as a network’s activation function is interpolated from GeLU
(smooth) to ReLU (non-smooth), the accuracy of the central flow approximation degrades, both in weight space and
function space. Note that it is not clear how best to precisely quantify “smoothness” in this context.
Large spikes When the EOS dynamics lead to extremely large spikes (e.g. in the gradient norm), we have found
such spikes can cause the real trajectory to deviate from the central flow, as illustrated in Figure 39. This may be
related to the large learning rate issue described above.
Interactions with loss criterion We have empirically found that the higher-order terms issue and the large spike
issue are more common with cross-entropy loss than with mean squared error loss (although we do not have a
satisfactory explanation for these observations). Consequently, the central flow approximation is often more accurate
under the MSE loss than the cross-entropy loss.
Overall, despite these limitations, we argue that the central flow describes the behavior of the corresponding optimizer
“to a first approximation.” Even in cases where the central flow is not a perfect quantitative match to the discrete
trajectory, it may still capture the important qualitative trends.

33

7

Discussion

7.1

Modeling decisions

Deterministic setting Our analysis is restricted to the simple setting of deterministic (i.e. full-batch) training,
whereas practical deep learning generally involves minibatch training. We study the full-batch setting because, as
the simplest special case, understanding full-batch training is a necessary prerequisite for understanding minibatch
training. However, we also believe that understanding full-batch training might suffice for some practical purposes,
such as designing optimizers. For example, Kunstner et al. (2024) showed that the advantage of adaptive methods over
SGD grows larger with larger batch sizes, suggesting that the relevant algorithmic principles can be best understood
in the deterministic setting.
An interesting direction for future research is to try to extend our central flows methodology to the stochastic setting.
Like deterministic optimizers, stochastic optimizers are known to implicitly regularize the curvature along their
trajectories, and in fact this effect is stronger in the stochastic setting (Keskar et al., 2017; Jastrz˛ebski et al., 2020,
2021; Lee and Jang, 2023; Andreyev and Beneventano, 2024). However, extending the central flows methodology to
the stochastic setting may be nontrivial; due to the randomness, it is not clear whether there exists a deterministic
differential equation around which SGD oscillates. An interesting question is whether there exists a differential
equation that can predict derived metrics such as network predictions or training loss curves, even if it cannot model
the weight-space trajectory of SGD. Finally, while our analysis in this paper sheds light on adaptive optimizers in the
deterministic setting, these optimizers could exhibit substantially different behavior in the stochastic setting.
Black-box model of the loss Our analysis treats the loss function as a black box, and never uses that the optimization
problem at hand involves training a neural network. The advantage of this approach is its generality: we expect
our analysis to apply to generic deep learning architectures and learning problems, including those that do not yet
exist. The disadvantage, however, is that the predictions made by our theory are at the abstraction level of the loss
landscape, and would need to be further translated in order to make concrete claims about the network architecture or
learning problem. For example, our theory tells us that the learning rate hyperparameter modulates the strength of
an implicit sharpness penalty, but does not tell us how this sharpness penalty affects learning. Nor does our theory
shed light on how different layers of the neural network are mechanistically implicated in progressive sharpening or
sharpness reduction.
On the one hand, the loss landscape level of abstraction is in some sense “natural” — the overall path that optimizers
follow really does intrinsically depend on the (effective) sharpness. But on the other hand, understanding many
important aspects of optimization in deep learning will likely require cracking open the black box a bit more.

7.2

Takeaways from our analysis

The unreasonable effectiveness of time-averaging Prior works on EOS show that it is challenging to analyze
the oscillatory EOS dynamics in fine-grained detail. Our work shows that, perhaps surprisingly, simple heuristics
allow us to analyze the time-averaged trajectory with excellent numerical accuracy. Interestingly, the success of this
time-averaging approach seems to imply that the oscillations only affect the macroscopic trajectory in an ergodic
sense, i.e. via their covariance rather than via their fine-grained details. An promising direction for future work is to
identify realistic conditions under which our heuristic time-averaging arguments can be made rigorous.
Necessity of third-order Taylor expansions While optimization theory generally relies on second-order Taylor
expansions of the loss, Damian et al. (2023) showed that a third-order Taylor expansion is necessary for understanding
the convergence of gradient descent; such a Taylor expansion reveals that oscillations implicitly trigger curvature
reduction, a form of negative feedback which stabilizes optimization. In this work, we have shown that a third-order
Taylor expansion is similarly necessary for understanding the acceleration via mechanism which underlies the success
of adaptive optimizers. Thus, our work further underscores the necessity of a third-order Taylor expansion when
analyzing optimization in deep learning.

34

Oscillatory first-order methods are implicitly second-order methods Over the last decade, optimizers that
explicitly use Hessian information have failed to outperform first-order adaptive optimizers which employ only
gradient information. Our work demystifies this observation. We have shown that that when first-order optimizers
oscillate, they implicitly leverage second order information. Thus, even though RMSProp is a first-order optimizer, it
implicitly employs a second-order preconditioning strategy, detailed in Section 5.3.1. Further, this preconditioning
strategy is efficient, requiring no more gradient queries than gradient descent does. An exciting direction for future
work is to intentionally design first-order adaptive methods with such implicit preconditioners in mind.
Adapting to curvature is not enough Traditional optimization theory views the curvature of the loss as a preexisting feature of the optimization problem, and views the job of an optimizer as adapting to this pre-existing
curvature. We have shown that the adaptive optimizers that we study do not merely passively adapt to the curvature;
they also actively shape the curvature along their trajectory, by steering away from high-curvature regions where they
would need to take small steps. Further, we have shown that this effect is crucial for their optimization efficacy. Thus,
our work suggests that acceleration via regularization is a vital design principle for adaptive optimizers.

8

Limitations

Before concluding, we review the limitations of our approach. First, our analysis is currently nonrigorous and is
ultimately supported by experiments rather than mathematical proof. Second, our approach is currently restricted to
the setting of deterministic (i.e. full-batch) training and does not yet apply to stochastic training or to momentum.
Third, it is quite computationally expensive to discretize the central flows, and doing so is only feasible for small
networks on small datasets. Fourth, the central flows tend to degrade in accuracy as the learning rate hyperparameter
is made increasingly large, and are generally more accurate for MSE loss than cross-entropy loss. Fifth, the central
flows may not work well on networks with ReLU or other non-smooth architectural components. Despite these
limitations, we believe that central flows are the best available tool for reasoning about the dynamics of optimization
in deep learning.

9

Conclusion

In this paper, we have developed a methodology for analyzing deep learning optimizers. To analyze an optimization
algorithm, we derive a central flow which models the optimizer’s time-averaged trajectory, rendering explicit what
was previously implicit in the optimizer’s oscillatory dynamics. We have empirically shown that these central flows
can accurately predict long-term optimization trajectories of neural networks, and by interpreting these flows we have
obtained new insights about optimizers’ behavior.
These advances are made possible by the fact that we adopt different goals from most works in optimization. Rather
than try to characterize global convergence rates, we set ourselves the more modest goal of characterizing the local
optimization dynamics throughout training. The local dynamics are important, they are more interesting than may
have been assumed (even vanilla gradient descent gives rise to rich, complex dynamics), and they are empirically
consistent across different deep learning settings, which suggests that a general theory is feasible. We believe that
similar analyses can be fruitfully conducted for other optimizers, and we hope to inspire work in that direction.

35

Acknowledgements
AD acknowledges support from an NSF Graduate Research Fellowship and a Jane Street Graduate Research
Fellowship. JDL acknowledges support of Open Philanthropy, NSF IIS 2107304, NSF CCF 2212262, NSF CAREER
Award 2144994, and NSF CCF 2019844. AT acknowledges support from National Science Foundation grants
IIS1705121, IIS1838017, IIS2046613, IIS2112471, and funding from Meta, Morgan Stanley, Amazon, Google, and
Scribe. JC would like to thank Jim and Marilyn Simons for their support of basic research via the Flatiron Institute.
Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and
do not necessarily reflect the views of any of these funding agencies.
The authors are grateful for feedback from Nikhil Ghosh, Yiding Jiang, Sam Sokota, and Zhili Feng.

36

References
M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin,
S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg,
D. Mané, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar,
P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viégas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu,
and X. Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http:
//tensorflow.org/. Software available from tensorflow.org.
A. Agarwala, F. Pedregosa, and J. Pennington. Second-order regression models exhibit progressive sharpening to the
edge of stability. In Proceedings of the 40th International Conference on Machine Learning, ICML’23, 2023.
K. Ahn, S. Bubeck, S. Chewi, Y. T. Lee, F. Suarez, and Y. Zhang. Learning threshold neurons via edge of stability.
Advances in Neural Information Processing Systems, 36, 2024.
A. Andreyev and P. Beneventano. Edge of stochastic stability: Revisiting the edge of stability for sgd. arXiv preprint
arXiv:2412.20553, 2024.
S. Arora, Z. Li, and A. Panigrahi. Understanding gradient descent on the edge of stability in deep learning. In
International Conference on Machine Learning, pages 948–1024. PMLR, 2022.
Z. Bai, Z. Zhou, J. Zhao, X. Li, Z. Li, F. Xiong, H. Yang, Y. Zhang, and Z.-Q. J. Xu. Adaptive preconditioners trigger
loss spikes in adam. arXiv preprint arXiv:2506.04805, 2025.
D. Barrett and B. Dherin. Implicit gradient regularization. In International Conference on Learning Representations,
2021. URL https://openreview.net/forum?id=3q5IqUrkcF.
L. Biewald. Experiment tracking with weights and biases, 2020. URL https://www.wandb.com/. Software
available from wandb.com.
G. Blanc, N. Gupta, G. Valiant, and P. Valiant. Implicit regularization for deep neural networks driven by an
ornstein-uhlenbeck like process. In Annual Conference Computational Learning Theory, 2019. URL https:
//api.semanticscholar.org/CorpusID:125944013.
J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas,
S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL
http://github.com/jax-ml/jax.
S. Burer and R. D. Monteiro. Local minima and convergence in low-rank semidefinite programming. Mathematical
programming, 103(3):427–444, 2005.
M. D. Cattaneo, J. M. Klusowski, and B. Shigida. On the implicit bias of Adam. In Proceedings of the 41st
International Conference on Machine Learning, 2024.
C. Chen, L. Shen, F. Zou, and W. Liu. Towards practical adam: Non-convexity, convergence theory, and mini-batch
acceleration. Journal of Machine Learning Research, 23(229):1–47, 2022.
L. Chen and J. Bruna. Beyond the edge of stability via two-step gradient updates. In Proceedings of the 40th
International Conference on Machine Learning, ICML’23. JMLR.org, 2023.
X. Chen, S. Liu, R. Sun, and M. Hong. On the convergence of a class of adam-type algorithms for non-convex
optimization. In International Conference on Learning Representations, 2019a. URL https://openreview.
net/forum?id=H1x-x309tm.
Z. Chen, Z. Yuan, J. Yi, B. Zhou, E. Chen, and T. Yang. Universal stagewise learning for non-convex problems
with convergence on averaged solutions. In International Conference on Learning Representations, 2019b. URL
https://openreview.net/forum?id=Syx5V2CcFm.
37

J. Cohen, S. Kaur, Y. Li, J. Z. Kolter, and A. Talwalkar. Gradient descent on neural networks typically occurs at the
edge of stability. In International Conference on Learning Representations, 2021. URL https://openreview.
net/forum?id=jh-rTtvkGeM.
J. M. Cohen, B. Ghorbani, S. Krishnan, N. Agarwal, S. Medapati, M. Badura, D. Suo, D. Cardoze, Z. Nado, G. E.
Dahl, and J. Gilmer. Adaptive gradient methods at the edge of stability. arXiv preprint arXiv:2207.14484, 2022.
E. M. Compagnoni, L. Biggio, A. Orvieto, F. N. Proske, H. Kersting, and A. Lucchi. An sde for modeling sam:
Theory and insights. In International Conference on Machine Learning, pages 25209–25253. PMLR, 2023.
E. M. Compagnoni, T. Liu, R. Islamov, F. N. Proske, A. Orvieto, and A. Lucchi. Adaptive methods through the
lens of SDEs: Theoretical insights on the role of noise. In The Thirteenth International Conference on Learning
Representations, 2025. URL https://openreview.net/forum?id=ww3CLRhF1v.
B. Cornet. Existence of slow solutions for a class of differential inclusions. Journal of Mathematical Analysis and
Applications, 96(1):130–147, Oct. 1983. ISSN 0022-247X. doi:10.1016/0022-247X(83)90032-X.
M. Crawshaw, M. Liu, F. Orabona, W. Zhang, and Z. Zhuang. Robustness to unbounded smoothness of generalized
signsgd. Advances in Neural Information Processing Systems, 35:9955–9968, 2022.
A. Damian, T. Ma, and J. D. Lee. Label noise sgd provably prefers flat global minimizers. Advances in Neural
Information Processing Systems, 34:27449–27461, 2021.
A. Damian, E. Nichani, and J. D. Lee. Self-stabilization: The implicit bias of gradient descent at the edge of stability.
In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.
net/forum?id=nhKHA59gXz.
Y. Dauphin, A. Agarwala, and H. Mobahi. How hessian structure explains mysteries in sharpness regularization.
Advances in Neural Information Processing Systems, 37, 2024.
M. K. de Carli Silva and L. Tunçel. Strict complementarity in maxcut sdp, 2018. URL https://arxiv.org/
abs/1806.01173.
A. Défossez, L. Bottou, F. Bach, and N. Usunier. A simple convergence proof of adam and adagrad. Transactions
on Machine Learning Research, 2022. ISSN 2835-8856. URL https://openreview.net/forum?id=
ZPQhzTSWA7.
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=YicbFdNTTy.
J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization.
Journal of machine learning research, 12(7), 2011.
O. Elkabetz and N. Cohen. Continuous vs. discrete optimization of deep neural networks. Advances in Neural
Information Processing Systems, 34:4947–4960, 2021.
M. Even, S. Pesme, S. Gunasekar, and N. Flammarion. (s)gd over diagonal linear networks: implicit bias, large
stepsizes and edge of stability. In Proceedings of the 37th International Conference on Neural Information
Processing Systems, NIPS ’23, 2024.
J. Geiping, M. Goldblum, P. Pope, M. Moeller, and T. Goldstein. Stochastic training is not necessary for generalization.
In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?
id=ZBESeIUB5k.

38

A. Ghosh, H. Lyu, X. Zhang, and R. Wang. Implicit regularization in heavy-ball momentum accelerated stochastic
gradient descent. In The Eleventh International Conference on Learning Representations, 2023. URL https:
//openreview.net/forum?id=ZzdBhtEH9yB.
M. X. Goemans and D. P. Williamson. Improved approximation algorithms for maximum cut and satisfiability
problems using semidefinite programming. Journal of the ACM (JACM), 42(6):1115–1145, 1995.
A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. In First Conference on
Language Modeling, 2024. URL https://openreview.net/forum?id=tEYskw1VY2.
J. Guckenheimer and P. Holmes. Nonlinear Oscillations, Dynamical Systems, and Bifurcations of Vector Fields.
Springer-Verlag, 1983.
Z. Guo, Y. Xu, W. Yin, R. Jin, and T. Yang. A novel convergence analysis for algorithms of the adam family. arXiv
preprint arXiv:2112.03459, 2021.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pages 770–778, 2016.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 1997.
Y. Hong and J. Lin. On convergence of adam for stochastic optimization under relaxed assumptions. Advances in
Neural Information Processing Systems, 37:10827–10877, 2024.
F. Hübler, J. Yang, X. Li, and N. He. Parameter-agnostic optimization under relaxed smoothness. In International
Conference on Artificial Intelligence and Statistics, pages 4861–4869. PMLR, 2024.
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift.
In International conference on machine learning, pages 448–456. pmlr, 2015.
S. Jastrz˛ebski, Z. Kenton, N. Ballas, A. Fischer, Y. Bengio, and A. Storkey. On the relation between the sharpest
directions of DNN loss and the SGD step length. In International Conference on Learning Representations, 2019.
URL https://openreview.net/forum?id=SkgEaj05t7.
S. Jastrz˛ebski, M. Szymczak, S. Fort, D. Arpit, J. Tabor, K. Cho*, and K. Geras*. The break-even point on
optimization trajectories of deep neural networks. In International Conference on Learning Representations, 2020.
URL https://openreview.net/forum?id=r1g87C4KwB.
S. Jastrz˛ebski, D. Arpit, O. Astrand, G. B. Kerg, H. Wang, C. Xiong, R. Socher, K. Cho, and K. J. Geras. Catastrophic
fisher explosion: Early phase fisher matrix impacts generalization. In International Conference on Machine
Learning, pages 4772–4784. PMLR, 2021.
A. Karpathy. mingpt - demo.ipynb. https://github.com/karpathy/minGPT/blob/master/demo.
ipynb, 2020.
N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training for deep learning:
Generalization gap and sharp minima. In International Conference on Learning Representations, 2017.
A. Khaled, K. Mishchenko, and C. Jin. Dowg unleashed: An efficient universal parameter-free gradient descent
method. Advances in Neural Information Processing Systems, 36:6748–6769, 2023.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Y. Bengio and Y. LeCun, editors,
3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.
A. V. Knyazev. Toward the optimal preconditioned eigensolver: Locally optimal block preconditioned conjugate
gradient method. SIAM Journal on Scientific Computing, 23(2):517–541, 2001.
39

I. Kreisler, M. S. Nacson, D. Soudry, and Y. Carmon. Gradient descent monotonically decreases the sharpness of
gradient flow solutions in scalar networks and beyond. In International Conference on Machine Learning, pages
17684–17744. PMLR, 2023.
A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.
F. Kunstner, P. Hennig, and L. Balles. Limitations of the empirical fisher approximation for natural gradient descent.
Advances in Neural Information Processing Systems, 32, 2019.
F. Kunstner, R. Yadav, A. Milligan, M. Schmidt, and A. Bietti. Heavy-tailed class imbalance and why adam
outperforms gradient descent on language models. In Neural Information Processing Systems, 2024.
S. Lee and C. Jang. A new characterization of the edge of stability based on a sharpness measure aware of batch
gradient distribution. In The Eleventh International Conference on Learning Representations, 2023. URL
https://openreview.net/forum?id=bH-kCY6LdKg.
A. Lewkowycz, Y. Bahri, E. Dyer, J. Sohl-Dickstein, and G. Gur-Ari. The large learning rate phase of deep learning:
the catapult mechanism. arXiv preprint arXiv:2003.02218, 2020.
√
H. Li and Z. Lin. On the O( d/t1/4 ) convergence rate of RMSProp and its momentum extension measured by l1
norm: Better dependence on the dimension. arXiv preprint arXiv:2402.00389, 2024.
H. Li, A. Rakhlin, and A. Jadbabaie. Convergence of adam under relaxed assumptions. Advances in Neural
Information Processing Systems, 36, 2024.
Q. Li, C. Tai, and W. E. Stochastic modified equations and adaptive stochastic gradient algorithms. In Proceedings of
the 34th International Conference on Machine Learning, 2017.
X. Li, Z.-Q. J. Xu, and Z. Zhang. Loss spike in training neural networks. arXiv preprint arXiv:2305.12133, 2023.
Y. Li, C. Wei, and T. Ma. Towards explaining the regularization effect of initial large learning rate in training neural
networks. Advances in Neural Information Processing Systems, 32, 2019.
Z. Li, S. Malladi, and S. Arora. On the validity of modeling SGD with stochastic differential equations (SDEs). In
A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing
Systems, 2021. URL https://openreview.net/forum?id=goEdyJ_nVQI.
Z. Li, T. Wang, and S. Arora. What happens after SGD reaches zero loss? –a mathematical framework. In
International Conference on Learning Representations, 2022a. URL https://openreview.net/forum?
id=siCt4xZn5Ve.
Z. Li, Z. Wang, and J. Li. Analyzing sharpness along gd trajectory: Progressive sharpening and edge of stability. In
Neural Information Processing Systems, 2022b.
Y. Liu, Z. Liu, and J. Gore. Focus: First order concentrated updating scheme. arXiv preprint arXiv:2501.12243,
2025a.
Z. Liu, Y. Liu, J. Gore, and M. Tegmark. Neural thermodynamic laws for large language model training, 2025b. URL
https://arxiv.org/abs/2505.10559.
LucidRains. Vision transformer - pytorch. https://github.com/lucidrains/vit-pytorch, 2024.
K. Lyu, Z. Li, and S. Arora. Understanding the generalization benefit of normalization layers: Sharpness reduction.
Advances in Neural Information Processing Systems, 35:34689–34708, 2022.
C. Ma, L. Wu, and E. Weinan. A qualitative study of the dynamic behavior for adaptive gradient algorithms. In
Mathematical and Scientific Machine Learning, pages 671–692. PMLR, 2022.
40

S. Malladi, K. Lyu, A. Panigrahi, and S. Arora.
On the sdes and scaling rules for adaptive gradient algorithms.
In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 7697–7711. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/
32ac710102f0620d0f28d5d05a44fe08-Paper-Conference.pdf.
J. Martens. New insights and perspectives on the natural gradient method. Journal of Machine Learning Research, 21
(146):1–76, 2020. URL http://jmlr.org/papers/v21/17-678.html.
A. Mishkin, A. Khaled, Y. Wang, A. Defazio, and R. M. Gower. Directional smoothness and gradient methods:
Convergence and adaptivity. Advances in Neural Information Processing Systems, 2024.
D. Morales-Brotons, T. Vogels, and H. Hendrikx. Exponential moving average of weights in deep learning: Dynamics and benefits. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL
https://openreview.net/forum?id=2M9CUnYnBA.
J.-J. Moreau. Décomposition orthogonale d’un espace hilbertien selon deux cones mutuellement polaires. Comptes
Rendus de l’Académie des Sciences, 255:238–240, 1962.
V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th
International Conference on Machine Learning (ICML), 2010.
F. Orabona. Neural networks (maybe) evolved to make adam the best optimizer. https://parameterfree.
com/2020/12/06/neural-network-maybe-evolved-to-make-adam-the-best-optimizer/,
2020. Accessed: October 17, 2024.
S. J. Reddi, S. Kale, and S. Kumar. On the convergence of adam and beyond. In International Conference on Learning
Representations, 2018. URL https://openreview.net/forum?id=ryQu7f-RZ.
M. Rosca, Y. Wu, C. Qin, and B. Dherin. On a continuous time model of gradient descent dynamics and instability
in deep learning. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://
openreview.net/forum?id=EYrRzKPinA.
E. Rosenfeld and A. Risteski. Outliers with opposing signals have an outsized effect on neural network optimization.
In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.
net/forum?id=kIZ3S3tel6.
V. Roulet, A. Agarwala, J.-B. Grill, G. M. Swirszcz, M. Blondel, and F. Pedregosa. Stepping on the edge: Curvature
aware learning rate tuners. In The Thirty-eighth Annual Conference on Neural Information Processing Systems,
2024. URL https://openreview.net/forum?id=SEflLHIhhJ.
M. Sandler, A. Zhmoginov, M. Vladymyrov, and N. Miller. Training trajectories, mini-batch losses and the curious
role of the learning rate, 2023. URL https://arxiv.org/abs/2301.02312.
N. Shi, D. Li, M. Hong, and R. Sun. RMSprop converges with proper hyper-parameter. In International Conference
on Learning Representations, 2021. URL https://openreview.net/forum?id=3UDSdyIcBDA.
S. L. Smith, B. Dherin, D. Barrett, and S. De. On the origin of implicit regularization in stochastic gradient descent.
In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?
id=rq_Qr0c1Hyo.
M. Song and C. Yun. Trajectory alignment: Understanding the edge of stability phenomenon via bifurcation theory.
In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.
net/forum?id=PnJaA0A8Lr.

41

D. E. Stewart.
Dynamics with Inequalities.
Society for Industrial and Applied Mathematics,
2011.
doi:10.1137/1.9781611970715.
URL https://epubs.siam.org/doi/abs/10.1137/1.
9781611970715.
T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.
COURSERA: Neural networks for machine learning, 4(2):26, 2012.
A. Torres-Leguet. mamba.py. https://github.com/alxndrTL/mamba.py, 2024.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all
you need. Advances in Neural Information Processing Systems, 30, 2017.
B. Wang, J. Fu, H. Zhang, N. Zheng, and W. Chen. Closing the gap between the upper bound and lower bound of
adam’s iteration complexity. Advances in Neural Information Processing Systems, 36, 2024a.
B. Wang, H. Zhang, Q. Meng, R. Sun, Z.-M. Ma, and W. Chen. On the convergence of adam under non-uniform
smoothness: Separability from sgdm and beyond. arXiv preprint arXiv:2403.15146, 2024b.
B. Wang, Y. Zhang, H. Zhang, Q. Meng, R. Sun, Z.-M. Ma, T.-Y. Liu, Z.-Q. Luo, and W. Chen. Provable adaptivity
of adam under non-uniform smoothness. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining. Association for Computing Machinery, 2024c. ISBN 9798400704901.
M. Wang, J. Wang, H. He, Z. Wang, G. Huang, F. Xiong, Z. li, W. E, and L. Wu. Improving generalization and
convergence by enhancing implicit regularization. In The Thirty-eighth Annual Conference on Neural Information
Processing Systems, 2024d. URL https://openreview.net/forum?id=cjM2bhLOiC.
K. Wen, Z. Li, J. S. Wang, D. L. W. Hall, P. Liang, and T. Ma. Understanding warmup-stable-decay learning rates: A
river valley loss landscape view. In The Thirteenth International Conference on Learning Representations, 2025.
URL https://openreview.net/forum?id=m51BgoqvbP.
J. Wu, V. Braverman, and J. D. Lee. Implicit bias of gradient descent for logistic regression at the edge of stability.
Advances in Neural Information Processing Systems, 36, 2024.
L. Wu, C. Ma, and W. E. How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/paper/2018/file/
6651526b6fb8f29a00507de6a49ce30f-Paper.pdf.
Y. Wu and K. He. Group normalization. In Proceedings of the European conference on computer vision (ECCV),
pages 3–19, 2018.
C. Xing, D. Arpit, C. Tsirigotis, and Y. Bengio. A walk with sgd. arXiv preprint arXiv:1802.08770, 2018.
J. Yang, X. Li, I. Fatkhullin, and N. He. Two sides of one coin: the limits of untuned sgd and the power of adaptive
methods. Advances in Neural Information Processing Systems, 36, 2024.
M. Zaheer, S. Reddi, D. Sachan, S. Kale, and S. Kumar. Adaptive methods for nonconvex optimization. Advances in
Neural Information Processing Systems, 31, 2018.
Q. Zhang, Y. Zhou, and S. Zou. Convergence guarantees for RMSProp and adam in generalized-smooth non-convex
optimization with affine noise variance. Transactions on Machine Learning Research, 2025. ISSN 2835-8856.
URL https://openreview.net/forum?id=QIzRdjIWnS.
Y. Zhang, C. Chen, N. Shi, R. Sun, and Z.-Q. Luo. Adam can converge without any modification on update rules.
Advances in Neural Information Processing Systems, 35:28386–28399, 2022.
42

X. Zhu, Z. Wang, X. Wang, M. Zhou, and R. Ge. Understanding edge-of-stability training dynamics with a
minimalist example. In The Eleventh International Conference on Learning Representations, 2023. URL
https://openreview.net/forum?id=p7EagBsMAEO.
F. Zou, L. Shen, Z. Jie, W. Zhang, and W. Liu. A sufficient condition for convergences of adam and rmsprop. In
Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 11127–11135, 2019.

43

A

Central Flow Derivations

In this appendix, we derive central flows for the three optimizers studied in this paper: gradient descent (Section A.2),
Scalar RMSProp (Section A.3), and RMSProp (Section A.4). We reiterate that these derivations rely on informal
mathematical reasoning; our claim that the central flow accurately matches the optimizer trajectory is ultimately
supported empirically by the experiments described in Section 6. An interesting direction for future work is to prove
that, under realistic conditions, the discrete optimizer follows the central flow.

A.1

Preliminaries

A.1.1

Notation

We use L(w) to denote the training objective, a function of weights w ∈ Rd . We will assume that L is three-times
differentiable. We will frequently use H(w) as a shorthand for ∇2 L(w), the Hessian matrix at w.

We use ⟨A, B⟩ to denote the Frobenius inner product tr A⊤ B between two matrices A and B, equivalent to flattening
the matrices into vectors and taking the dot product. We use ker and span to denote the kernel and span of a matrix,
and dim to denote the dimension of a vector space.
We use Sym(Rd ) to denote the set of d × d symmetric matrices. For a k-dimensional subspace U ⊆ Rd , we use
Sym(U) to denote the set of d × d symmetric matrices whose span is contained within U. Equivalently, this is the set
of matrices that can be written as U XU ⊤ , where U ∈ Rd×k is a basis for U and X ∈ Sym(Rk ).
For a subspace U ⊆ Rd and a matrix A ∈ Rd×d , we use A U to denote the restriction of A to U, that is,
A U := ΠU A ΠU ,

(39)

where ΠU is the matrix that projects onto U, e.g. ΠU = U U ⊤ for any orthogonal basis U of U.
For a subspace U ⊆ Rd and a matrix A ∈ Sym(Rd ), we write A ⪰ U 0 to denote that A is positive semidefinite (PSD)
over the subspace U, i.e. u⊤ Au ≥ 0 ∀u ∈ U, or equivalently, A U ⪰ 0. We analogously define A ⪯ U 0 to mean
that A is negative semidefinite (NSD) over U.
We will need to work with higher-order tensors, though we will try our best to keep the tensor notation at a minimum.
For an order-k tensor T and an order-s tensor X, with s < k, we define the contraction T [X] as the order-(k − s)
tensor obtained by multiplying T and X componentwise along the last s coordinates of T and all coordinates of X,
and then summing over those coordinates.
X
(T [X])i1 ,...,ik−s :=
Ti1 ,...,ik−s ,j1 ,...,js Xj1 ,...,js .
j1 ,...,js

We will also write T [u1 , . . . , us ] for the result of contracting T sequentially with the vectors u1 , . . . , us one at a time,
equivalent to T [u1 ⊗ . . . ⊗ us ]. When T is fully symmetric, permuting u1 , . . . , us does not change the result.
It can be helpful to reshape a higher-order tensor into a matrix; this viewpoint lets us work with higher-order tensors
while using the familiar language of linear algebra. For example, consider ∇3 L(w), the order-3 tensor of third
2
derivatives. By flattening together the first two dimensions, we could view this as a matrix of shape Rd ×d . In tensor
2
product notation, we are identifying ∇3 L as an element of Rd ⊗ Rd . (We note that understanding tensor product
notation is not necessary for understanding this paper.) One could similarly identify ∇3 L as an element of Rd×d ⊗ Rd
or Sym(Rd ) ⊗ Rd . These views naturally correspond to linear operators; for example, an element of the tensor
product space Sym(Rd ) ⊗ Rd can be treated as a linear operator Rd → Sym(Rd ). We will switch freely among the
full-tensor, tensor-product, and operator views as convenient.
In particular, we will use the notation ∇H(w) ∈ Sym(Rd ) ⊗ Rd to denote the reshaping of ∇3 L(w) that collects

44

together the first two indices (i.e. the “gradient of the Hessian”):
∇H(w)ij,p := ∇3 L(w)ijp =

∂Hij (w)
.
∂wp

Intuitively, for any direction v ∈ Rd , the operator ∇H(w) : Rd → Sym(Rd ) returns the directional derivative of the
Hessian H(w) when moving in the direction v:
[∇H(w)[v]]ij =

d
X
∂Hij (w)
p=1

∂wp

vp .

Meanwhile, for any matrix Σ ∈ Sym(Rd ), the transpose operator ∇H(w)⊤ : Sym(Rd ) → Rd returns the gradient of
the Σ-weighted Hessian ⟨Σ, H(w)⟩:
h

∇H(w)⊤ [Σ]

i
p

=

d X
d
X
∂Hij (w)
i=1 j=1

∂wp

Σij =⇒ ∇H(w)⊤ [Σ] = ∇w ⟨Σ, H(w)⟩ .

For a vector space V , we abbreviate V ⊗ V as V ⊗2 , e.g. we abbreviate Sym(Rd ) ⊗ Sym(Rd ) as Sym(Rd )⊗2 .
A.1.2

Third-order Taylor expansions

If L : Rd → R is k-times differentiable, then Taylor’s theorem for the k-th order Taylor expansion of L is:
L(w + δ) =

k
X
1
j=0

j!

∇j L(w)[δ ⊗j ] + o(∥δ∥k ).

In particular, the third-order Taylor expansion of L around any w ∈ Rd is given by:
L(w + δ) = L(w) + ∇L(w)[δ] + 21 ∇2 L(w)[δ, δ] + 61 ∇3 L(w)[δ, δ, δ] + o(∥δ∥3 ).

(40)

Likewise, the second-order Taylor expansion of the gradient ∇L around w is:
∇L(w + δ) = ∇L(w) + ∇2 L(w)[δ] + 21 ∇3 L(w)[δ, δ] + o(∥δ∥2 ).

(41)

Since ∇3 L(w)[δ, δ] = ∇3 L(w)[δδ ⊤ ], and recalling our H, ∇H notation from the preceding section, we can
equivalently write this Taylor expansion in the form:
∇L(w + δ) = ∇L(w) + H(w)[δ] + 12 ∇H(w)⊤ [δδ ⊤ ] + o(∥δ∥2 ).

(42)

This is the form that we will directly use in our central flow derivations.
A.1.3

Complementarity

A complementarity relation is a constraint on two non-negative variables which enforces that at least one of the
variables is zero, i.e. both cannot be strictly positive. An example is the following set of three conditions over the two
scalar-valued variables x, y ∈ R:
x ≥ 0,

y ≥ 0,

xy = 0.

(43)

By convention, such a condition is often abbreviated using the shorthand notation:
0 ≤ x ⊥ y ≥ 0.
45

(44)

Complementarity relations can be extended to vectors and matrices. We will be particularly interested in the matrix
case. For two symmetric matrices X, Y ∈ Sym(Rd ), consider the following complementarity relation:
X ⪰ 0,

Y ⪰ 0,

⟨X, Y ⟩ = 0,

(45)

which we will often abbreviate using the shorthand:
0 ⪯ X ⊥ Y ⪰ 0.

(46)

This condition is equivalent to X, Y being PSD with orthogonal spans (Section A.5, Fact 2). It is also equivalent to
X, Y being PSD with span X ⊆ ker Y (Section A.5, Corollary 1).
A.1.4

Semidefinite complementarity problems

The semidefinite complementarity problem (SDCP) will be a recurring primitive throughout this work. An SDCP
asks to find a matrix Σ ∈ Sym(Rd ) that is complementary to an affine function of itself. Namely, let α ∈ Sym(Rd )
be a symmetric matrix and let β ∈ Sym(Rd )⊗2 be a tensor, viewed as a linear operator over symmetric matrices
Sym(Rd ) → Sym(Rd ). The semidefinite complementarity problem is to find a matrix Σ ∈ Sym(Rd ) such that:
0 ⪯ Σ ⊥ α + β[Σ] ⪰ 0.

(47)

This is a generalization of the well-studied linear complementarity problem from vectors in the non-negative orthant
to matrices in the positive semidefinite cone.
Remark 1. It is easily verified that if β −1 [−α] ⪰ 0, then the linear inverse Σ = β −1 [−α] is a solution to the SDCP
eq. (47). Interestingly, along the central flows, this will be the solution to the SDCP at almost all times.
Remark 2. In the scalar-valued case, where α, β ∈ R, one can verify by case-checking that the SDCP
0 ≤ σ 2 ⊥ α + βσ 2 ≥ 0

(48)

has the closed-form solution σ 2 = max(− αβ , 0) provided that β > 0.
It will be useful to restrict the domain of an SDCP to an arbitrary subspace U ⊆ Rd . Recall that we use Sym(U) to
denote the set of symmetric matrices whose span is contained within U. We thus have the following definition:
Definition 1 (Semidefinite Complementarity Problem). For a subspace U ⊆ Rd , matrix α ∈ Sym(U), and tensor
β ∈ Sym(U)⊗2 , we define the solution set of the SDCP as:
SDCPU (α, β) := {Σ ∈ Sym(U) : 0 ⪯ Σ ⊥ α + β[Σ] ⪰ U 0}.

(49)

A priori, it is unclear whether the SDCP has zero, one, or many solutions. The following lemma shows that the SDCP
always has one unique solution if β is symmetric and positive definite as an operator over Sym(U ), i.e if:
β[Σ, Σ′ ] = β[Σ′ , Σ]

∀ Σ, Σ′ ∈ Sym(U)

and

β[Σ, Σ] > 0

∀ Σ ∈ Sym(U)\{0}.

Lemma 1. If β is symmetric and positive definite over Sym(U), then the cardinality of the solution set satisfies
|SDCPU (α, β)| = 1.
Proof. Consider the following quadratic program with a semidefinite constraint:
min
Σ∈Sym(U )

⟨α, Σ⟩ + 12 β[Σ, Σ]

subject to

Σ ⪰ 0.

(50)

As β ≻ 0, the objective is strictly convex so there is a unique minimizer Σ⋆ . The KKT conditions for Σ⋆ are exactly
0 ⪯ Σ⋆ ⊥ α + β[Σ⋆ ] ⪰ 0 and Σ ∈ Sym(U), so Σ⋆ ∈ SDCPU (α, β). Similarly if Σ ∈ SDCPU (α, β) then Σ
satisfies the KKT conditions for the strictly convex semidefinite quadratic program, so Σ = Σ⋆ .
46

Note that this lemma is a straightforward adaptation of a standard argument for linear complementarity problems.
In the case where the lemma applies and the solution to the SDCP is unique, we will overload notation and use
SDCPU (α, β) to denote this unique solution.
Efficient computation Fortunately, solving SDCPU (α, β) does not actually require materializing α ∈ Sym(Rd )
and β ∈ Sym(Rd )⊗2 in full. Let k := dim U denote the dimension of U, which is typically ≪ d, and let U ∈ Rd×k
denote a basis for U. Then any Σ ∈ Sym(U ) can be expressed as Σ = U XU ⊤ for some X ∈ Sym(Rk ). The SDCP
condition eq. (49) then reduces to a k-dimensional SDCP over Rk :
0 ⪯ X ⊥ αU + βU [X] ⪰ 0 ⇐⇒ X ∈ SDCPRk (αU , βU ),

(51)

where the matrix αU ∈ Sym(Rk ) is defined as:
αU := U ⊤ αU

⇐⇒

(αU )ij = u⊤
i αuj

(52)

(βU )ijpq = β[ui , uj , up , uq ].

(53)

and the tensor βU ∈ Sym(Rk )⊗2 is defined via its action as:
βU [X] := U ⊤ β[U XU ⊤ ]U

⇐⇒

Thus, to solve the original d-dimensional problem Σ ∈ SDCPU (α, β), one can instead solve the k-dimensional
problem X ∈ SDCPRk (αU , βU ), and then represent Σ = U XU ⊤ .
To solve SDCPRk (αU , βU ), we use a standard convex solver to solve the convex program eq. (50):
min

X∈Sym(Rk )

A.1.5

⟨αU , X⟩ + 12 βU [X, X]

subject to

X ⪰ 0.

(54)

On local time averaging

We intentionally do not specialize to a specific notion of “local time-average”. The only properties of the local
time-averaging operator E that we use are:
1. linearity, i.e. E[f + g] = E[f ] + E[g] and E[cf ] = c E[f ] for any constant c
2. the local time average of a constant c is itself: E[c] = c
3. in the EOS regime when the sharpness fluctuates around 2/η, the time-average is coarse enough to smooth out
these fluctuations so that S(E[wt ]) = 2/η
One reason why we do not further define the time-averaging operator is that even the appropriate timescale for the
averaging operation (e.g. window size or kernel bandwidth) depends nontrivially on the local dynamics. Recall that in
the relatively simple setting of one unstable eigenvalue that was analyzed in Damian et al. (2023), the EOS dynamics
consists of consecutive cycles where the sharpness rises above, then falls below, the critical threshold 2/η. In this
setting, it is natural to choose an averaging timescale so as to average over a cycle. Yet, the analysis of Damian
et al. (2023) shows that the length of the cycle depends on the initial position of the iterate along the top Hessian
eigenvector at the instant where the sharpness crosses 2/η (the closer the iterate is to the directionwise optimum, the
longer the cycle). Hence, even the choice of timescale is very nontrivial.
A.1.6

Smoothness of the central flows

Central flows are ordinary differential equations of the form: dw(t)
dt = f (w), where f is not continuous everywhere.
Thus, the ODE should be interpreted in the sense of Carathéodory: w(t) is only differentiable at almost all t, and can
have points of non-differentiability where the left and right derivatives differ. For example, at the instant when the
gradient descent central flow first reaches EOS, the right and left derivatives of w(t) will differ, as the left derivative
is −η∇L(w) while the right derivative is −ηΠ⊥
∇S(w) ∇L(w). However, the central flow is right-differentiable for all
d
t. Therefore, when we write dt
(e.g. in Proposition 1), it can either be interpreted as holding for almost all t, or it can
be alternatively interpreted as a statement about the right derivative.

47

A.2

Gradient Descent

We now derive the central flow for gradient descent. In Section 3.2.1 we considered the special case where one
eigenvalue is at the edge of stability, and is continuing to remain there. The complete central flow, derived here,
applies in the more general setting where multiple eigenvalues are potentially at the edge of stability. It also allows
eigenvalues to enter and leave the edge of stability when appropriate.
This section is structured as follows:
1. First, in Section A.2.1, we formulate the central flow as a differential complementarity problem (DCP): a
dynamical system defined implicitly by combining differential equations with complementarity constraints.
2. Next, in Section A.2.2, we show that this DCP can be re-formulated into an ordinary differential equation with
an explicit right-hand side that involves the solution to a semidefinite complementarity problem.
3. In Section A.2.3, we show that the central flow can be equivalently formulated as a projected gradient flow
that projects the negative gradient onto the tangent cone of the stable region. Leveraging this projection
interpretation, we prove that the central flow decreases the loss monotonically (Proposition 1), but at a slower
rate than the unregularized gradient flow (Proposition 2).
4. Finally, in Section A.2.4, we describe how to discretize the central flow in practice.
A.2.1

The Differential Complementarity Problem (DCP) formulation

The central flow w(t) will model the time-averaged trajectory of gradient descent {wt }:
w(t) := E[wt ].

(55)

Let δt := wt − w(t) denote the displacement between gradient descent and the central flow (i.e. “the oscillation”) at
step/time t. From the definition of w(t) as the time-average, it follows that E[δt ] = 0. Let Σ(t) := E[δt δt⊤ ] denote
the covariance of these oscillations. That is, we are modeling the gradient descent trajectory as:
wt = w(t) + δt ,

where

E[δt ] = 0

and

E[δt δt⊤ ] = Σ(t).

(56)

Recall that gradient descent oscillates along the eigenvectors that are at the edge of stability. When the Hessian
has multiple eigenvalues at 2/η, the corresponding eigenvectors are not individually identifiable, since any linear
combination of eigenvectors is also an eigenvector. Instead, what is identifiable is the corresponding eigenspace, i.e.
the linear subspace comprising all such eigenvectors. We refer to this eigenspace as the critical subspace:
Definition 2 (Critical subspace for gradient descent). Given weights w ∈ Rd , the critical subspace U(w) ⊆ Rd is
defined as the Hessian’s eigenspace corresponding to the eigenvalue 2/η:
h
i n
o
U(w) := ker H(w) − η2 I = u ∈ Rd : H(w) u = η2 u .
(57)

Thus, we assume that the oscillations {δt } are fully contained within the critical subspace:
δt ∈ U(w(t)) ⇐⇒ span[Σ(t)] ⊆ U(w(t)).

(58)

We will now derive the central flow. By Taylor expansion of ∇L around w(t) (see Section A.1.2, eq. (42)), the
gradient at step t is:
∇L(wt ) ≈ ∇L(w(t)) + H(w(t))δt + 21 ∇H(w(t))⊤ [δt δt⊤ ].

(59)

Time-averaging both sides and using that E[δt ] = 0 and E[δt δt⊤ ] = Σ(t) gives:
E[∇L(wt )] ≈ ∇L(w(t)) + 21 ∇H(w(t))⊤ [Σ(t)].
48

(60)

Therefore, we make the ansatz that the central flow w(t) takes the form:
h
i
dw
= −η ∇L(w) + 12 ∇H(w)⊤ [Σ(t)] ,
dt

(61)

for some unknown Σ(t) which we will now determine.
To solve for Σ(t), we impose three conditions for all times t:
1. PSD: As a covariance matrix, Σ(t) is positive semidefinite (PSD): Σ(t) ⪰ 0.
2. Stability: The sharpness remains bounded by 2/η: H(w(t)) ⪯ (2/η)I.
3. Complementarity: The oscillations are contained within the critical subspace: span[Σ(t)] ⊆ U(w(t)).
It will now be helpful to define the “residual” matrix A(w) as:
A(w) := η2 I − H(w).

(62)

Notably, ker A(w) is precisely the critical subspace U(w), i.e. the eigenspace of H(w) with eigenvalue 2/η. With
this notation, Conditions 1-3 can be expressed as:
Σ(t) ⪰ 0,
| {z }
PSD

A(w(t)) ⪰ 0,
|
{z
}
stability

span[Σ(t)] ⊆ ker A(w(t)) .
|
{z
}

(63)

complementarity

As discussed in Section A.1.3, these three conditions are equivalent to the complementarity relation:
Σ(t) ⪰ 0,

A(w(t)) ⪰ 0,

Σ(t) ⊥ A(w(t)),

(64)

which we write more compactly as:
0 ⪯ Σ(t) ⊥ A(w(t)) ⪰ 0.

(65)

We say (w(t), Σ(t)) follow the central flow if they satisfy eq. (61) along with this complementarity relation:58
Definition 3 (Gradient Descent Central Flow, DCP Formulation). We say that {(w(t), Σ(t))}t≥0 follow the gradient
descent central flow if, for almost all t, they satisfy eq. (61) along with the conditions: 0 ⪯ Σ(t) ⊥ A(w(t)) ⪰ 0,
where A(w) := η2 I − H(w).
Definition 3 is an example of a differential complementarity problem (DCP) (Stewart, 2011), which are described
in more detail in Section A.6. In a DCP, Σ(t) is not defined explicitly, but is rather defined implicitly via a
complementarity relation that the trajectory (w(t), Σ(t)) is required to satisfy.
A priori, it is not clear that a feasible Σ(t) exists or is unique. To give an explicit expression for Σ(t), and thereby
turn Definition 3 into an ODE with an explicit right-hand side, we will next show that for almost all times t, Σ(t)
must be the unique solution to a certain semidefinite complementarity problem.
A.2.2

The Ordinary Differential Equation (ODE) formulation

Before deriving the ODE formulation of the central flow, let us first explain why the DCP formulation, Definition 3,
fails to immediately specify Σ(t). At any instant t, there can be multiple Σ’s which satisfy 0 ⪯ Σ ⊥ A(w(t)) ⪰ 0;
for example, the trivial choice Σ = 0 always works. Yet, most of these Σ’s would cause the stability constraint
A(w(t + ϵ)) ⪰ 0 to be violated if the dynamics eq. (61) are run for an infinitesimal amount of time ϵ. Intuitively, we
need to meld together the static constraint 0 ⪯ Σ(t) ⊥ A(w(t)) ⪰ 0 with the dynamics eq. (61).
58

We only require w(t), Σ(t) to satisfy eq. (61) for “almost all t” as w(t) may not be differentiable when an eigenvalue enters or leaves
EOS. This is in line with the standard definition of a differential complementarity problem.

49

To do so, we appeal to Lemma 6 in Section A.6. This result essentially “differentiates” the complementarity relation
d
0 ⪯ Σ(t) ⊥ A(w(t)) ⪰ 0, to yield a new complementarity relation between Σ(t) and the time derivative dt
A(w(t)).
In particular, Lemma 6 implies that under the flow defined by Definition 3, we must have:
d
A(w(t)) ⪰ U (w) 0.
0 ⪯ Σ(t) ⊥ dt

(66)

We have thus turned a “position-level” constraint on the residual A(w(t)) into a “velocity-level” constraint on its time
d
d
derivative dt
A(w(t)). We now expand dt
A(w(t)) to reveal its dependence on Σ(t):
d
dw
dt A(w(t)) = ∇A(w) dt
 
= −∇H(w) dw
dt





(chain rule)
(definition of A)

h h
ii
= −∇H(w) −η ∇L(w) + 21 ∇H(w)⊤ [Σ(t)]

(form of dw
dt )

= η∇H(w)[∇L(w)] + 12 η∇H(w)∇H(w)⊤ [Σ(t)]
|
{z
} |
{z
}

(linearity)

=:α(w)

=:β(w)

d
This reveals that dt
A(w(t)) is affine in Σ(t). Namely, if we define the matrix α(w) and tensor β(w) as:

α(w) := η∇H(w)[∇L(w)] ∈ Sym(Rd ),

β(w) := η2 ∇H(w)∇H(w)⊤ ∈ Sym(Rd )⊗2 ,

(67)

d
then dt
A(w(t)) is given by the affine expression:
d
dt A(w(t)) = α(w) + β(w)[Σ(t)].

(68)

Substituting this into eq. (66) implies that Σ(t) must satisfy the complementarity relation:
0 ⪯ Σ(t) ⊥ α(w) + β(w)[Σ(t)] ⪰ U (w) 0.

(69)

Since Σ(t) ∈ Sym(U (w)), this is precisely an SDCP (Section A.1.4) defined over the critical subspace U(w):
Σ(t) ∈ SDCPU (w) (α(w), β(w)).

(70)

Thus, we are now ready to state the ODE formulation of the central flow.
Definition 4 (Gradient Descent Central Flow, ODE Formulation). We say {w(t)}t≥0 follows the gradient descent
central flow if for almost all t ≥ 0, w(t) satisfies eq. (61) for some Σ(t) ∈ SDCPU (w(t)) (α(w(t)), β(w(t))).
This DCP-to-ODE conversion can be straightforwardly generalized to a broader class of DCPs, and this is done in
Section A.6, Lemma 7. Our subsequent central flow derivations will directly invoke Lemma 7.
Existence and uniqueness of SDCP solution Σ(t) Recall from Section A.1.4, Lemma 1 that the SDCP eq. (69)
will have a unique solution Σ(t) when β(w) is symmetric and positive definite as a linear operator over Sym(U).
Due to its outer product structure, β(w) is always symmetric and positive semi-definite. If β is also full rank as an
operator acting on Sym(U(w)), then it is positive definite, and Σ(t) is unique. Empirically, in our experiments, we
always do observe that this full-rank condition is satisfied (it is equivalent to full-rankness of βU (w) defined below in
eq. (80)), and thus Σ(t) is unique. Note that existence and uniqueness of Σ(t) does not necessarily imply existence
and uniqueness of the central flow ODE.
Existence and uniqueness of central flow Provided that β(w) is full-rank as an operator on Sym(U(w)) for all w,
prior results imply existence and uniqueness for the gradient descent central flow (see Section A.6).
One unstable eigenvalue As a sanity check, we now verify that when there is one eigenvalue at the edge of stability
(i.e. when the critical subspace has dimension 1), Definition 4 recovers the central flow defined in Section 3.2.1.
50

In general, a subspace U of dimension 1 has the form U = span u for some u ∈ Rd , so Sym(U) = {σ 2 uu⊤ : σ 2 ∈
R}, and SDCPU (α, β) reduces to a 1-dimensional SDCP:
SDCPU (α, β) = σ 2 uu⊤ ,

σ 2 = SDCPR (αu , βu ),

α := u⊤ α u,
| u {z
}
∈R

βu := u⊤ β[uu⊤ ]u .
| {z }
∈R

Thus, σ 2 has the closed-form solution described in Remark 2:


αu
2
σ = max − , 0 .
βu
Therefore, when there is one eigenvalue at the edge of stability, Σ(t) from eq. (70) becomes:


αu (w)
2
⊤
2
Σ(t) = σ uu , σ = max −
, αu (w) = u⊤ α(w)u, βu (w) = u⊤ β(w)[uu⊤ ]u,
βu (w)
where u ∈ Rd is the top eigenvector of H(w) at w, and α(w), β(w) were defined in eq. (67). These simplify to:
αu (w) = η∇L(w)⊤ ∇S(w),

βu (w) = η2 ∥∇S(w)∥2 ,

(71)

where we recall that S(w) denotes the top eigenvalue of H(w) at w. Therefore:


−2∇L(w)⊤ ∇S(w)
2
,0 .
σ = max
∥∇S(w)∥2

(72)

When ⟨−∇L(w), ∇S(w)⟩ > 0, i.e. when progressive sharpening holds, this recovers eq. (13). Else, σ 2 = 0, and the
central flow will leave the edge of stability.
Finally, since Σ(t) = σ 2 uu⊤ , eq. (61) reduces to:


dw
= −η ∇L(w) + 12 σ 2 ∇S(w) ,
dt
which recovers eq. (14).
Predicting time-averages The central flow can predict the time-average of various quantities, such as the loss or
squared gradient norm, along the gradient descent trajectory. For any quantity f (w), we write f¯(t) for the central
flow’s prediction for E[f (wt )] at step t.
For example, the central flow’s prediction L̄(t) for the time-averaged loss E[L(wt )] at step t is given by:
E[L(wt )] = E[L(w(t) + δt )]
h
i
≈ E L(w(t)) + ∇L(w(t))⊤ δt + 12 δt⊤ H(w(t))δt
= L(w(t)) + 12 ⟨H(w(t)), Σ(t)⟩
h i
= L(w(t)) + 12 tr η2 Σ

(Taylor expansion)
(E[δt ] = 0, E[δt δt⊤ ] = Σ(t))
(HΣ = η2 Σ)

= L(w(t)) + η1 tr Σ(t)
:= L̄(t).

(73)

Similarly, the prediction for the time-averaged squared gradient norm E[∥∇L(wt )∥2 ] at step t is:


E[∥∇L(wt )∥2 ] ≈ E ∥∇L(w(t)) + H(w(t))δt ∥2
= ∥∇L(w(t))∥2 + H 2 (w(t)), Σ(t)
= ∥∇L(w(t))∥2 + η42 tr Σ(t).
=: ∥∇L(t)∥2
51

(74)

In general, for any function f (w), the central flow predicts that the time-average of f (wt ) at step t is:
h
i
E[f (wt )] ≈ E f (w(t)) + ∇f (w(t))⊤ δt + 12 δt⊤ ∇2 f (w(t))δt
= f (w(t)) + 21 ∇2 f (w(t)), Σ(t)
:= f¯(t)

(75)

(Note: our prediction eq. (74) for the squared gradient norm does not fit this template, as we choose to do a first-order
of expansion of ∇L and then take the norm, rather than do a second-order expansion of f (w) = ∥∇L(w)∥2 .)
The central flow can also predict the covariance with which gradient descent oscillates around the central flow. Let
Σ(t) = V (t) Λ(t) V (t)⊤ be the (reduced) eigenvalue decomposition of the rank-k matrix Σ(t), where V (t) ∈ Rd×k
and Λ(t) ∈ diag(Rk ). Define xt := V (t)⊤ (wt − w(t)) ∈ Rk as the displacement of gradient descent from the central
flow along these eigenvectors. Then the central flow predicts that the covariance of these displacements is:
⊤
⊤
⊤
E[xt x⊤
t ] = V (t) E[δt δt ]V (t) = V (t) Σ(t)V (t) = Λ(t).

In particular, if we consider the i-th diagonal entry, the central flow predicts that the variance of oscillations along the
i-th eigenvector of Σ(t) should be equal to the i-th eigenvalue of Σ(t):

2 
⊤
E vi (t) (wt − w(t))
= λi (t).
(76)
Basis-dependent version Naively computing the central flow’s dw
dt would be impractical, as storing Σ(t) and
α(w) would require O(d2 ) space, and storing β(w) would require O(d4 ) space. Fortunately, because all necessary
quantities are supported on the low-rank critical subspace, the central flow’s dw
dt can be computed efficiently using only
O(k 2 d + k 4 ) space, where k = dim U(w) is the dimension of the critical subspace, which is typically ≪ d.
In particular, fix t, and let U ∈ Rd×k be a basis for the critical subspace U(w(t)). Then recall from Section A.1.4
that Σ(t) can be represented as Σ(t) = U XU ⊤ for some low-dimensional matrix X ∈ Sym(Rk ) that solves
X ∈ SDCPRk (αU (w), βU (w)), where αU ∈ Sym(Rk ) and βU ∈ Sym(Rk )⊗2 were defined in eqs. (52) and (53).
Now we define HU (w) := U ⊤ H(w)U ∈ Sym(Rk ) and its gradient ∇HU (w) ∈ Sym(Rk ) ⊗ Rd :
HU (w)ij := u⊤
i H(w)uj

∇HU (w)ij := ∇w [u⊤
i H(w)uj ].

and

(77)

The tensor ∇HU (w) only requires O(k 2 d) space to store, and can be computed inO(k 2 d) time by looping over all
dw
d
pairs (ui , uj ) of columns of U and computing the third derivative ∇w u⊤
i H(w)uj ∈ R . Crucially, computing dt
only requires access to the smaller ∇HU (w) rather than the full ∇H(w). To see this, note that:
∇HU (w)[v] = U ⊤ ∇H(w)[v]U

∇HU (w)⊤ [X] = ∇H(w)⊤ [U XU ⊤ ].

and

(78)

Thus, the central flow eq. (61) takes the form:
h
i
dw
= −η ∇L(w) + 12 ∇HU⊤ (w)[X] ,
dt

(79)

and αU (w) ∈ Sym(Rk ), βU (w) ∈ Sym(Rk )⊗2 take the form:
αU (w) = η∇HU (w)[∇L(w)],

βU (w) = η2 ∇HU (w) ∇HU (w)⊤ .

(80)

Thus, to compute dw
dt , we can compute ∇HU (w), then use this to compute αU (w) and βU (w) via eq. (80), then solve
the k-dimensional problem X ∈ SDCPRk (αU (w), βU (w)), and then compute dw
dt via eq. (79).
In practice, due to the non-smoothness of the central flow, we do not discretize the central flow by computing dw
dt and
taking an Euler step; instead, we directly discretize the DCP formulation, as described in Section A.6.1.
52

The time-averaged predictions can also be computed efficiently given a basis. If we pick U to be orthonormal
(U ⊤ U = I), then the central flow’s prediction eq. (73) for the time-averaged training loss at step t is:
h
i
L̄(t) := L(w(t)) + η1 tr U XU ⊤
h
i
= L(w(t)) + η1 tr XU ⊤ U
= L(w(t)) + η1 tr X.

(81)

Similarly, the prediction eq. (74) for the time-averaged squared gradient norm at step t is:
h
i
∥∇L(t)∥2 := ∥∇L(w(t))∥2 + η42 tr U XU ⊤ .
= ∥∇L(w(t))∥2 + η42 tr X.

(82)

In general, for any function f (w), the prediction eq. (75) can be computed as:
D
E
f¯(t) = f (w(t)) + 21 U ⊤ ∇2 f (w(t))U, X .
As for predicting the oscillation covariance, we can evaluate both sides of eq. (76) without needing to materialize
⊤ denotes the eigenvalue decomposition of X, and if we define V (t) = U U , then
Σ(t) in full. If X = UX Λ(t) UX
X
⊤
Σ(t) = V (t) Λ(t) V (t) is the eigenvalue decomposition of Σ(t). Note that UX and X will depend on the basis U ,
while V (t) and Λ(t) are independent of U .
Smoothness of the central flow At a finite set of times, a new eigenvalue enters or leaves the edge of stability.
We refer to these instants as breakpoints. In between the breakpoints, Σ(t) is continuous and w(t) is differentiable.
Moreover, the SDCP is solved by the linear inverse Σ = −U βU−1 [αU ]U ⊤ where αU , βU are defined in eq. (80) (see
d
Remark 1). Further, dt
A(w(t)) U (w) = 0, i.e. all Hessian eigenvalues that are at EOS remain fixed at 2/η. However,
at the breakpoints, Σ(t) is discontinuous and w(t) is not differentiable (although they are still right-continuous and
right-differentiable, respectively).
A.2.3

The Projection Formulation

In this section we will show that the gradient descent central flow (Definition 3 and Definition 4) can be equivalently
interpreted as projected gradient flow constrained to the stable set Sη , i.e. the subset of weight space where gradient
descent is locally stable:
Sη := {w : S(w) ≤ 2/η}.

(83)

For general constrained optimization problems, a projected gradient flow projects the negative gradient onto the
tangent cone of the constraint set before taking an infinitesimal step. The tangent cone consists of the set of allowable
directions that would not cause any constraints to be violated.
In our case, the tangent cone TSη (w) of the stable set Sη at the point w ∈ Sη is the set of directions that, to first order,
would not increase the sharpness if we moved in that direction from w. This tangent cone is given by:
TSη (w) = {z ∈ Rd : ∇H(w)[z] ⪯ U (w) 0}.

(84)

Note that this is a convex cone, since it is closed under linear combinations with non-negative weights.
We use projM (·) to denote the usual Euclidean projection onto a set M ⊆ Rd :
projM (v) = arg min ∥v − z∥22 .
z∈M

Projecting a vector onto the tangent cone of the stable set involves solving a certain SDCP:
53

(85)

Lemma 2. The projection of a vector v ∈ Rd onto the tangent cone of Sη at w ∈ Sη is given by:


projTSη (w) [v] = v − 21 ∇H(w)⊤ [Σ] where Σ ∈ SDCPU (w) −∇H(w)[v], 12 ∇H(w)∇H(w)⊤ ,

(86)

h
i
where U(w) := ker H(w) − η2 I is the critical subspace (Definition 2).
Proof. Recall that the tangent cone of Sη is the set: {z ∈ Rd : ∇H(w)[z] ⪯ U (w) 0}. Therefore, the projection of v
onto this set is given by:
projTSη (w) [v] = v + δ ∗ ,

(87)

where the perturbation δ ∗ is the optimal solution to the optimization problem:
min ∥δ∥2
δ

such that

∇H(w)[v + δ] ⪯ U (w) 0.

(88)

This is a quadratic program with a semidefinite constraint. Introducing a dual variable Σ ∈ Sym(U(w)), the KKT
conditions for this optimization problem are:
δ = − 12 ∇H(w)⊤ [Σ],
|
{z
}
stationarity

⟨Σ, ∇H(w)[v + δ]⟩ = 0,
{z
}
|
complementary slackness

∇H(w)[v + δ] ⪯ U (w) 0,
{z
}
|
primal feasibility

Σ⪰0
| {z }

.

(89)

dual feasibility

Substituting the first condition into the middle two yields the following three conditions:
0 ⪯ Σ ⊥ −∇H(w)[v] + 21 ∇H(w)∇H(w)⊤ [Σ] ⪰ U (w) 0.
We recognize these as precisely the characterization of an SDCP:


Σ ∈ SDCPU (w) −∇H(w)[v], 12 ∇H(w)∇H(w)⊤ .

(90)

(91)

Therefore, if Σ satisfies eq. (91), then δ = − 12 ∇H(w)⊤ [Σ] is an optimal solution to the optimization problem eq. (88),
and v + δ is the desired projection.
We are now ready to state the projection formulation of the gradient descent central flow:
Definition 5 (Gradient Descent Central Flow, Projection Formulation). We say that {w(t)}t≥0 follows the gradient
descent central flow if for almost all t,
dw
= projTSη (w) [−η∇L(w)]
dt

where

Sη := {w : S(w) ≤ 2/η}.

(92)

The equivalence between the projection formulation (Definition 5) and the ODE formulation (Definition 4) follows
from applying Lemma 2 to the vector v = −η∇L(w) and noting SDCP(α, β) = SDCP(cα, cβ) for c > 0.
Understanding the projection formulation

We now give intuition for the projection formulation:

• When S(w) < 2/η, w is in the interior of Sη so U(w) = ∅, the tangent cone is the entire space, and the
projection is the identity map. Therefore eq. (92) reduces to gradient flow.
• When there is a single eigenvalue at 2/η, w is on the boundary of Sη and the tangent cone is given by the
halfspace: TSη (w) = {v : ⟨∇S(w), v⟩ ≤ 0}. If the negative gradient lies outside this halfspace (i.e. if gradient
flow threatens to increase the sharpness above 2/η), then the projection onto the halfspace is given by the
projection onto the hyperplane: −ηΠ⊥
∇S(w) ∇L(w) (see Figure 24). But, if the negative gradient already lies in
the halfspace, the projection is the identity map, so the central flow follows gradient flow and leaves EOS.

54

T𝕊

η (w)

={

v:

∇S

vT

∇S(w)
(w)

≤0

T𝕊

η (w)

−η ∇L(w)

}

w

pro

jT

𝕊η (w
)

[− η

∇L

={

v:

v T∇

∇S(w)
S(w

)≤

0}

w

(w)

]

−η ∇L(w)

= projT (w)[−η ∇L(w)]
𝕊η

stable set

stable set

𝕊η = {w : S(w) ≤ 2/η}

𝕊η = {w : S(w) ≤ 2/η}

Figure 24: This cartoon illustrates projecting onto the tangent cone of the stable set TSη (w) in the case where one
eigenvalue is at the edge of stability. The iterate w is on the border of the stable set (grey blob). The tangent cone is
the half-space {v : v ⊤ ∇S(w) ≤ 0} (shaded red). Left: on the one hand, if the negative gradient (blue arrow) points
out of the stable set, then the projection (black arrow) removes the component aligned with S(w) (red arrow). Right:
on the other hand, if the negative gradient (blue arrow) already points into the stable set, then the projection (black
arrow) does nothing.
• In general, computing the projection onto TSη (w) requires solving a semidefinite quadratic program for which Σ
is the Lagrangian dual variable. The KKT conditions of this quadratic program are equivalent (up to a constant)
to the SDCP that defines Σ above.
Properties of projection This projection formulation is helpful because Euclidean projection onto a convex cone
shares some useful properties with Euclidean projection onto a linear subspace. We will use these properties below to
reason about the rate of loss decrease.
First, projection onto a convex cone C is positive homogeneous: for any scalar c > 0 we have:
projC [cv] = c projC [v].

(93)

In our case, this can also be seen directly by combining the characterization of the projection in Lemma 2, with the
identity X ∈ SDCP(α, β) ⇐⇒ cX ∈ SDCP(cα, β).
Second, by the Moreau decomposition (Moreau, 1962), any vector v can be orthogonally decomposed into the
projection onto a convex cone C and the projection onto its dual cone C ∗ :
v = projC [v] + projC ∗ [v]

where

⟨projC [v], projC ∗ [v]⟩ = 0.

(94)

In particular, this implies that:
⟨projC [v], v − projC [v]⟩ = 0.

(95)

⟨v, projC [v]⟩ = ∥projC [v]∥2 .

(96)

and:

In our case, C is the tangent cone to the stable set at w, its dual cone C ∗ is the so-called normal cone to the stable
set at w: {∇H(w)⊤ [Σ] : Σ ∈ Sym(U(w)), Σ ⪰ 0}, and eq. (95) can be proved by rearranging the complementarity
relation in eq. (90).
Rate of loss decrease We now use the projection formulation (Definition 5) to reason about the rate of loss decrease
under the central flow. We first show the following helper lemma:
Lemma 3. Under the gradient descent central flow (Definition 5), for almost all t we have
2
dL(w)
= −η projTSη (w) [−∇L(w)] .
dt

55

(97)

Proof. By the chain rule, we have
dL(w)
=
dt



dw
∇L(w),
dt



E
D
= ∇L(w), projTSη (w) [−η∇L(w)]
D
E
= −η −∇L(w), projTSη (w) [−∇L(w)]
2

= −η projTSη (w) [−∇L(w)] ,
where the first line is the chain rule, the second line is the projection formulation of the central flow, the third line is
due to positive homogeneity of the projection operation eq. (93), and the last line is due to the orthogonality of the
projection eq. (96).
A simple corollary is that the training loss monotonically decreases under the gradient descent central flow:
Proposition 1 (Restated). Under the GD central flow (Definition 5), for almost all t, the loss curve L(w(t)) is
monotonically decreasing:
dL(w(t))
≤ 0.
dt

(98)

Proof. The claim follows by combining Lemma 3 with the fact that a norm is always non-negative.
Another simple corollary is that the central flow decreases the loss at a less steep rate than would gradient flow. In
other words, the oscillations induce a slowdown in the rate of loss decrease:
Proposition 2 (Restated). Under the GD central flow (Definition 5), for almost all t, the slope of the loss curve is less
steep than that of gradient flow:
−η∥∇L(w(t))∥2 ≤

dL(w(t))
.
dt

(99)

Proof. Projecting a vector v onto a convex cone C always makes the norm smaller:
∥v∥2 = ∥projC [v] + [v − projC [v]]∥2
= ∥projC [v]∥2 + ∥v − projC [v]∥2
≥ ∥projC [v]∥2 ,
where the second line is due to the orthogonality of projC [v] and v − projC [v] i.e. eq. (95), and the third line is due
to the non-negativity of a square.
Thus, recalling Lemma 3, we have:
2
dL(w)
= −η projTSη (w) [−∇L(w)]
dt
≥ −η∥ − ∇L(w)∥2

= −η∥∇L(w)∥2 .

56

Why not start with the projection formulation? Since the projection formulation of the central flow is arguably
the simplest one, one might ask why we first went through the DCP/ODE formulations before arriving at the projection
formulation. After all, since the sharpness equilibrates at 2/η at EOS, one might think that a projected gradient flow
constrained to the set {w : S(w) ≤ 2/η} is already a natural approximation. The trouble with this thinking is that
there are actually an infinite number of flows which keep the sharpness locked at 2/η, moving within the tangent
cone of the stable set. Among these, the significance of the central flow (Definition 5) is that it follows the particular
vector within the tangent cone that is closest (in Euclidean distance) to the negative gradient; that is, it makes the
smallest perturbation to the negative gradient that will force it inside the tangent cone. A priori, there is no reason
why this should be the case, and thus jumping straight to the projection formulation would be arbitrary. In fact, we
will see in our analyses of Scalar RMSProp and RMSProp that the central flows do not pick the closest tangent vector
in Euclidean distance and their central flows cannot be interpreted as a projected gradient flow.
A.2.4

Discretizing the gradient descent central flow

Discretizing the central flow is nontrivial, because the flow is nonsmooth at points where the dimension of the critical
subspace (i.e. the number of unstable eigenvalues) undergoes a change. To discretize the flow, we directly discretize
the DCP formulation (Definition 3) rather than going through the ODE formulation (Definition 4). We describe our
general procedure for discretizing DCPs in Section A.6.1. Let us now describe how this general procedure specializes
to the gradient descent case.
We use w(t) , Σ(t) to denote our estimate for the central flow’s w(t), Σ(t). Let ϵ > 0 be the discretization step size,
2
e.g. ϵ = 0.25. For some tolerance τ > 0, e.g. τ = 0.05
η , we will regard Hessian eigenvalues greater than η − τ as
those which might become unstable in the next discretization time step.
At each discretization step, we first compute all Hessian eigenvalues that are greater than η2 − τ , as well as the
corresponding eigenvectors. Let k be the number of such eigenvalues, let D ∈ Rk×k be a diagonal matrix containing
such eigenvalues on the diagonal, and let U ∈ Rd×k be the corresponding orthonormal eigenvectors. Then, we
compute the tensor ∇HU as in eq. (77), though note that U now refers to a basis of eigenvectors whose eigenvalues
are almost 2 rather than exactly equal to 2. Then, we compute αU and βU as in eq. (80). Then, we solve the following
k-dimensional SDCP:
X (t) = SDCPRk ( η2 I − D + ϵ αU , ϵ βU ),
so that Σ(t) = U X (t) U ⊤ . Then, we update the weights via:
i
h
w(t+ϵ) = w(t) − ϵ η ∇L(w(t) ) + 12 ∇HU⊤ [X (t) ] .

(100)

(101)

To predict the time-average of the train loss, the squared gradient, and the covariance of the oscillations, we use
eq. (81), eq. (82), and eq. (76), respectively.

A.3

Scalar RMSProp

We model the Scalar RMSProp iterates {wt } as oscillating around a central flow w(t) with covariance Σ(t). That is, if
δt := wt − w(t) denotes the displacement (“the oscillation”), then E[δt ] = 0 and E[δt δt⊤ ] = Σ(t). Let ν(t) := E[νt ]
model the time-averaged νt , and we will frequently neglect the distinction between the two, implicitly assuming that
νt concentrates tightly around ν(t).
A similar argument as for gradient descent implies that the time-averaged gradient is approximately:
E[∇L(wt )] ≈ ∇L(w(t)) + 12 ∇H(w(t))⊤ [Σ(t)].

57

(102)

Meanwhile, we approximate the time-average of the squared gradient norm as:59
E[∥∇L(wt )∥2 ] ≈ E[∥∇L(w(t)) + H(w(t)) δt ∥2 ]
= ∥∇L(w(t))∥2 + E[∥H(w(t)) δt ∥2 ]

(103)

where the first line is a first-order Taylor expansion of ∇L around w(t) and the second line is because E[δt ] = 0.
We define the critical subspace U(w, ν) as the eigenspace of the effective Hessian √ην H(w) corresponding to the
eigenvalue 2:
h
i
U(w, ν) := ker √ην H(w) − 2I .
(104)
We model Scalar RMSProp as oscillating within the critical subspace, i.e. we assume that δt ∈ U(w(t), ν(t)).
√
2 ν(t)
This implies that H(w(t)) δt = η δt , which lets us simplify eq. (103) as:
E[∥∇L(wt )∥2 ] ≈ ∥∇L(w(t))∥2 + E[∥H(w(t)) δt ∥2 ]
E[∥δt ∥2 ]
= ∥∇L(w(t))∥2 + 4ν(t)
η2
= ∥∇L(w(t))∥2 + 4ν(t)
tr Σ(t),
η2

(105)


where the final line is because E[∥δt ∥2 ] = E[tr δt δt⊤ ] = tr Σ(t).
Based on the time averages eq. (102) and eq. (105), we make the ansatz that the joint dynamics of (wt , νt ) can be
modeled by a central flow (w(t), ν(t)) of the form:
i
dw
η h
= − √ ∇L(w) + 21 ∇H(w)⊤ [Σ(t)]
dt
ν
i
h
dν
2
4ν
2
= 1−β
tr(Σ(t))
−
ν
.
∥∇L(w)∥
+
2
β2
η
dt

(106)

As with gradient descent, to determine Σ(t) we will impose three conditions for all times t:
1. PSD: As a covariance matrix, Σ(t) is positive semidefinite (PSD), i.e. Σ(t) ⪰ 0.
2. Stability: The effective sharpness remains bounded by 2, i.e. √ην H(w(t)) ⪯ 2I.
3. Complementarity: The oscillations are contained within the critical subspace, i.e span Σ(t) ⊆ U (w(t), ν(t)).
To write these concisely, it will be convenient to define the matrix-valued “residual” function A(w, ν) by:
√

A(w, ν) := 2 η ν I − H(w),

(107)

so that stability is equivalent to A(w, ν) ⪰ 0 and the critical subspace is precisely U(w, ν) = ker A(w, ν). With this
notation, we can concisely express the above three conditions as a semidefinite complementarity relation:
0 ⪯ Σ(t) ⊥ A(w(t), ν(t)) ⪰ 0.
We say that (w(t), Σ(t)) follow the Scalar RMSProp central flow if they follow eq. (106) along with this semidefinite
complementarity relation:
Definition 6 (Scalar RMSProp Central Flow, DCP Formulation). We say that {(w(t), ν(t), Σ(t))}t≥0 satisfy the
Scalar RMSProp central flow if they satisfy eq. (106) and 0 ⪯ Σ(t) ⊥ A(w(t), ν(t)) ⪰ 0 for almost all t.
59
We note here that this is not a “faithful” second-order Taylor expansion of the gradient ∇L(wt ) around w(t). We are implicitly assuming
that ∥E[∇L(wt )]∥2 ≈ ∥∇L(E[wt ])∥2 which neglects the term ∇3 L(w(t))[∇L(w(t)), Σ(t)]. This omission simplifies the expressions and
our numerical experiments still successfully predict Σ(t) across a variety of datasets and architectures, which justifies this omission.

58

Since this DCP can be expressed in the general form described in Section A.6, we can use Lemma 7 to convert it into
an equivalent ODE. In particular, if w
e := [w, ν]⊤ denotes the augmented state, we can write eq. (106) as:
#
" η
"
#
− 2√ν ∇H(w)⊤ [Σ]
− √ην ∇L(w)
dw
e
and B(w)[Σ]
e
:=
= f (w)
e + B(w)[Σ]
e
where f (w)
e := 1−β2
,
1−β2 4ν
2
dt
β2 · η 2 tr Σ
β2 [∥∇L(w)∥ − ν]
and we can write the complementarity relation as 0 ⪯ Σ(t) ⊥ A(w(t))
e
⪰ 0.
Therefore, Lemma 7 implies that
Σ(t) ∈ SDCPU (w,ν) (α(w, ν), β(w, ν)),
where the matrix α(w, ν) ∈ Sym(Rd ) and tensor β(w, ν) ∈ Sym(Rd )⊗2 are defined by:
α(w, ν) := ∇A(w)[f
e (w)]
e
"
#
h
i
− √ην ∇L(w)
1
h
i
= −∇H(w) η√ν I 1−β2
∥∇L(w)∥2 − ν
β2
i
η
1 1 − β2 h
= √ ∇H(w)[∇L(w)] + √
∥∇L(w)∥2 − ν I
ν
η ν β2

(108)

and
β(w, ν)[Σ] := ∇A(w)B(
e
w)[Σ]
e
" η
#
⊤ [Σ]
h
i − √
∇H(w)
1
2 ν
= −∇H(w) η√ν I
1−β2 4ν
β2 · η 2 tr Σ
√
4 ν 1 − β2
η
⊤
tr[Σ]I.
= √ ∇H(w)∇H(w) [Σ] + 3 ·
η
β2
2 ν

(109)

Note that β(w)
e is indeed symmetric PSD, as for gradient descent:
E 4√ν 1 − β
 
η D
2
′
⊤
⊤ ′
β(w̃)[Σ, Σ ] = √ ∇H(w) [Σ], ∇H(w) [Σ ] + 3 ·
tr[Σ] tr Σ′
η
β2
2 ν
√
2
η
4 ν 1 − β2
=⇒ β(w̃)[Σ, Σ] = √ ∇H(w)⊤ [Σ] + 3 ·
tr[Σ]2 ≥ 0.
η
β2
2 ν
This gives us the ODE formulation of the Scalar RMSProp central flow:
Definition 7 (Scalar RMSProp Central Flow, ODE Formulation). We say that {w(t), ν(t)}t≥0 follow the Scalar
RMSProp central flow if for almost all t, they satisfy eq. (106) for some
Σ(t) ∈ SDCPU (w(t),ν(t)) (α(w(t), ν(t)), β(w(t), ν(t))).
One unstable eigenvalue As a sanity check, we now verify that this formulation recovers eq. (27) when there is
one eigenvalue at the edge of stability. Just as with gradient descent (see above), in this setting Σ(t) reduces to:


αu (w, ν)
2
⊤
2
Σ(t) = σ uu , σ = max −
, 0 , αu (w, ν) = u⊤ α(w, ν)u, βu (w, ν) = u⊤ β(w, ν)[uu⊤ ]u,
βu (w, ν)
where u ∈ Rd is the unit-norm top eigenvector of H(w) at w. Simplifying, we have:
i
η
1
1 − β2 h
αu (w, ν) = √ ⟨∇L(w), ∇S(w)⟩ + √ ·
∥∇L(w)∥2 − ν
β2
ν
η ν
√
η
4 ν 1 − β2
βu (w, ν) = √ ∥∇S(w)∥2 + 3 ·
.
η
β2
2 ν
59

Then, when αu (w, ν) is negative, we have:
σ2 = −

αu (w, ν)
βu (w, ν)
h

1
2
√η ⟨∇L(w), ∇S(w)⟩ + √
∥∇L(w)∥2 − ν
· 1−β
β2
ν
η ν
√
=−
4 ν 1−β2
2
η
√
∥∇S(w)∥
+
· β2
η3
2 ν

=

=

i
(definition of αu , βu )

h
i
β2 ⟨−∇L(w), ∇S(w)⟩ + η12 · (1 − β2 ) ν − ∥∇L(w)∥2
β2 · 12 ∥∇S(w)∥2 + (1 − β2 ) 4ν
η4
h
i
2
∥∇L(w)∥2
β2 ⟨−∇L(w), ∇S(w)⟩ + (1 − β2 ) S(w)
−
2
4
η
β2 · 12 ∥∇S(w)∥2 + (1 − β2 )S(w)2 /η 2

.

(simplify)

2

2

(ν = η S(w)
at EOS)
4

which indeed recovers eq. (27). On the other hand, when αu (w, ν) is positive, σ 2 = 0 and the central flow reduces to
the stable flow eq. (25).
Normalized gradient descent As β2 → 0, Scalar RMSProp becomes normalized gradient descent, and the formula
eq. (27) for σ 2 (w; η, β2 ) reduces to:
σ 2 (w; η) =

η 2 ∥∇L(w)∥2
−
.
4
S(w)2

In practice, we observe that the first term generally dominates the second term. Thus, for NGD with one unstable
eigenvalue, we have approximately:
σ 2 (w; η) ≈

η2
.
4

(110)

This clearly illustrates how σ 2 grows monotonically with η.
Does the loss decrease? Whereas the gradient descent central flow decreases the loss monotonically (Proposition 1),
this is not true for the Scalar RMSProp central flow. Indeed, we have seen that with one unstable eigenvalue, the
Scalar RMSProp central flow takes the form:


2
1 2
dw
=−
∇L(w) + σ (w; η, β2 )∇S(w) .
dt
S(w)
2
Thus, by the chain rule, the rate of change in the loss is given by:


dL
dw
= ∇L(w),
dt
dt


2
1 2
2
=−
∥∇L(w)∥ + σ (w; η, β2 ) ⟨∇L(w), ∇S(w)⟩ .
S(w)
2
If progressive sharpening holds, i.e. if ⟨∇L(w), ∇S(w)⟩ < 0, then the second term is acting to increase L, and for
sufficiently large values of σ 2 , this increase will outweigh the decrease from the first term, causing L to go up. Indeed,
if progressive sharpening holds, then dL
dt > 0 so long as:
σ 2 (w; η, β2 ) >

2∥∇L(w)∥2
.
− ⟨∇L(w), ∇S(w)⟩

This can indeed occur. For instance, if we consider the case of normalized gradient descent (β2 → 0) and if we make
the approximation described in eq. (110), then dL
dt > 0 so long as the learning rate η satisfies:
s
8∥∇L(w)∥2
η>
.
(111)
− ⟨∇L(w), ∇S(w)⟩
60

Thus, for sufficiently large learning rates η, the train loss will go up rather than down under the central flow.
The effect of the hyperparameters η, β2 We can use the Scalar RMSProp central flow to reason about the effect
of the algorithm’s hyperparameters. In Section 4.3, we discussed the case of a single eigenvalue at EOS; we showed
that at any point w ∈ Rd in weight space, the strength σ 2 of the implicit sharpness regularization is monotonically
increasing in η. We also showed that β2 monotonically interpolates σ 2 between a certain value for NGD (β2 = 0) and
a certain value for GD (β1 = 1). Proposition 5 extends both of these results to the fully general setting where more
than one eigenvalue can be at EOS, and eigenvalues can enter or leave EOS.
Proposition 5. Fix an initial point w = w(0). Let U ⊆ Rd be the top eigenspace of H(w), and assume that ∇H(w)
has full rank on U. For a choice of parameters η, β2 , let ν = ν(0) = η 2 S(w)2 /4 so that the effective sharpness is 2.
Run the Scalar RMSProp central flow starting at (w, ν) with parameters (η, β2 ) to get {(w(t), ν(t), Σ(t))}t≥0 . Then:
• tr Σ(0) is non-decreasing in η, i.e. larger learning rate leads to larger oscillations. Furthermore, dS(w)
dt
non-increasing in η, i.e. larger learning rates lead to more curvature reduction.

t=0

is

• tr Σ(0) is either non-decreasing or non-increasing in β2 , i.e. β2 has a monotonic effect on oscillation size.
Furthermore, if tr Σ(0) is non-decreasing in β2 , dS(w)
is non-increasing in β2 and vice-versa, i.e. larger
dt
t=0
oscillations always lead to smaller curvature.
Before proving Proposition 5, we begin with the following simple lemma for SDCPs:
Lemma 4. If 0 ⪯ X ⊥ Y ⪰ 0 and 0 ⪯ X ′ ⊥ Y ′ ⪰ 0, then ⟨X ′ − X, Y ′ − Y ⟩ ≤ 0.
Proof.
X ′ − X, Y ′ − Y = X ′ , Y ′ − X ′ , Y − X, Y ′ + ⟨X, Y ⟩ ≤ 0.
| {z } | {z } | {z } | {z }
≥0

=0

≥0

=0

The heavy lifting will be done by the following result:
Proposition 6. For some scalar parameter s ∈ R, consider the family of SDCP’s:
Σ(s) = SDCP(α(s), β(s)),
where the parameterized matrix α(s) ∈ Sym(Rk ) and tensor β(s) ∈ Sym(Rk )⊗2 have the form:
α(s) := α0 + scα I

and

β(s) := β0 + scβ I ⊗ I,

for matrix α0 ∈ Sym(Rk ), tensor β0 ∈ Sym(Rk )⊗2 , and scalars cα , cβ ∈ R. Assume β0 ≻ 0 and cβ > 0. Then for
any s ≥ 0,
• If cα > 0, then tr Σ(s) is non-increasing in the parameter s.
• In general, tr Σ(s) is monotonic (either non-increasing or non-decreasing) in the parameter s.
Furthermore, if λ(s) := λmin (α0 + β0 [Σ(s)]), then λ(s) and tr Σ(s) are co-monotone:
(tr Σ(s′ ) − tr Σ(s))(λ(s′ ) − λ(s)) ≥ 0,
meaning they either increase together or decrease together in s.
Proof. Define the helper function F (s) := s(cα + cβ tr Σ(s)), so that:
α(s) + β(s)[Σ(s)] = α0 + scα I + β0 [Σ(s)] + scβ tr Σ(s)I
= α0 + β0 [Σ(s)] + F (s)I.
61

(112)

For any s, s′ ≥ 0, Lemma 4 implies that:
0 ≥ Σ(s′ ) − Σ(s), α(s′ ) + β(s′ )[Σ(s′ )] − α(s) − β(s)[Σ(s)]
= Σ(s′ ) − Σ(s), β0 [Σ(s′ ) − Σ(s)] + (F (s′ ) − F (s))I
= β0 [Σ(s′ ) − Σ(s), Σ(s′ ) − Σ(s)] + (tr Σ(s′ ) − tr Σ(s))(F (s′ ) − F (s))
≥ (tr Σ(s′ ) − tr Σ(s))(F (s′ ) − F (s)),
where the last line is because β0 is positive definite. Thus we see that tr Σ(s) and F (s) are counter-monotone:
(tr Σ(s′ ) − tr Σ(s))(F (s′ ) − F (s)) ≤ 0.

(113)

Next, we can write F (s′ ) − F (s) as:
F (s′ ) − F (s) = cα (s′ − s) + cβ (s′ tr Σ(s′ ) − s tr Σ(s))
= (s′ − s)(cα + cβ tr Σ(s)) + cβ s′ [tr Σ(s′ ) − tr Σ(s)].
Plugging this into eq. (113) gives:
0 ≥ (tr Σ(s′ ) − tr Σ(s))(s′ − s)(cα + cβ tr Σ(s)) + cβ s′ [tr Σ(s′ ) − tr Σ(s)]2
≥ (tr Σ(s′ ) − tr Σ(s))(s′ − s)(cα + cβ tr Σ(s)),

(114)

since s′ , cβ ≥ 0 by assumption. First, if cα , cβ > 0 then since tr Σ(s) ≥ 0, eq. (114) implies that
(tr Σ(s′ ) − tr Σ(s))(s′ − s) ≤ 0,
i.e. that tr Σ(s) is non-increasing in s, which was the first stated claim. More generally, for any pair s, s′ ≥ 0,
applying eq. (114) to both s, s′ and the reverse s′ , s yields:

(tr Σ(s′ ) − tr Σ(s))(s′ − s)(cα + cβ tr Σ(s)) ≤ 0 and (tr Σ(s′ ) − tr Σ(s))(s′ − s) cα + cβ tr Σ(s′ ) ≤ 0,
which implies that the sign of cα + cβ tr Σ(·) must be the same at both s and s′ . Since this hold for arbitrary s, s′ we
conclude that the sign of cα + cβ tr Σ(·) must be the same everywhere. Thus, eq. (114) implies that tr Σ(s) must be
monotone in s, which proves the second stated claim.
Finally, we will prove λ(s) := λmin (α0 + β0 [Σ(s)]) is co-monotone with tr Σ(s). To do so, we will prove that:
λ(s) = max (λmin (α0 ), −F (s)) .

(115)

To show this, we split into the two cases based on whether α(s) is PSD (or equivalently whether λmin (α0 ) + scα ≥ 0).
First, if α(s) ⪰ 0 then Σ(s) = 0 is the unique solution to the SDCP, so λ(s) = λmin (α0 ). In addition, α(s) ⪰ 0 is
equivalent to λmin (α0 ) + scα ≥ 0 and since F (s) = scα whenever Σ(s) = 0, this implies that λmin (α0 ) ≥ −F (s)
which proves eq. (115) when α(s) ⪰ 0.
Next, suppose that α(s) ̸⪰ 0, or equivalently that λmin (α0 ) + scα < 0. This implies that Σ(s) ̸= 0 in order for it to
satisfy the SDCP. Thus, there exists some v ∈ span Σ(s). Since span Σ(s) ⊆ ker[α(s) + β(s)[Σ(s)]], we have that:
v ∈ ker [α(s) + β(s)[Σ(s)]] ⇐⇒ (α0 + β0 [Σ(s)])v = −F (s)v,
which means that −F (s) is an eigenvalue of α0 + β0 [Σ(s)]. Meanwhile, we also have that:
α(s) + β(s)[Σ(s)] ⪰ 0 ⇐⇒ α0 + β0 [Σ(s)] ⪰ −F (s)I,
which means that −F (s) lower-bounds all eigenvalues of α0 + β0 [Σ(s)]. From these, we can conclude that −F (s) is
the smallest eigenvalue of α0 + β0 [Σ(s)], i.e. λ(s) = −F (s). Finally, note that complementarity implies that
0 = ⟨Σ(s), α0 + β0 [Σ(s)] + F (s)I⟩ = ⟨Σ(s), α0 ⟩ + β0 [Σ(s), Σ(s)] +F (s) tr Σ(s) ≥ tr Σ(s)[λmin (α0 ) + F (s)].
| {z } |
{z
}
≥0

≥tr Σ(s)λmin (α0 )

62

Since Σ(s) ̸= 0 and Σ(s) ⪰ 0, we must have tr Σ(s) > 0 so we can divide by tr Σ(s) to get −F (s) ≥ λmin (α0 )
which completes the proof of eq. (115).
To conclude, we observe that since g(x) := max(λmin (α0 ), −x) is non-increasing in x, and F is counter-monotone
with tr Σ, it immediately follows that λ(s) = g(F (s)) is co-monotone with tr Σ. This completes the proof.
We are now ready to prove Proposition 5:
Proof of Proposition 5. Let U ∈ Rd×k be a basis for U(w), let HU (w) = U ⊤ H(w)U ∈ Sym(Rk ), and let
∇HU (w) ∈ Sym(Rk ) ⊗ Rd be the tensor defined as ∇HU (w)ij,p =

∂[u⊤
i H(w)uj ]
.
∂wp

2
To simplify notation, define γ := 1−β
β2 , and note that this is decreasing in β2 .

The time derivative of the sharpness at t = 0 is given by:


dHU (w)
dS(w)
.
= λmax
dt t=0
dt
t=0
2

2

U
Since ν = η S(w)
, we have that dH
4
dt is given by:

dHU (w)
= −(αH + βH [X]),
dt
t=0
where:
αH :=

2
∇HU (w)[∇L(w)]
S(w)

and

βH :=

1
∇HU (w)∇HU (w)⊤ ,
S(w)

and X is the covariance of the oscillations within the basis U , i.e. Σ(0) = U XU ⊤ . This is, in turn, defined as the
solution to an SDCP:
X = SDCP(α, β),
where:


1
α = αH + γ 2
η




2
S(w)
∥∇L(w)∥2 I − γ
I,
S(w)
2

and:



1
β = βH + γ 2 (2S(w)) I ⊗ I.
η
For the η result, observe that α, β can be written in the form α = α0 +
α0 = αH − γ S(w)
2 I,



1
η2



cα I and β = β0 +



1
η2



cβ I ⊗ I, where:




2
cα = γ
∥∇L(w)∥2 ,
S(w)

β0 = βH ,

cβ = γ(2S(w)).

Further, note that cα > 0. Therefore, Proposition 6 implies that tr X is monotonically non-increasing in η12 . Since
tr Σ(0) = tr X, and η12 is decreasing in η, this implies that tr Σ(0) is monotonically non-decreasing in η, as claimed.
Proposition 6 further implies that λmin (α0 + β0 [X]) = λmin (αH + βH [X] − γ S(w)
2 I) is co-monotone with tr X,
1
which means that it must be monotonically non-increasing in η2 , or monotonically non-decreasing in η. This
implies that λmin (αH + βH [X]) must also be monotonically non-decreasing in η, which implies that the negative
λmax (−(αH + βH [X])) = dS(w)
is monotonically non-increasing in η, as claimed.
dt
t=0
63

For the β2 result, observe that α, β can be written in the form α = α0 + γcα I and β = β0 + γcβ I ⊗ I, where:

 
 
2
S(w)
1
1
2
∥∇L(w)∥ −
(2S(w)).
, β0 = βH , cβ =
α0 = αH , cα =
2
η
S(w)
2
η2
Therefore, Proposition 6 implies that tr X is monotone in γ. Since tr Σ(0) = tr X, and γ is decreasing in β2 , this
implies that tr Σ(0) is also monotone in β2 , as was claimed. Proposition 6 further implies that λmin (α0 + β0 [X]) =
λmin (αH + βH [X]) is co-monotone with tr X = tr Σ(0). Thus its negative λmax (−(αH + βH [X])) = dS(w)
dt
t=0
must be counter-monotone with tr Σ(0), as was claimed.

Predicting time-averages As with gradient descent, the Scalar RMSProp central flow can predict the time-average
of various quantities, such as the loss or squared gradient norm, along the Scalar RMSProp trajectory. Recall that for
a function f (w), we use f¯(t) to denote the central flow’s prediction for the time-average E[f (wt )] at step t.
The prediction L̄(t) for the time-averaged loss at step t is:
√

E[L(wt )] ≈ L(w(t)) +

ν(t)
η tr Σ(t) =: L̄(t).

(116)

The prediction for the time-averaged squared gradient norm at step t is:
E[∥∇L(wt )∥2 ] ≈ ∥∇L(w(t))∥2 + 4ν(t)
tr Σ(t) =: ∥∇L∥2 (t).
η2

(117)

These can be derived along similar lines as√eqs. (73) and (74) for gradient descent, but using the Scalar RMSProp
complementarity condition H(w) Σ(t) = 2 ην(t) Σ(t).
As for predicting the covariance of the oscillations, let Σ(t) = V (t)Λ(t)V (t)⊤ be the (reduced) eigenvalue decomposition of the rank-k matrix Σ(t), where V (t) ∈ Rd×k and Λ(t) ∈ diag(Rk ). Then the central flow predicts that the
variance of oscillations along the i-th eigenvector of Σ(t) should be equal to the i-th eigenvalue of Σ(t):

2 
⊤
E vi (t) (wt − w(t))
= λi (t).
(118)
Practical implementation When implementing the Scalar RMSProp central flow, we treat Scalar RMSProp as an
instance of a more general class of adaptive preconditioned methods that is described in Section A.5. Please refer to
that section for details on how we discretize the central flow in practice.

A.4

RMSProp

We will begin by describing the stability condition for preconditioned gradient descent on a quadratic. Consider
optimizing the quadratic L(w) = 12 w⊤ Hw using preconditioned gradient descent with preconditioner P ≻ 0:
w ← w − P −1 ∇L(w) = w − P −1 Hw = (I − P −1 H)w.

(119)

The matrix P −1 H is non-symmetric, but is similar to the symmetric matrix P −1/2 HP −1/2 , and thus has the same
eigenvalues, which are necessarily real. If all eigenvalues of P −1 H are contained in (0, 2) then all eigenvalues of
(I − P −1 H) are contained in (−1, 1), and hence w → 0 exponentially fast. Otherwise, the dynamics will diverge
along the right eigenvectors of P −1 H with eigenvalues outside this range.60
We now derive the RMSProp central flow. We model the RMSProp iterates {wt } as oscillating around a central
flow w(t) with mean zero and covariance Σ(t). That is, if δt := wt − w(t) denotes the displacement between
RMSProp and the central flow (“the oscillation”), then E[δt ] = 0 and E[δt δt⊤ ] = Σ(t). Let ν(t) := E[vt ] model
60

Each right eigenvector u of P −1 H corresponds to an eigenvector v of P −1/2 HP −1/2 via the relation v = P 1/2 u.

64

the time-averaged νt , and we will frequently neglect the distinction between the two, implicitly assuming that νt
concentrates tightly around ν(t).
A similar argument as for gradient descent implies that the time-averaged gradient is approximately:
E[∇L(wt )] ≈ ∇L(w(t)) + 12 ∇H(w(t))⊤ [Σ(t)].

(120)

Meanwhile, we approximate the time-average of the elementwise squared gradient as:




E ∇L(wt )⊙2 = E ∇L(w(t) + δt )⊙2
(wt = w(t) + δt )
h
i
⊙2
≈ E (∇L(w(t)) + H(w(t))δt )
(Taylor expansion)


= E ∇L(w(t))⊙2 + 2∇L(w(t)) ⊙ H(w(t))δt + (H(w(t))δt )⊙2 (expand the square)


= ∇L(w(t))⊙2 + E (H(w(t))δt )⊙2
(E[δt ] = 0)


=⇒ E[∇L(wt )⊙2 ] ≈ ∇L(w(t))⊙2 + E (H(w(t))δt )⊙2 .
(121)
Recall that RMSProp can be viewed as preconditioned gradient descent with the dynamic preconditioner:
√
P (ν) := diag( ν/η).

(122)

We therefore define stability for RMSProp by the condition λmax (P (ν)−1 H(w)) ≤ 2, and we define the critical
subspace U(w, ν) as the eigenspace of the effective Hessian P (ν)−1 H(w) corresponding to the eigenvalue 2:


U(w, ν) := ker P (ν)−1 H(w) − 2I .
We model RMSProp as oscillating within the critical subspace, i.e. we assume that δt ∈ U(w(t), ν(t)). This allows
us to simplify eq. (121) as:




E ∇L(wt )⊙2 ≈ ∇L(w(t))⊙2 + E (H(w(t))δt )⊙2


= ∇L(w(t))⊙2 + 4 E (P (ν(t))δt )⊙2
(H(w)δt = 2P (ν)δt )


4
= ∇L(w(t))⊙2 + 2 ν(t) ⊙ E δt⊙2
(P (ν) = diag[ν 1/2 /η])
η
h
i
4
= ∇L(w(t))⊙2 + 2 ν(t) ⊙ E diag[δt δt⊤ ]
(v ⊙2 = diag[vv ⊤ ])
η
4
= ∇L(w(t))⊙2 + 2 ν(t) ⊙ diag[Σ(t)].
(E[δt δt⊤ ] = Σ(t))
η
Based on these time averages, we make the central flow ansatz:
h
i
dw
η
= − √ ⊙ ∇L(w) + 21 ∇H(w)⊤ [Σ(t)]
dt
ν


dν
1 − β2
4ν
⊙2
=
∇L(w) + 2 ⊙ diag[Σ(t)] − ν .
dt
β2
η

(123)

As with the previous optimizers, to determine Σ(t) we impose three conditions for all times t:
1. PSD: As a covariance matrix, Σ(t) is positive semidefinite (PSD), i.e. Σ(t) ⪰ 0.
2. Stability: The effective sharpness remains bounded by 2, i.e. λmax (P (ν(t))−1 H(w(t))) ≤ 2.
3. Complementarity: The oscillations are contained within the critical subspace, i.e. span Σ(t) ⊆ U(w(t), ν(t)).

65

To express these conditions more concisely, we define the matrix-valued function A(w, ν) as:
A(w, ν) := 2P (ν) − H(w),
so that stability is A(w, ν) ⪰ 0, and the critical subspace is precisely U(w, ν) = ker A(w, ν). Using this notation,
the above conditions can be summarized more compactly as the semidefinite complementarity relation:
0 ⪯ Σ(t) ⊥ A(w(t), ν(t)) ⪰ 0.
Definition 8 (RMSProp Central Flow, Differential Complementarity Problem). We say that {w(t), ν(t)}t≥0 follow
the RMSProp central flow if for almost all t ≥ 0, they satisfy eq. (123) along with the complementarity relation:
0 ⪯ Σ(t) ⊥ A(w(t), ν(t)) ⪰ 0.
Since this DCP can be expressed in the general form described in Section A.6, we can use Lemma 7 to convert it into
an equivalent ODE. In particular, if w
e := [w, ν]⊤ denotes the augmented state, we can write eq. (123) as:
"
#
"
#
−P (ν)−1 ∇L(w)
− 21 P (ν)−1 ∇H(w)⊤ [Σ]
dw
e
= f (w)
e + B(w)[Σ]
e
where f (w)
e := 1−β2
and B(w)[Σ]
e
:= 1−β2
,
⊙2 − ν]
2
dt
β2 [∇L(w)
β2 · 4P (ν) diag Σ
and we can write the complementarity relation as 0 ⪯ Σ(t) ⊥ A(w(t))
e
⪰ 0, where:


1
1
√ ⊙ z = 2 diag[P (ν)−1 z]
∇A(w)
e = [−∇H(w), 2∇ν P (ν)] where ∇ν P (ν)[z] = diag
2η
2η ν
Therefore, Lemma 7 implies that
Σ(t) ∈ SDCPU (w,ν) (α(w, ν), β(w, ν))
where the matrix α(w, ν) ∈ Sym(Rd ) and tensor β(w, ν) ∈ Sym(Rd )⊗2 are defined by:
α(w, ν) := ∇A(w)[f
e (w)]
e
"


= −∇H(w) 2∇ν P (ν)

#
−P (ν)−1 ∇L(w)
1−β2
⊙2 − ν]
β2 [∇L(w)

= ∇H(w)[P (ν)−1 ∇L(w)] +



1 − β2 1
· 2 · diag P (ν)−1 ∇L(w)⊙2 − ν
β2
η

and
β(w, ν)[Σ] := ∇A(w)B(
e
w)[Σ]
e
"
#

 − 12 P (ν)−1 ∇H(w)⊤ [Σ]
= −∇H(w) 2∇ν P (ν)
1−β2
2
β2 · 4P (ν) diag Σ
= 12 ∇H(w)P (ν)−1 ∇H(w)⊤ [Σ] +

1 − β2 4
· 2 · diag[P (ν) diag[Σ]].
β2
η

Note that β(w)
e is indeed symmetric PSD, as for gradient descent and Scalar RMSProp:
1 − β2 4
· 2 diag[Σ], diag[Σ′ ] P (ν)
β2
η
P (ν)
2
1 − β2 4
=⇒ β(w̃)[Σ, Σ] = 12 ∇H(w)⊤ [Σ]
+
· 2 ∥diag[Σ]∥2P (ν) ≥ 0.
β2
η
P (ν)−1

D
E
β(w̃)[Σ, Σ′ ] = 12 ∇H(w)⊤ [Σ], ∇H(w)⊤ [Σ′ ]

+
−1

This gives us the ODE formulation of the RMSProp central flow:
66

∀z.

Definition 9 (RMSProp Central Flow, ODE Formulation). We say that {w(t), ν(t)}t≥0 follow the RMSProp central
flow if, for almost all t, they satisfy eq. (123) with
Σ(t) ∈ SDCPU (w(t),ν(t)) (α(w(t), ν(t)), β(w(t), ν(t))).
Predicting time-averages As with the previous optimizers, the RMSProp central flow can predict the time-average
of various quantities, such as the loss or squared gradient norm, along the RMSProp trajectory. Recall that for
a function f (w), we use f¯(t) to denote the central flow’s prediction for the time-average E[f (wt )] at step t. See
Section A.5 for the derivation of the following statements. We write P (t) := P (ν(t)) for brevity.
The prediction L̄(t) for the time-averaged loss at step t is:
E[L(wt )] ≈ L(w(t)) + tr [P (t) Σ(t)] =: L̄(t).

(124)

The prediction for the time-averaged squared gradient norm at step t is:
h
i
E[∥∇L(wt )∥2 ] ≈ ∥∇L(w(t))∥2 + 4 tr P (t) Σ(t) P (t)⊤ =: ∥∇L∥2 (t).

(125)

As for predicting the covariance of the oscillations, let Σ(t) = V (t) Λ(t) V (t)⊤ be the (reduced) eigenvalue
decomposition of the rank-k matrix P 1/2 (t) Σ(t) P 1/2 (t), where V (t) ∈ Rd×k and Λ(t) ∈ diag(Rk ). Then the
central flow predicts that the P -whitened variance of oscillations along the i-th eigenvector of P (t)1/2 Σ(t) P (t)1/2
should be equal to the i-th eigenvalue of that matrix:

2 
⊤
1/2
E vi (t) P (t) (wt − w(t))
= λi (t).
(126)
See Section A.5 for a discussion of why we predict the P -whitened covariance of oscillations.
Practical implementation When implementing the RMSProp central flow, we treat RMSProp as an instance of a
more general class of adaptive preconditioned methods that is described in Section A.5. Please refer to that section
for details on how we discretize the central flow in practice.
A.4.1

Stationarity analysis

In this appendix, we provide supporting derivations for our analysis of RMSProp’s stationary preconditioner.
Stationary preconditioner The RMSProp central flow eq. (123) is a joint flow over (w, ν). However, suppose that
the ν dynamics occur “fast” relative to the w dynamics, so that ν is always “fully caught up” to the current w. For any
fixed w, solving eq. (123) for the stationarity condition dν
dt = 0 gives the condition:
ν = ∇L(w)⊙2 +

4
ν ⊙ diag[Σ].
η2

(127)

In addition, by the PSD, stability, and complementarity conditions, we have:
0 ⪯ Σ ⊥ 2P (ν) − H(w) ⪰ 0

where

P (ν) := diag

√ 
ν
η

.

(128)

We will now show that for any w, there is a unique pair ν, Σ that satisfies eqs. (127) and (128).
p We will denote this
pair as ν(w) and Σ(w). We will further show that the corresponding preconditioner diag( ν(w)/η) is the unique
optimum to the following convex optimization problem over diagonal preconditioners:
arg min
P diagonal, P ⪰0

tr(P ) + η12 ∥∇L(w)∥2P −1

such that

where ∥v∥2P −1 := v ⊤ P −1 v. We will denote this preconditioner as P (w).
67

H(w) ⪯ 2P,

(129)

We show this in two parts. First, in Proposition 7 we prove that if ν, Σ satisfy eqs. (127) and (128), then P (ν) is an
optimum for eq. (129). Then, in Proposition 8 we show that the solution to eq. (129) is unique, implying that this
must be the unique optimum. At the end of this section, we describe how to numerically compute P (w) and ν̄(w)
when H(w) is so large that it can only be feasibly accessed via matrix-vector products.
√
Proposition 7. Define P (ν) := diag( ν/η). For any g ∈ Rd , H ∈ Sym(Rd ), if ν ∈ Rd , Σ ∈ Sym(Rd ) satisfy:
ν = g ⊙2 + η42 ν ⊙ diag(Σ)

(130)

0 ⪯ Σ ⊥ 2P (ν) − H ⪰ 0,

(131)

then P (ν) is an optimum for the convex program:
arg min
P diagonal, P ⪰0

tr(P ) + η12 g ⊤ P −1 g

such that

H ⪯ 2P.

(132)

Proof. Parameterizing P = diag(p) for a vector p ∈ Rd , the convex program eq. (132) can be written as:
min

p∈Rd , p≥0

d
X

pi +

i=1

1 gi2
η 2 pi

such that

H ⪯ 2 diag(p).

(133)

Introducing a dual variable Z ⪰ 0 for the semidefinite constraint, the Lagrangian is:

d 
X
1 gi2
L(p, Z) =
− ⟨Z, 2 diag(p) − H⟩ .
pi + 2
η pi

(134)

i=1

Therefore, the KKT conditions are:
1 g2
1 − 2 i2 − 2Zii = 0
η pi
{z
|
stationarity

∀i,
}

H ⪯ 2 diag(p),
{z
}
|
primal feasibility

Z⪰0 ,
| {z }

dual feasibility

2 diag(p) − H ⊥ Z ,
{z
}
|

(135)

complementary slackness
√

as well as p ≥ 0. We claim that if (ν, Σ) satisfy eqs. (130) and (131), then (p, Z) = ( ην , η22 Σ) solve these KKT
conditions. First, elementwise dividing both sides of eq. (130) by ν gives:
 
gi2
4
1−
Σii = 0 ∀i
(136)
−
νi
η2
√
which is equivalent to the stationary condition in eq. (135) after substituting p = ν/η and Z = η22 Σ. Next, the
√
remaining parts of eq. (135) are implied by eq. (131). Finally, we must have ν ≥ 0 or P (ν) would be imaginary.
We now prove that the solution to the optimization problem eq. (132) is unique. A custom proof is needed because,
while both the objective and constraints of eq. (132) are convex, the objective is not strictly convex.
Proposition 8. For any g ∈ Rd , H ∈ Sym(Rd ), the solution to eq. (132) is unique.
Proof. Assume there are two minimizers P, P ′ and let p := diag(P ), δ := diag(P ′ − P ). Then by convexity,
diag[p + ϵδ] also minimizes eq. (129) for any ϵ ≤ 1. Therefore, differentiating the objective function in this direction
gives:

X 
1 gi2
δi 1 − 2 2 = 0.
(137)
η pi
i

68

Taking another derivative implies that:
X g2
i

i 2
δ = 0.
p3i i

(138)

This implies that δi = 0 in any direction where gi ̸= 0. Let I := {i : gi = 0}. From the above, δi = 0 for i ∈ I c , so
pI c = p′I c . It remains to show equality on I. Define G by
(
pi i ∈ I c
G[vI ]i :=
.
vi i ∈ I

Define A⊤ [vI ] := diag G[vI ] ⊕ diag[vI ]. Then both pI , p′ I minimize the reduced SDP
X
min
pi s.t. 12 H ⊕ 0|I|×|I| ⪯ A⊤ (pI ).
pI

i∈I

Now we apply de Carli Silva and Tunçel (2018, Proposition 1) with (A, 1|I| ). First, note that A[Id+|I| ] = 21|I| which
satisfies the first condition. Next, for any y ̸= 0, we can take z = |y| to satisfy the second condition, as in the proof of
(de Carli Silva and Tunçel, 2018, Corollary 2). Therefore pI = p′I , and as we have already shown equality on I c , we
must have p = p′ .

Stationary flow Suppose that the ν dynamics (preconditioner adaptation) happen infinitely fast relative to the
w dynamics (optimization), so that we can treat ν as always being fixed at its current stationary value ν(w). This
motivates the stationary flow:
Definition 10 (RMSProp Stationary Flow). We say that {w(t)}t≥0 follow the RMSProp stationary flow if, for almost
all t, they satisfy
h
i
dw
η
= −p
⊙ ∇L(w) + 12 ∇H(w)⊤ [Σ(t)]
(139)
dt
ν(w)
with
Σ(t) ∈ SDCPU (w(t),ν(w(t))) (α(w(t), ν(w(t))), β(w(t), ν(w(t)))).
Note that Σ(t) is defined as the solution to an SDCP, and is not, in general, equal to Σ̄(w(t)). However, we often
expect these to be close, especially for small β2 , and we can consider the approximation where Σ(t) is replaced with
Σ̄(w(t))61 :
h
i
η
dw
= −p
⊙ ∇L(w) + 12 ∇H(w)⊤ [Σ̄(w)] .
dt
ν̄(w)

(140)

Limit of large η In the limit of large η, the second term in the objective eq. (129) vanishes, and the stationary
preconditioner P (w) tends towards the minimum-trace, diagonal stable preconditioner, which we denote P̂ (w):
P̂ (w) := arg min

tr(P )

such that

H(w) ⪯ 2P.

(141)

P diagonal, P ⪰0

Note that the dual to this semidefinite program is the following semidefinite program:
Ẑ(w) := arg max ⟨Z, H(w)⟩
Z⪰0

subject to

Zii ≤ 21

(142)

2
Note that Σ(t) would be exactly equal to Σ̄(w(t)) if α(w, ν) and β(w, ν) had only the second terms (i.e. the ones that scale with 1−β
)
β2
and not the first terms.

61

69

and a primal/dual optimal pair P̂ (w), Ẑ(w) must satisfy the KKT conditions:
Ẑii (w) = 12 whenever P̂ii (w) > 0.

0 ⪯ Ẑ(w) ⊥ 2P̂ (w) − H(w) ⪰ 0,

(143)

In the limit of large η, the stationary EMA ν̄(w) and the stationary oscillation covariance Σ̄(w) become:
ν̄(w) → η 2 diag[P̂ (w)]◦2

and Σ̄(w) →

η2
Ẑ(w).
2

As a result, the approximation eq. (140) can be shown to be equivalent to:
h
i
2
dw
= −P̂ (w)−1 ∇L(w) + η4 ∇ tr P̂ (w) .
dt
D
E
This is because tr P̂ (w) = Ẑ(w), H(w) by duality, so by Danskin’s theorem:

(144)

(145)

∇ tr P̂ (w) = ∇H(w)⊤ [Ẑ(w)]
= η22 ∇H(w)⊤ [Σ̄(w)].
2

Hence, η4 ∇ tr P̂ (w) = 12 ∇H(w)⊤ [Σ̄(w)], and eq. (145) follows.
Connection to MaxCut Interestingly, the SDP eq. (142) is precisely the SDP relaxation of MaxCut (Goemans and
Williamson, 1995) where the Laplacian matrix of the graph is given by 12 H(w). Meanwhile, the SDP eq. (141) that
defines P̂ is the dual to the MaxCut SDP relaxation.
Numerically solving for the stationary preconditioner When the problem dimension d is small, the optimization
problem eq. (129) can be solved exactly using a standard convex solver, e.g. cvxpy. But when d is large (e.g. the
number of weights in a reasonably sized neural network), solving eq. (129) exactly is not practical, as it is not even
practical to materialize the matrix H ∈ Rd×d . Therefore, we instead solve eq. (129) using a fixed point iteration
which only requires access to H using matrix-vector products.
We parameterize Σ in the factorized form Σ = DD⊤ where D ∈ Rd×r and r is intended to be at least as large as the
rank of Σ. This is similar to the Burer-Monteiro factorization (Burer and Monteiro, 2005). We start from a random
initial guess for D and then iteratively update D and ν by:
ν ← g ⊙2 + (HD)⊙2 1,
η
D ← diag[ν −1/2 ]HD.
2
where the second update uses the ν that was just computed in the first update.

(146)
(147)

If this update scheme reaches a fixed point (D, ν), then we have:
ν = g ⊙2 + (HD)⊙2 1,
HD = η2 diag[ν

1/2

(148)

]D.

(149)
√

If (D, ν) satisfy these two conditions, as well as the stability condition H ⪯ 2 diag( ν/η), then it can be shown that
Σ = DD⊤ and ν satisfy eqs. (130) and (131). Indeed, Σ ⪰ 0 holds by construction, eq. (130) follows by substituting
√
√
eq. (149) into eq. (148), and eq. (149) implies [2 diag( ν/η) − H]D = 0 which implies 2 diag( ν/η) − H ⊥ Σ
provided that the stability condition holds.
√
Thus, if the update scheme reaches a fixed point eqs. (148) and (149), and if the stability condition H ⪯ 2 diag( ν/η)
√
is also satisfied there, then we know that P = diag( ν/η) solves the optimization problem eq. (129).
Empirically, we observe that this update scheme does reach a fixed point in practice. We moreover observe that if r is
sufficiently large (in particular, if it is as large as the rank of the true Σ), then the stability condition is satisfied at this
fixed point, implying that the corresponding preconditioner indeed solves eq. (129). On the other hand, we observe
that if r is too small (less than the rank of the true Σ), then while the update scheme converges to a fixed point, the
stability condition is not satisfied there.
70

Algorithm 1: Solving for the Stationary Preconditioner
Input: Gradient g ∈ Rd , Hessian-vector oracle v 7→ Hv, learning rate η, rank parameter r, number of steps
nsteps, tolerance parameter tolν
Output: ν, P , and D
Initialize D ∈ Rd×r with standard normal entries;
for i = 1 to nsteps do
if i > 1 then
νprev ← ν;
end
ν ← g ⊙2 + sum_rows((HD)⊙2 ) ;
D ← η2 diag[ν −1/2 ]HD ;
end
if

∥ν−νprev ∥2
≥ tolν then
∥ν∥2

return "Error: more steps needed";
end
√
p ← ν/η;
if λmax (diag(p−1/2 )H diag(p−1/2 )) > 2 then
return "Error: higher r needed";
end
return ν, diag(p), D;

A.5

General Class of Adaptive Preconditioned Methods

In this section, we derive a central flow for a general class of adaptive preconditioned methods that subsumes gradient
descent, Scalar RMSProp, and RMSProp as special cases. In these special cases, this flow will reduce to the central
flows that we have already derived, and whose accuracy we have verified empirically. However, we do not claim
that the central flow derived in this section will be empirically accurate for any method within this class. Rather, we
include this section because it allows us to treat all three considered optimizers in a unified manner, and to easily
generalize our central flows to minor variants of the same algorithms (e.g. gradient descent with a learning rate
schedule, RMSProp with bias correction). Our implementation in code is based on this formulation.
We consider methods which update some “optimizer state” ν ∈ Rdν based on the current gradient, and then take a
preconditioned gradient step using some preconditioner P (ν) that is derived from this state:
wt+1 = wt − P (νt )−1 ∇L(wt ).

νt = νt−1 + G(νt−1 , ∇L(wt )),

(150)

Here, G : Rdν × Rd → Rdν determines how the optimizer state ν ∈ Rdν is updated based on the gradient, and
P : Rdν → Sym(Rd ) determines how the optimizer state affects the preconditioner.
This formulation is very general and includes a wide variety of optimizers including:
• Vanilla GD: ignore ν and set P = η −1 I.
• GD with a learning rate schedule η(t): Set G(ν, g) = 1 so that ν(t) = t, and P (t) = η(t)−1 I.
√
• Vanilla RMSProp:62 Set G(ν, g) = (1 − β2 )[g ⊙2 − ν] and P (ν) = diag[ ν/η]
• RMSProp with ϵ, bias correction, and learning rate schedule η(t): Set ν = [v, t], G([v, t], g) = [(1−β2 )[g ⊙2 −
v], 1] and define
r

v
1
P ([v, t]) =
diag
+ϵ .
η(t)
1 − β2t
2
This formalism doesn’t directly handle our small β2 correction of β2 → 1−β
. We view this correction as “orthogonal” in the sense that
β2
it comes when deriving a stable flow analogue of the discrete time update νt = νt−1 + G(νt−1 , ∇L(wt )) when this takes the form of an EMA.

62

71

Note that this trick of embedding t into the state variable ν allows us to automatically derive central flows for any
smooth hyperparameter schedule (e.g. η(t), β2 (t), ϵ(t)) as a simple corollary.
Remark 3. Not all algorithms of the form eq. (150) are necessarily sensible optimizers. Moreover, we will see below
that the central flow is only well-defined if G and P satisfy a certain condition (Remark 4). Thus, it may make sense
for future work to further restrict the formulation eq. (150).
The stability of the algorithm requires λmax (P (ν)−1 H(w)) ≤ 2 or equivalently H(w) ⪯ 2P (ν). We define
A(w, ν) := 2P (ν) − H(w),

(151)

so that stability is equivalent to A(w, ν) ⪰ 0. We define the critical subspace by U (w, ν) := ker A(w, ν).
To derive the central flow, we model wt = w(t) + δt with E[δt ] = 0, E[δt δt⊤ ] = Σ(t), and δt ∈ ker A(w(t), ν(t)).
For ease of notation, we will sometimes use g(w) as a shorthand for the gradient ∇L(w).
We will first compute the time-average of G, i.e. E[G(ν, g(wt ))] = E[G(ν, g(w(t) + δt ))]. A first-order Taylor
expansion of g around any point w yields:
g(w + δ) ≈ g(w) + H(w) δ.
Meanwhile, a second-order Taylor expansion of G in its second argument yields:
G(v, g + ∆g) ≈ G(ν, g) + ∇g G(ν, g)⊤ ∆g + 12 ∇2g G(ν, g)[∆g ∆g ⊤ ],

(152)

where ∇2g G(ν, g) ∈ Rdν ⊗ Sym(Rd ) is a tensor that represents the Hessian of each entry of G. Putting these together,
with ∆g = H(w)δ, we have:
G(ν, g(w + δ)) ≈ G(ν, g(w)) + ∇g G(ν, g)⊤ H(w)δ + 12 ∇2g G(ν, g)[H(w) δδ ⊤ H(w)],
Taking the time-average over δ with E[δ] = 0 and E[δδ ⊤ ] = Σ yields:
E[G(v, g(w + δ)] ≈ G(ν, g(w)) + 12 ∇2g G(ν, g(w))[H(w) Σ H(w)].

(153)

Since H(w)Σ = 2P (ν)Σ, we have H(w)ΣH(w) = 4P (ν)ΣP (ν), and therefore the second term becomes:
E[G(v, g(w + δ)] ≈ G(ν, g(w)) + 2∇2g G(ν, g(w))[P (ν) Σ P (ν)].

(154)

These motivate the central flow ansatz:
h
i
dw
= −P (ν)−1 ∇L(w) + 21 ∇H(w)⊤ [Σ]
dt
dν
= G(ν, g(w)) + 2∇2g G(ν, g(w))[P (ν) Σ P (ν)].
dt

(155)

We say that {w(t), ν(t), Σ(t)}t≥0 satisfy the DCP formulation of the central flow if for almost all t ≥ 0 they satisfy
eq. (155) along with the complementarity relation:
0 ⪯ Σ(t) ⊥ A(w(t), ν(t)) ⪰ 0.
As for gradient descent, Scalar RMSProp, and RMSProp, we can invoke Lemma 7 to obtain an equivalent ODE
formulation that makes Σ(t) explicit. We begin by writing eq. (155) as:




dw
e
− 21 P (ν)−1 ∇H(w)⊤ [Σ]
−P (ν)−1 ∇L(w)
= f (w)
e + B(w)[Σ]
e
where f (w)
e :=
and B(w)[Σ]
e
:=
.
2∇2g G(ν, g(w)) [P (ν) Σ P (ν)]
G(ν, g(w))
dt
72

where the complementarity relation is 0 ⪯ Σ ⊥ A(w)
e ⪰ 0 with ∇A given by:
∇A(w)
e = [−∇H(w), 2∇P (ν)].
Therefore, Lemma 7 implies that:
Σ(t) ∈ SDCPU (w,ν) (α(w, ν), β(w, ν)),
where the matrix α(w, ν) ∈ Sym(Rd ) and tensor β(w, ν) ∈ Sym(Rd )⊗2 are defined by:
α(w)
e := ∇A(w)[f
e (w)]
e



 −P (ν)−1 ∇L(w)
= −∇H(w) 2∇P (ν)
G(ν, g(w))
= ∇H(w)[P (ν)−1 ∇L(w)] + 2∇P (ν)[G(ν, g(w))]
and
β(w)[Σ]
e
:= ∇A(w)B(
e
w)[Σ]
e



= −∇H(w) 2∇P (ν)


− 21 P (ν)−1 ∇H(w)⊤ [Σ]
2∇2g G(ν, g(w))[P (ν)ΣP (ν)]

= 21 ∇H(w)P (ν)−1 ∇H(w)⊤ [Σ] + 4∇P (ν) ∇2g G(ν, g(w)) [P (ν) Σ P (ν)].
Remark 4. For gradient descent, Scalar RMSProp, and RMSProp, the β we derived was always symmetric PSD.
However, at the current level of generality this is not always true:
D
E
D
E
⊤
2
′
β(w, ν)[Σ, Σ′ ] = 21 ∇H(w)⊤ [Σ], ∇H(w)⊤ [Σ′ ]
+
∇P
(ν)
[Σ],
∇
G(ν,
g(w))[P
(ν)Σ
P
(ν)]
.
g
−1
P (ν)

While the first term is symmetric in Σ, Σ′ , the second is not necessarily symmetric, so results about existence and
uniqueness for solutions to the SDCP may no longer hold.
Predicting time-averages The central flow’s predictions for time-averages can be computed as follows. Recall that
for a function f (w), we use f¯(t) to denote the central flow’s prediction for the time-average E[f (wt )] at step t. In
what follows, we write P (t) := P (ν(t)) for brevity.
The prediction for the time-averaged loss at time t is:
h
i
E[L(wt )] ≈ E L(w(t)) + ∇L(w(t))⊤ δt + 12 δt⊤ H(w(t))δt
= L(w(t)) + 12 ⟨H(w(t)), Σ(t)⟩
= L(w(t)) + 12 tr [2P (t)Σ]

(Taylor expansion)
(E[δt ] = 0, E[δt δt⊤ ] = Σ(t))
(HΣ = 2P Σ)

= L(w(t)) + tr [P (t) Σ(t)]
=: L̄(t).

(156)

Similarly, the prediction for the time-averaged squared gradient norm at time t is:


E[∥∇L(wt )∥2 ] ≈ E ∥∇L(w(t)) + H(w(t))δt ∥2
= ∥∇L(w(t))∥2 + H 2 (w(t)), Σ(t)
i
h
= ∥∇L(w(t))∥2 + 4 tr P (t) Σ(t) P (t)⊤
=: ∥∇L∥2 (t).

(157)

The central flow can also predict the covariance of the oscillations. The natural basis to examine the oscillations is
the one in which the dynamics of each coordinate are decoupled under preconditioned gradient descent on the local
73

quadratic Taylor approximation. Thus, we define the P -whitened displacement δ̂t := P (t)1/2 δt = P (t)1/2 (wt −w(t)),
and the P -whitened covariance matrix of the oscillations E[δ̂t δ̂t⊤ ] = P (t)1/2 Σ(t) P (t)1/2 . Let V (t) Λ(t) V ⊤ (t) be
the eigenvalue decomposition of P (t)1/2 Σ P (t)1/2 , and define xt := V (t)⊤ δ̂t = V (t)⊤ P (t)1/2 (wt − w(t)) as the
P -whitened displacement between the discrete optimizer and the central flow along the eigenvectors V (t). Then the
central flow predicts that:
⊤
1/2
E[xt x⊤
E[δt δt⊤ ]P (t)1/2 V (t) = V (t)⊤ P (t)1/2 Σ(t) P (t)1/2 V (t) = Λ(t).
t ] = V (t) P (t)

In particular, the P -whitened variance of oscillations along the i-th eigenvector of P (t)1/2 Σ(t) P (t)1/2 is predicted
to be the i-th eigenvalue of that matrix:

2 
⊤
1/2
E vi (t) P (t) (wt − w(t))
= λi (t).
(158)
Basis-dependent version We can use the general recipe given in Section A.6 to obtain a basis-dependent version
of the central flow ODE that can be computed in time linear in d, when the preconditioner is diagonal.
Fix a time t, and we will often abbreviate w(t), ν(t), and P (ν(t)) as w, ν and P . Let U ∈ Rd×k be a basis for the
critical subspace U(w, ν). Define HU (w) := U ⊤ H(w)U ∈ Sym(Rk ) and PU (ν) := U ⊤ P (ν)U ∈ Sym(Rk ), as
well as their gradients ∇HU (w) ∈ Sym(Rk ) ⊗ Rd and ∇PU (ν) ∈ Sym(Rk ) ⊗ Rdν . Explicitly:
∇HU (w)ij = ∇w [u⊤
i H(w)uj ]

and

∇PU (ν)ij = ∇ν [u⊤
i P (ν)uj ].

(159)

Similarly, let ∇2 GP U (ν, g(w)) ∈ Rdν ⊗ Sym(Rk ) be the tensor defined as:
∇2 GP U (ν, g(w))q,ij = (P ui )⊤ ∇2g G(ν, g(w))q (P uj ).
Then the central flow takes the form:
h
i
dw
= −P (ν)−1 ∇L(w) + 12 ∇HU (w)⊤ [X]
dt
dν
= G(ν, g(w)) + 2∇2 GP U (ν, g(w))[X]
dt
X ∈ SDCPRk (αU (w, ν), βU (w, ν))


αU (w, ν) = ∇HU (w) P (ν)−1 ∇L(w) + 2∇PU (ν)[G(ν, g(w))]
βU (w, ν) = 12 ∇HU (w)P (ν)−1 ∇HU (w)⊤ + 4∇PU (ν) ∇2 GP U (ν, g(w)).

(160)

(161)
(162)
(163)
(164)
(165)

Suppose that we pick the basis U to be orthonormal w.r.t the preconditioner P , i.e. U ⊤ P U = I. Then the central
flow’s prediction eq. (156) for the time-averaged train loss can be efficiently computed as:
h
i
L̄(t) := L(w(t)) + tr P U XU ⊤
h
i
= L(w(t)) + tr XU ⊤ P U
= L(w(t)) + tr [X].
Similarly, the prediction eq. (157) for the time-averaged squared gradient norm can be computed as:
h
i
∥∇L∥2 (t) = ∥∇L(w(t))∥2 + 4 tr (P U )X(P U )⊤ .

(166)

(167)

As for the oscillation covariance, we can compute both sides of eq. (158) without materializing Σ(t) in full. If
⊤ is the eigenvalue decomposition of X, and V (t) := P 1/2 U U , then V (t) Λ(t) V (t)⊤ is the
X = UX Λ(t) UX
X
eigenvalue decomposition of P 1/2 Σ(t) P 1/2 . Note that X and UX are dependent on the basis U , while V (t) and
Λ(t) are independent of U .
Warning: variation in notation Although gradient descent can be cast as an instance of an adaptive preconditioned
method (with P = η −1 I), the U defined here is different from the U in Section A.2.4, as the U there was orthonormal,
i.e. U ⊤ U = I, whereas the U here is orthonormal w.r.t the preconditioner P , i.e. U ⊤ P U = I or U ⊤ U = ηI. As a
result, the X defined here differs from the X defined in Section A.2.4 by a factor of η.
74

A.5.1

Discretizing the central flow for a generic adaptive preconditioned method

We describe our general procedure for discretizing DCPs in Section A.6.1. Let us now describe how this general
procedure specializes to the case of our generic adaptive preconditioned method.
We use w(t) , ν (t) , and Σ(t) to denote our estimate for the central flow’s w(t), ν(t), and Σ(t). Let ϵ > 0 be the
discretization step size, e.g. ϵ = 0.25. For some tolerance τ > 0, e.g. τ = 0.05, we will regard eigenvalues of the
effective Hessian P (ν)−1 H(w) greater than 2 − τ as those which might become unstable in the next discretization
time step.63
At each discretization step, we do the following. Abbreviate w = w(t) and P = P (ν (t) ). First, we compute all
eigenvalues of the effective Hessian P −1 H(w) that are greater than 2 − τ , as well as the corresponding eigenvectors.
Let k be the number of such eigenvalues, let D ∈ diag(Rk ) be a diagonal matrix containing such eigenvalues on
the diagonal, and let U ∈ Rd×k be the corresponding eigenvectors, normalized so that they are orthonormal w.r.t P ,
that is, U ⊤ P U = I. For example, one could set U = P −1/2 Ũ , where the columns of Ũ ∈ Rd×k are orthonormal
eigenvectors of P −1/2 H(w)P −1/2 .
Then, we compute the tensors ∇HU , ∇PU , ∇2 GP U as in eqs. (159) and (160), though note that U now refers to a
basis of eigenvectors whose eigenvalues are almost 2 rather than exactly equal to 2. Then, we compute αU and βU as
in eqs. (164) and (165). Then, we solve the following k-dimensional SDCP:
X (t) = SDCPRk (2I − D + ϵ αU , ϵ βU ),
so that Σ(t) = U X (t) U ⊤ . Then, we update the weights and optimizer state via:
i
h
w(t+ϵ) = w(t) − ϵ P (ν (t) )−1 ∇L(w(t) ) + 21 ∇HU⊤ [X (t) ]
h
i
(t+ϵ)
(t)
(t)
(t)
2
(t)
(t)
(t)
ν
= ν + ϵ G(ν , g(w )) + 2∇ GP U (ν , g(w ))[X ] .

(168)

(169)
(170)

To predict the time-average of the train loss, the squared gradient, and the covariance of the oscillations, we use
eq. (166), eq. (167), and eq. (158), respectively.

A.6

Differential Complementarity Problems

In this appendix, we give some brief background on differential complementarity problems (Stewart, 2011), and we
describe how to turn these into ordinary differential equations.
A differential complementarity problem (DCP) (Stewart, 2011) is a dynamical system that is defined in terms of a
complementarity relation. In this paper, we will consider DCPs of the form:
d
dt w(t) = f (w(t)) + B(w(t))[Σ(t)]

where

0 ⪯ Σ(t) ⊥ A(w(t)) ⪰ 0.

(DCP)

Here f : Rd → Rd , B : Rd → Rd ⊗ Sym(Rd ) and A : Rd → Sym(Rd ) are given, and w(t) ∈ Rd and
Σ(t) ∈ Sym(Rd ) are dynamical variables that must respect eq. (DCP) for almost all times t.
Example 1. The following 1-dimensional DCP models a particle moving to the right which hits a wall at w = 1:
dw(t)
= 1 − Σ(t)
dt

where

0 ≤ Σ(t) ⊥ 1 − w(t) ≥ 0.

When w(t) < 1, complementarity forces Σ(t) = 0 so that dw(t)
dt = 1, i.e. the particle moves to the right. Once
= 0 in order to prevent
the particle has made contact with the wall at w = 1, Σ(t) must jump to 1 so that dw(t)
dt+
⋆
the particle from violating the condition w(t) ≤ 1. Thus, if t denotes the t when w(t) hits the wall, then we have
Σ(t) = 0 for t < t⋆ and Σ(t) = 1 for t > t⋆ . The choice of Σ(t) at t⋆ itself is arbitrary and does not affect the DCP.
63

We remark that there is a slight discrepancy between the procedure described in Section A.6.1 and this one, as we look at eigenvalues of
P H close to 2, rather than eigenvalues of 2P − H close to 0. Our procedure here is equivalent to looking at the generalized eigenvalues of
2P − H with respect to P , i.e. (2P − H)v = λP v as these eigenvalues are exactly equal to the eigenvalues of P −1 H shifted by 2. These
procedures produce the same trajectory as ϵ → 0, and even at finite ϵ we believe that the trajectories will empirically remain very close.
−1

75

Throughout our derivations, we will search for a Σ(t) that is right-continuous, e.g. in the above setting we would
define Σ(t⋆ ) = 1, so that the leftwards force is applied the instant that w(t) reaches 1.
To turn eq. (DCP) into an ordinary differential equation with an explicit right-hand side, we will now prove some
additional constraints that the system must satisfy.
First, we prove that if A(w(t)) ⪰ 0 for all times t, then the right derivative dtd+ A(w(t)) must be PSD over the
subspace ker A(w(t)). We abbreviate A(w(t)) as A(t).
Lemma 5 (Semidefinite Tangent Cone). Let A : R → Sym(Rd ) be a matrix-valued function such that A(t) ⪰ 0 for
d
all t. If A is right-differentiable at t, dtd+ A(t) ⪰ U 0 where U = ker A(t). If A is differentiable at t, dt
A(t) U = 0.
Proof. Let u ∈ ker A(t). Then for any ϵ > 0,




⊤ A(t + ϵ)
⊤ A(t + ϵ) − A(t)
u=u
u ≥ 0.
u
ϵ
ϵ
Taking ϵ → 0 proves that dtd+ A(t) ⪰ U 0. By reversing time if A is left differentiable at t then dtd− A(t) ⪯ U 0.
d
Combining these inequalities shows that if A is differentiable at t then dt
A(t) U = 0.
Next, we show that if the complementarity relation 0 ⪯ Σ(t) ⊥ A(w(t)) ⪰ 0 holds for all times t, then the right
d
derivative dtd+ A(w(t)) must satisfy its own complementarity relation: 0 ⪯ Σ(t) ⊥ dt
A(w(t)) ⪰ ker A(w(t)) 0.
Lemma 6 (Differentiating the complementarity relation). If A : R → Sym(Rd ) is right-differentiable at t, 0 ⪯
Σ(t) ⊥ A(t) ⪰ 0 for all t, and Σ(·) is right-continuous at t, then 0 ⪯ Σ(t) ⊥ dtd+ A(t) ⪰ U 0 where U = ker A(t).
Proof. The only condition that needs to be checked is Σ(t) ⊥ dtd+ A(t), since Σ(t) ⪰ 0 is given and dtd+ A(t) ⪰ U 0
is implied by Lemma 5. First, for any ϵ > 0, we have Σ(t + ϵ) ⊥ A(t + ϵ), which implies:




A(t + ϵ) − A(t)
A(t)
Σ(t + ϵ),
= − Σ(t + ϵ),
≤ 0.
ϵ
ϵ
Taking ϵ → 0 and using right-continuity of Σ(·) implies Σ(t), dtd+ A(t) ≤ 0. Meanwhile, the assumption 0 ⪯
Σ(t) ⊥ A(t) ⪰ 0 implies that span Σ(t) ⊆ U. Since dtd+ A(t) ⪰ U 0 by Lemma 5, we have that Σ(t), dtd+ A(t) ≥ 0.
Since Σ(t), dtd+ A(t) is both ≤ 0 and ≥ 0, it must be = 0, i.e. Σ(t) ⊥ dtd+ A(t) as desired.
dw
Note that as discussed in Section A.1.6, throughout the rest of this work, when we write dw
dt we either mean dt+ or we
mean that the statement holds for almost all t.

Now we have enough information to solve for Σ(t). Explicitly, if U(w) := ker A(w), we can define
α(w) := ∇A(w)[f (w)] ∈ Sym(Rd ),

β(w) := ∇A(w)B(w) ∈ Sym(Rd )⊗2 .

Lemma 7. If w(t), Σ(t) satisfy eq. (DCP) for almost all t, and Σ(·) is right-continuous, then
Σ(t) ∈ SDCPU (w) (α(w(t)), β(w(t)))
for all t, where U(w) = ker A(w). Furthermore if β(w(t)) is symmetric positive definite as an operator on
Sym(U(w)) for all t, then Σ(t) = Σ(w(t)) is unique.
Proof. First, Lemma 6 implies that
d
A(w(t)) ⪰ U (w(t)) 0
0 ⪯ Σ(t) ⊥ dt

76

where U(w) = ker A(w(t)). By the chain rule,
 
d
dw
= ∇A(w(t))[f (w(t)) + B(w(t))[Σ(t)]].
A(w(t)) = ∇A(w(t))
dt
dt
d
Therefore, using that Σ ∈ Sym(U (w)), we can expand dt
A(w(t)) as:

d
A(w(t)) = α(w(t)) + β(w(t))[Σ(t)],
dt
which implies that:
Σ(t) ∈ SDCPU (w(t)) (α(w(t)), β(w(t))).

This lets us turn eq. (DCP) into an ODE:
dw
= f (w) + B(w)[Σ] where Σ ∈ SDCPU (w) (α(w), β(w))
dt
α(w) := ∇A(w)[f (w)], β(w) := ∇A(w)B(w), U(w) := ker A(w)
(DCP to ODE)
We can also define an equivalent basis-dependent version of this ODE, which makes clear that simulating the ODE
only requires time and space that scale linearly in the dimension d. For each t, let U ∈ Rd×k denote a basis for the
critical subspace U(w(t)), where k = dim U(w(t)). Then we can write eq. (DCP) in terms of U as follows:
dw
= f (w) + B(w)[U XU ⊤ ]
dt
X ∈ SDCPRk (αU (w), βU (w))
αU (w) := U ⊤ α(w)U ∈ Sym(Rk )


βU (w)[X] := U ⊤ β(w)[U XU ⊤ ] U ∈ Sym(Rk )

(DCP to ODE, basis-dependent)
∀X ∈ Sym(Rk ).

k
k ⊗2
Thus, to compute dw
dt , one need only compute αU ∈ Sym(R ) and βU ∈ Sym(R ) , then solve a k-dimensional
dw
k
SDCP to obtain X ∈ Sym(R ), then form dt in terms of X.

In practice, due to nonsmoothness of the DCP, we do not discretize it by computing dw
dt and then taking Euler steps.
Instead, we discretize the DCP directly, as described below in Section A.6.1.
Relation to the broader DCP literature In the literature on differential complementarity problems, it is a standard
practice to turn a DCP into an ODE by differentiating the constraints. In particular, eq. (DCP) is known as a pure
index-one DCP (Stewart, 2011, Section 5.2.1), because it needs to be differentiated exactly once in order to be turned
into an explicit ODE.
Existence and uniqueness of the ODE Existence and uniqueness for projected gradient flows (including the
gradient descent central flow) is guaranteed by Cornet (1983), under the assumption that β(w) is positive definite
on Sym(U(w)) for all w, where U(w) = ker A(w) denotes the critical subspace. We view this assumption as mild.
Existence and uniqueness can be argued for DCPs more generally (including the other central flows) under the
assumption that β is positive definite over all Sym(Rd ) (Stewart, 2011). However, this assumption is too strong for
our setting; indeed, for gradient descent, β has rank at most d and hence cannot span the d(d+1)
dimensional space of
2
Sym(Rd ) whenever d > 1. However, we suspect that under additional reasonable regularity conditions, this could be
relaxed to a condition that β need only be positive definite over Sym(ker A(w)).

77

Smoothness of the ODE The ODE is non-smooth at breakpoints where the dimension of U(w) = ker A(w)
changes. In between the breakpoints, Σ(t) is continuous and w(t) is differentiable. Moreover, the solution to
d
the SDCP is given by the linear inverse Σ(t) = U βU−1 (w)[αU (w)]U ⊤ and we have dt
A(t) U (w) = 0. At the
breakpoints, however, Σ(t) is discontinuous and w(t) is not differentiable (although they are right-continuous and
right-differentiable, respectively)
A.6.1

Discretizing the DCP

We now describe how we discretize eq. (DCP) in practice. Let ϵ > 0 denote the discretization step size, and consider
a grid of time steps T = {t0 , t1 , . . . , tN } that are spaced apart by ϵ. For any t ∈ T , let w(t) denote our estimate for
the central flow’s w(t), and let Σ(t) denote our estimate for the central flow’s Σ(t). In what follows, we will use
t ∈ T to denote the current time step and t + ϵ ∈ T to denote the next one.
It is not straightforward to discretize eq. (DCP) using time-stepping. For example, it doesn’t make sense to search for
a pair w(t+ϵ) , Σ(t) that satisfies
 


h
i


w(t+ϵ) = w(t) + ϵ f w(t) + B w(t) Σ(t)
and 0 ⪯ Σ(t) ⊥ A w(t) ⪰ 0,
as this would always be satisfied by Σ(t) = 0. We could instead search for a pair w(t+ϵ) , Σ(t) which satisfies


h
i


 
and 0 ⪯ Σ(t) ⊥ A w(t+ϵ) ⪰ 0,
w(t+ϵ) = w(t) + ϵ f w(t) + B w(t) Σ(t)
where the complementarity constraint is enforced at time t + ϵ, rather than at time t. However, it is still difficult to
handle this constraint due to the nonlinearity of A. We will therefore linearize A around w(t) . Define Alin (w) as the
linearization of A around the current point w(t) :



h
i
Alin (w) := A w(t) + ∇A w(t) w − w(t) .
(171)
We can therefore look for a choice of w(t+ϵ) and Σ(t) that together satisfy

h
i

 


and 0 ⪯ Σ(t) ⊥ Alin w(t+ϵ) ⪰ 0.
w(t+ϵ) = w(t) + ϵ f w(t) + B w(t) Σ(t)

(172)

This implies the following set of conditions on Σ(t) alone:



h 


h
ii
0 ⪯ Σ(t) ⊥ A w(t) + ϵ ∇A w(t) f w(t) + B w(t) Σ(t) ,

(173)

which we recognize as precisely an SDCP:
 


h 
i

 

Σ(t) ∈ SDCPRd A w(t) + ϵ ∇A w(t) f w(t) , ϵ ∇A w(t) B w(t) .

(174)

Thus one could solve for Σ(t) above, and then take an Euler step on w:
 


h
i
w(t+ϵ) ← w(t) + ϵ f w(t) + B w(t) Σ(t) .
Unfortunately, it is not possible to directly run this “idealized” time-stepping scheme, as it is impractical to formulate
or solve an SDCP over Rd . Therefore, we instead approximate it by projecting it onto the bottom eigendirections
of A which are “close” to the stability threshold at 0. In particular, if U ∈ Rd×k is a basis of these directions and
U = span U , then we require Σ(t) ∈ Sym(U), and we enforce:


0 ⪯ Σ(t) ⊥ Alin w(t+ϵ) ⪰ U 0.
(175)

78

This boils down to an SDCP over the low-dimensional subspace U:
 


h 
i

 

Σ(t) ∈ SDCPU A w(t) + ϵ ∇A w(t) f w(t) , ϵ ∇A w(t) B w(t) .

(176)

As described in Section A.1.4, solving this SDCP only requires solving a k-dimensional SDCP:
 





Σ(t) = U X (t) U ⊤ , X (t) = SDCP AU w(t) + ϵ αU w(t) , ϵ βU w(t) ,

(177)

where AU (w) ∈ Sym(Rk ), αU (w) ∈ Sym(Rk ), and βU (w) ∈ Sym(Rk )⊗2 are defined as
AU (w) := U ⊤ A(w) U,
αU (w) := U ⊤ [∇A(w)[f (w)]]U,
h
h
h
iii
βU (w)[X] := U ⊤ ∇A(w) B(w) U XU ⊤ U.

(178)
(179)
(180)

Then, we update the weights using:
 


h
i
w(t+ϵ) = w(t) + ϵ f w(t) + B w(t) U X (t) U ⊤ .
For the central flows in this work, A and B are such that αU , βU , and dw
dt can be computed efficiently.
Correctness of this discretization scheme Under suitable conditions on f, A, B, this time-stepping scheme will
converge to the solution of the DCP as ϵ → 0 (Stewart, 2011). For technical reasons involving the rank of ∇AB, our
paper will not rigorously prove the convergence of this time-stepping scheme for our central flows. However, we do
empirically observe that the dynamics converge to a limiting curve as ϵ → 0.

A.7

Continuous-time approximation to an EMA

In this appendix, we justify our continuous-time approximation to an exponential moving average (EMA).
Consider a discrete-time EMA of a continuous process f (t) that is sampled at integer-valued times:
νt = β2 νt−1 + (1 − β2 )f (t).

(181)

What is a good continuous-time approximation ν(t) to νt ? This question arises when we derive stable and central
flows for Scalar RMSProp and RMSProp (where f is the squared gradient norm or the elementwise squared gradient,
respectively).
Subtracting νt−1 from both sides of eq. (181) and rearranging yields:
νt − νt−1 = (1 − β2 )(f (t) − νt−1 ).

(182)

This suggests the following continuous-time approximation to eq. (181):
ν ′ (t) = (1 − β2 )(f (t) − ν(t)).

(183)

However, this approximation breaks down for small β2 . Indeed, as β2 → 0, the discrete-time EMA eq. (181) adapts
“infinitely fast” so that νt ≈ f (t), yet the naive continuous-time approximation eq. (183) does not have this property.
Therefore, to obtain a continuous-time approximation that works well even for small β2 , we use the following
alternative approximation:


1 − β2
ν ′ (t) =
(f (t) − ν(t)).
(184)
β2
The β2 in the denominator ensures that when β2 ≈ 0, ν(t) adapts “infinitely” fast to f (t).
79

To give intuition for our approximation eq. (184), suppose that we are using an EMA to track a one-dimensional
linear process f (t) (i.e. f ′ (t) is constant). Then both the discrete and continuous time EMAs have closed forms. The
closed-form solution to the discrete time EMA eq. (181) can be written as:
νt = f ( t − τ ) + β2t [ν0 − f (−τ )]
| {z }
|
{z
}
time-lag

where

β2
.
1 − β2

τ :=

(185)

burn in

Since the burn-in term vanishes exponentially with time, the steady state is that νt will track f (t) with a “time delay”
β2
of τ := 1−β
.
2
Similarly, for a continuous-time EMA of the form ν ′ (t) = γ[f (t) − ν(t)] for some γ, the general solution is:
ν(t) = f ( t − τ ) + e−γt [ν(0) − f (−τ )]
| {z }
{z
}
|
time-lag

where

τ := 1/γ.

(186)

burn in

Thus, the steady state is that ν(t) will track f (t) with a “time delay” of τ = 1/γ.
To ensure that the continuous-time EMA asymptotically matches the discrete-time EMA, we need to set γ so that the
time delays match:
β2
1
1 − β2
=
=⇒ γ =
,
1 − β2
γ
β2
This motivates our choice of scaling factor in eq. (184).
Note that if we instead wanted to match the burn in, we would set γ = log(1/β2 ). However, we believe it is more
important to match the time-delay than the burn in.

A.8

Miscellaneous math

Here, we state some miscellaneous mathematical facts that are used elsewhere.
Fact 1. Let L(w) be three-times differentiable, and let S(w) := λ1 (H(w)) denote the top Hessian eigenvalue.
Suppose that the top Hessian eigenvalue at w has multiplicity 1, and let u be the top Hessian eigenvector at w. Then,
for w = w + xu, we have:
2

∇L(w) = ∇L(w) + S(w)xu + x2 ∇S(w) + o(x2 ).

(187)

Proof. We begin by writing the Taylor expansion of ∇L(w) around w:
2

∇L(w) = ∇L(w) + H(w)xu + x2 ∇3 L(w)[u, u] + o(x2 ).

(188)

Because u is an eigenvector of H(w) with eigenvalue S(w), the second term can be simplified to S(w)xu. Finally,
by Danskin’s theorem (or equivalently the standard formula for the derivative of an eigenvalue):


h
i
⊤
∇w S(w) = ∇w max v H(w)v = ∇w u⊤ H(w)u = ∇3 L(w)[u, u]
(189)
∥v∥=1

where u is the argmax of the second expression, i.e. the top eigenvector of the Hessian at w.
Fact 2. For PSD matrices X, Y ⪰ 0, it holds that tr(XY ) = 0 if and only if span X ⊥ span Y .
Proof. First, if span X ⊥ span Y , then span Y ⊆ ker X. Thus, it must hold for every u that XY u = 0, implying
that XY = 0 and hence tr(XY ) = 0.

80

For the other direction, assume that tr(XY ) = 0. Then:


2
0 = tr(XY ) = tr Y 1/2 X 1/2 X 1/2 Y 1/2 = X 1/2 Y 1/2 .
F

This norm can only be zero if X 1/2 Y 1/2 = 0. Multiplying on the right by Y 1/2 and the left by X 1/2 gives XY = 0.
This implies that span X ⊥ span Y , as for any x ∈ span X and y ∈ span Y , we have x = Xu and y = Y v for some
u, v, and so
x⊤ y = (Xu)⊤ (Y v) = u⊤ XY v = 0.

Note that since the span of a symmetric matrix is orthogonal to its kernel, span X ⊥ span Y is equivalent to
span X ⊆ ker Y and to span Y ⊆ ker X. Thus the following corollary is immediate:
Corollary 1. For PSD matrices X, Y ⪰ 0, it holds that tr(XY ) = 0 if and only if span X ⊆ ker Y .

81

B

Experimental Details

B.1

Implementation details

Our code can be found at: http://github.com/centralflows/centralflows.
In order to reuse code between the central flows for gradient descent, Scalar RMSProp, and RMSProp, we cast all three
optimizers as instances of the generic adaptive preconditioned method that is described in Section A.5. This template
assumes that the weights are updated via a preconditioned gradient step of the form wt+1 = wt − P (νt )−1 ∇L(wt ),
where P (νt ) is a preconditioner that is derived from some optimizer state νt that is in turn updated based on the
gradients. For example, for gradient descent with learning rate η, the preconditioner is simply P = η −1 I. The
effective Hessian is defined as P (νt )−1 H(wt ), and the EOS condition is that the largest eigenvalue of this matrix (the
effective sharpness S eff ) is 2. See Section A.5 for more information.
Eigenvalue computation To regularly recompute the top eigenvalues and eigenvectors of the effective Hessian, we
use the LOBPCG algorithm (Knyazev, 2001), which only requires access to the Hessian via Hessian-vector products,
and which allows us to warm-start using the previously computed eigenvectors. We were originally inspired by the
LOBPCG implementation in Jax’s jax.experimental.sparse.linalg (Bradbury et al., 2018).
How many eigenvalues to track? For all processes (i.e. discrete optimizers, central flows, stable flows), we
track all eigenvalues of the effective Hessian that are above the threshold 1.5. We then track the same number of
eigenvalues of the Hessian. Note that for gradient descent and Scalar RMSProp the Hessian eigenvalues are trivially
related to the effective Hessian eigenvalues, whereas for RMSProp we need to do an extra eigenvalue solve to obtain
the Hessian eigenvalues.
Discretizing the stable flow To discretize the stable flows (e.g. gradient flow), we use Euler’s method. To
discretize for one unit of time, we pick some integer nsubsteps, we set ϵ = 1/nsubsteps, and we repeat
w ← w + ϵ dw
dt for nsubsteps times. We dynamically adapt nsubsteps based on the current effective sharpness
eff
S . The basic criterion of update stability requires that ϵ < 2/S eff . To be on the safe side, and to guard against any
implicit discretization effects, we enforce the stronger condition that ϵ < 0.5/S eff , or equivalently that nsubsteps
≥ ⌈2S eff ⌉. We also enforce a floor of nsubsteps ≥ 4. Thus, we set nsubsteps = max(4, ⌈2S eff ⌉).
Since discretizing the stable flow would take a prohibitively long time in regions of weight space where the effective
sharpness is too high, we automatically terminate the stable flow if the effective sharpness exceeds a certain threshold
(we used 100).
Discretizing the central flow To discretize the central flows, we use the scheme described in Section A.5.1. This is
in turn an instance of the general scheme described in Section A.6.1 for discretizing differential complementarity
problems. At each discretization time step, we do the following:
1. We compute the top eigenvalues and eigenvectors of the effective Hessian P (ν)−1 H(w). In particular, we
compute all eigenvalues greater than 2 − τ for some small tolerance τ > 0 (we use 0.05), as well as the
corresponding eigenvectors. Let k be the number of such eigenvalues. To compute eigenvalues of the nonsymmetric matrix P (ν)−1 H(w), we first use warm-started LOBPCG to compute the eigenvalues D ∈ diag(Rk )
and orthonormal eigenvectors Ũ ∈ Rd×k of the symmetric matrix P (ν)−1/2 H(w)P (ν)−1/2 . The eigenvalues
of P (ν)−1 H(w) are then D, and the eigenvectors are U = P (ν)−1/2 Ũ ∈ Rd×k . Note that these eigenvectors
are orthonormal w.r.t the preconditioner P (ν), i.e. U ⊤ P (ν)U = I.
2. We compute the third-derivative tensor ∇U H(w) ∈ Sym(Rk ) ⊗ Rd defined in eq. (159) by looping over all
d
pairs (ui , uj ) of columns of U ∈ Rd×k and computing the third derivative ∇w [u⊤
i H(w)uj ] ∈ R , which can
be done usingautomatic differentiation. Note that due to the symmetry, it is only necessary to compute and
store the k+1
“upper triangular” entries of this tensor, rather than the full k 2 .
2
3. We compute the tensor ∇PU (ν) ∈ Sym(Rk ) ⊗ Rdν defined in eq. (159), which measures the gradient of the
preconditioner w.r.t the optimizer state ν ∈ Rdν . Our implementation uses automatic differentiation to do this,
82

though one could also simply hard-code the derivatives for the various optimizers of interest. We similarly
compute the tensor ∇2 GP U (ν, g(w)) ∈ Rdν ⊗ Sym(Rk ) defined in eq. (160), where g(w) = ∇L(w) and dν is
the dimension of the optimizer state ν. This tensor measures the Hessian of the optimizer
state w.r.t the gradient

of the weights. For these tensors, we also only need to compute and store the k+1
upper
triangular entries.
2
4. Using ∇HU (w), ∇PU (ν), and ∇2 GP U (ν, g(w)), we compute the tensors αU (w, ν) ∈ Sym(Rk ) and βU (w, ν) ∈
Sym(Rk )⊗2 defined in eqs. (164) and (165). Here we simply materialize the full tensors, as k is small.
5. We solve the semidefinite complementarity problem:
X = SDCPRk (2I − Λ + ϵ αU , ϵ βU ).
We do so by formulating the SDCP as a semidefinite-constrained quadratic program eq. (54), as described in
Section A.1.4, and solving this using the convex programming library cvxpy.
6. We take an Euler step of size ϵ on the weights w and optimizer state ν, as given in eqs. (169) and (170).
To discretize the flow for one unit of time, we pick some integer nsubsteps, set the discretization step size as
ϵ = 1/nsubsteps, and repeat the above process for nsubsteps times. Note that because the central flows keep
the effective sharpness controlled at 2, it is not necessary to dynamically adapt nsubsteps throughout training,
as nsubsteps ≥ 2 always sufffices to ensure stability. We therefore used the fixed values nsubsteps = 4 and
ϵ = 0.25.
The computational cost of each discretization step is dominated by the cost of computing the top eigenvalues and
eigenvectors in step 1 above, as well as the cost of computing the third derivatives in step 2 above. The computational
cost of the rest of the steps (including solving the SDCP) is negligible. The time complexity of each
 discretization
k+1
step scales quadratically with the number of eigenvalues that are at the edge of stability, k, as 2 = Θ(k 2 ) third
derivatives need to be computed.
Verifying the central flow To assess whether the central flow accurately models the discrete optimizer, we run
both processes simultaneously. That is, we repeatedly both (a) take a step on the discrete optimizer; and (b) discretize
the central flow for one unit of time. Let wt denote the discrete optimizer’s iterate at step t, and let w(t) denote our
estimate for the central flow solution at time t.
To verify that the central flow approximates the long-term weight-space trajectory of the discrete optimizer, we record
∥w(t) − wt ∥, the Euclidean distance between the discrete optimizer and the central flow.
We use eqs. (166) and (167) to compute the central flow’s predictions for the time-average of the train loss and
squared gradient norm. For any quantity f (e.g. loss or squared gradient norm), let f¯(w(t) ) denote the central flow’s
prediction for the time-average of f at step/time t. To assess the accuracy of this prediction, we compare {f¯(w(t) )}
against a Gaussian smoothing of the empirical time series {f (wt )}. That is, for each t, we compare:
!


X
1
j2
(t)
¯
P
f (w ) vs.
cj f (wt+j ) where cj = exp − 2 ,
2σ
j cj
j

where σ 2 is the bandwidth of the Gaussian kernel. We describe below momentarily how we determine σ 2 .
(t)

(t)

When predicting the covariance of the oscillations, we compare both sides of eq. (158). Let (λi , vi ) denote the i-th
D
E2
(t)
(t)
eigenvalue and eigenvector of the matrix P 1/2 (ν (t) ) Σ(t) P 1/2 (ν (t) ), and define xi := vi , P 1/2 (ν (t) )(w(t) − wt ) .
(t)

Then for each eigenvalue index i, we compare the predicted time series {λi } against a Gaussian smoothing of the
(t)
empirical time series {xi }. For gradient descent and Scalar RMSProp, where P is a scalar, we post-hoc rescaled
both quantities by P so as to report the oscillations in terms of Σ rather than P 1/2 ΣP 1/2 .
For some plots (e.g. those in the main paper), we picked the Gaussian kernel’s bandwidth by visual inspection (we
emphasize that it is not possible to turn a bad prediction into a good prediction by adjusting the bandwidth). In fact,
83

for some figures (e.g. Figures 15 and 18), we found it best to re-tune the bandwidth within an experiment, whenever
the number of unstable eigenvalues underwent a change. On the other hand, for the plots on the “bulk” experiments
section (Section E), we picked a single bandwidth somewhat arbitrarily and used this for all experiments.
Warm-start In our experiments, we first run the discrete optimizer for 10-15 steps and then use this as an
initialization for the discrete optimizer, central flow, and stable flow. The first reason why we do this is that the
effective sharpness is sometimes much larger than 2 at the original initialization (particularly for the adaptive
optimizers), yet comes down below 2 within the very first few steps of training. (The central flow is not currently
defined when the effective sharpness is greater than 2.) Another reason is that even when the effective sharpness is
less than 2 at initialization, we observed that the quality of the central flow approximation is sometimes enhanced by
this warm start. This is potentially due to the size of the gradients during the first few steps, and the central flow could
possibly be improved during this phase by incorporating the implicit gradient norm penalty from Barrett and Dherin
(2021). However, we think it is likely that this source of deviation between the discrete optimizer and the stable /
central flows is negligible in the long run (see Section C.1), and thus we hypothesize that the warm-starting could be
removed in cases where the effective sharpness is less than 2 at initialization.
Second-order midpoints When reporting metrics from the discrete optimizers (gradient descent, Scalar RMSProp,
and RMSProp, as opposed to their central flows), we usually report the top Hessian eigenvalues measured not at the optimizer iterates {wt } themselves, but at the (second order) midpoints {ŵt }, defined as ŵt := 14 [2wt − wt−1 − wt+1 ]
(so named because it is the midpoint of the midpoints 12 [wt − wt−1 ] and 12 [wt+1 − wt ]). This empirically makes the
Hessian trajectories slightly crisper (less “noisy”), while not altering the fundamental patterns.

B.2

Architecture details

Here we describe our architectures. Note that our code for all architectures can be found at:
http://github.com/centralflows/centralflows.
Both our derivations and the analytic formulas for the central flows rely on higher-order information about the loss
function (e.g. Hessians and third derivatives). As a result, we require that all of the architectures are smooth. This
rules out the commonly used ReLU activation (Nair and Hinton, 2010).
CNN Our CNN has four layers, an initial channel width of 32, and 3x3 convolutional kernels. It uses the GeLU
activation function, average pooling, and a linear readout layer.
ResNet We use a ResNet (He et al., 2016) with 20 layers and GeLU activations. We use GroupNorm (Wu and
He, 2018) in place of BatchNorm (Ioffe and Szegedy, 2015), as we empirically find that BatchNorm often leads to
sub/super-quadraticity (see the discussion in Section C.2).
Vision Transformer We use the Vision Transformer (ViT) (Dosovitskiy et al., 2021) implementation from
LucidRains (2024). Our ViT has depth 3, embedding dimension 64, number of heads 8, MLP dimension 256, and
patch size 4. We initialize the weights and biases of the final linear layer to 0.5 times the default, as this makes the
curvature lower at initialization, which allows us to run gradient descent experiments at a broader range of learning
rates. For unknown reasons, we found that the core PyTorch LayerNorm implementation (written in C++) leads to
third derivatives being computed incorrectly; thus, we substituted in an alternative implementation written in vanilla
PyTorch, which empirically fixed the issue.
LSTM Our LSTM (Hochreiter and Schmidhuber, 1997) has 2 layers, an embedding dimension of 48, and a hidden
dimension of 48.
(Sequence) Transformer Our sequence transformer has 4 layers, an embedding dimension of 32, an MLP
dimension of 128, and 4 attention heads. We disabled dropout, to make the network deterministic. As with the ViT
(see above), we initialize the weights and biases of the final linear layer to be zero, and we substitute a vanilla PyTorch
LayerNorm implementation in place of the default C++ LayerNorm implementation.

84

Mamba We use the Mamba (Gu and Dao, 2024) implementation from Torres-Leguet (2024). Our Mamba has 2
layers and a model dimension of 64. Unfortunately, the efficient Mamba kernel based on parallel scan did not work
with PyTorch higher-order autotiff, so we needed to use the naive implementation of Mamba, which is slow.

B.3

Dataset details

Here we describe our datasets. The code can be found at:
http://github.com/centralflows/centralflows.
CIFAR-10 We test the vision architectures on a subset of CIFAR-10 that contains 1000 training examples, all from
the first 4 CIFAR-10 classes. We use the standard preprocessing of subtracting the dataset-wide channel-wise mean,
and dividing by the dataset-wide channel-wise standard deviation. When training using MSE loss, we encode the
ground truth class as 1 and the others as 0.
Sorting We test the sequence architectures on the synthetic sorting task described in Karpathy (2020). The network
is fed a sequence of numbers and is then tasked (via a language modeling loss) with returning these numbers in sorted
order. We used numbers 1 through 4, and sequences of length 8. The size of the training datraset was usually 1,000
(except for Mamba, where it was 250).

85

C

Miscellaneous

C.1

Implicit gradient regularization

Recall from Section 3.2 that when gradient descent is stable, we approximate its trajectory by the gradient flow:
dw
= −η ∇L(w).
dt

(190)

In particular, the central flow automatically reduces to eq. (190) whenever sharpness S(w) < 2/η.
On the other hand, Barrett and Dherin (2021) argued that gradient descent with step size η should instead be
approximated by a modified gradient flow with a penalty on the squared gradient norm:
h
i
dw
η
= −η ∇ L(w) + ∥∇L(w)∥2 ,
dt
4
h
i
η
= −η ∇L(w) + H(w)∇L(w) .
2

(191)
(192)

Subsequently, Rosca et al. (2023) showed how to improve the approximation by incorporating higher-order penalties,
and Cattaneo et al. (2024) extended the approach to adaptive optimizers.
Empirically, we find that in the stable regime, the modified gradient flow eq. (191) is indeed a better approximation
to gradient descent than the vanilla gradient flow eq. (190). This observation is illustrated in Figure 25. However,
all things considered, we observe that in the stable regime, the vanilla gradient flow is already a good enough
approximation to gradient descent (Figure 25). While gradient descent does differ from gradient flow in deep learning,
the vast majority of this difference appears to be due to the curvature-reduction effect of oscillations in the edge
of stability regime, not to discretization error that manifests even in the stable regime. This point is illustrated in
Figure 26. Thus, in the interest of simplicity, we left out any implicit gradient regularizer from our central flows.

gradient descent
gradient flow
+ IGR penalty

train loss

0.5

gradient norm2

0.4

max hessian eigenvalue (sharpness)
200

0.3

0.4
0.3
0.2
0

500

1000
step

1500

150

0.2

100

0.1

50

0.0

0

network output #1

500

1000
step

1500

network output #2
0.15

0.4

0

0

500

1000
step

1500

distance to gradient descent

0.004
0.003

0.10

0.2

0.002
0.001

0.05
0

500

1000
step

1500

0

500

1000
step

1500

0.000

0

500

1000
step

1500

Figure 25: In the stable regime, the IGR penalty marginally improves the accuracy of the gradient flow
approximation, but this accuracy is already good. We train a network using gradient descent (blue), vanilla
gradient flow eq. (190) (orange, dashed) and gradient flow with the IGR penalty eq. (192) (green, dotted), all with
η = 0.01. This figure shows the initial phase of training, when gradient descent is in the stable regime (i.e. sharpness
is below 2/η). Consistent with Barrett and Dherin (2021), notice that in the stable regime, gradient flow + IGR is a
visibly better approximation to gradient descent than vanilla gradient flow. In particular, observe that the distance
to the gradient descent trajectory is smaller for gradient flow + IGR than for vanilla gradient flow (bottom right).
Similarly, note that the network outputs on two examples, the train loss, and the squared gradient norm agree better
under gradient flow + IGR than under vanilla gradient flow. That said, notice that even the vanilla gradient flow
is a good approximation to gradient descent in this regime. Please refer to Figure 26 for the continuation of this
experiment into the EOS regime. Details: a CNN is trained on a subset of CIFAR-10 using MSE loss.

86

gradient descent
... smoothed
gradient flow
+ IGR penalty
central flow

gradient norm2

train loss
0.4
0.2
0

1000

2000
step

3000

4000

2000

30

1500

20

1000

10

500

0

0

network output #1

1000

2000
step

3000

4000

0

0.8

0.6

0.3

0.6

0.4

0.2

0.4

0.2

0.1

0.2

2000
step

3000

4000

0

1000

2000
step

3000

1000

2000
step

3000

4000

distance to gradient descent

0.4

1000

0

network output #2

0.8

0

max hessian eigenvalue (sharpness)

4000

0.0

0

1000

2000
step

3000

4000

Figure 26: In the EOS regime, the central flow accurately approximates the trajectory of gradient descent,
whereas neither the original nor the IGR-penalized gradient flow does so. We continue the experiment from
Figure 25 for more iterations, into the EOS regime. Observe that neither the original gradient flow (orange, dashed)
nor the IGR-penalized gradient flow (green, dotted) reasonably approximates the trajectory of gradient descent (blue)
in this regime, whereas the central flow (black, dashed) does so. In particular, notice that the distance from gradient
descent to the central flow stays small, whereas the distance to both the original and IGR-penalized gradient flows
grows large over time (bottom right). Further, the central flow accurately predicts the time-averaged network outputs
on two examples, as well as the train loss and squared gradient norm.
A subtle confounder Some works (e.g. Geiping et al. (2022)) have observed that adding an explicit squared
gradient norm penalty can help full-batch training recover the superior generalization performance of minibatch
training. This would seem to support the argument of Barrett and Dherin (2021) that the implicit regularization of
discrete gradient descent can be captured by a flow with a squared gradient norm penalty. Yet, we believe that these
results could be instead due to a subtle confounder: adding a squared gradient norm penalty changes the oscillatory
EOS dynamics, and in particular, enhances the implicit curvature regularization.
Consider running gradient descent with step size η, while adding an implicit gradient regularizer corresponding to
some step size τ . The update rule is:


wt+1 = wt − η ∇L(wt ) + τ2 H(wt ) ∇L(wt ) .
(193)
On the one-dimensional quadratic function L(w) = 12 Sw2 , the iterates would evolve according to:


wt+1 = wt − η Swt + τ2 S 2 wt
= [1 − ηS − 12 ητ S 2 ]wt .

(194)

Whereas vanilla gradient descent is unstable if S > 2/η, this iteration is unstable if ηS + 12 ητ S 2 > 2 ⇐⇒ S >
r

4τ
1+ η −1

, which is < η2 . That is, it becomes unstable at lower values of the sharpness S. Accordingly, in line with
the general EOS pattern, we find that on neural network objectives, while gradient descent implicitly constrains the
sharpness to 2/η, the update rule eq. (193) implicitly constrains the sharpness to this strictly smaller value (Figure 27).
In other words, adding an explicit gradient norm penalty also results in stronger implicit curvature regularization. This
acts as a subtle experimental confounder, which could explain the reports in the literature that explicitly penalizing
the gradient norm substantially boosts generalization performance.
τ

Thus, we hypothesize that if the the IGR-penalized gradient flow eq. (191) were properly discretized, it would not
yield improved generalization (as it would not substantially affect the trajectory). Yet, if an IGR-penalized objective
87

train loss

0.5
GD with = 0.01 on L(w)
GD with = 0.01 on
L(w) + 0.04
L(w) 2
2

top Hessian eigenvalue (sharpness)
200

0.4

150

0.3

100

0.2

2/

50

0.1
0

500 1000 1500 2000 2500 3000 3500 4000
step

0

1+4 /

0

1

500 1000 1500 2000 2500 3000 3500 4000
step

Figure 27: A dangerous confounder: explicit gradient regularization induces stronger implicit curvature
regularization. We train a CNN on a subset of CIFAR-10 using MSE loss. In blue, we run gradient descent with
step size η = 0.01 on the original objective L(w). In orange, we run gradient descent with step size η = 0.01 on the
implicitly regularized objective L(w) + τ4 ∥L(w)∥2 , with τ = 0.04. This mimics an attempt to capture the η = 0.04
dynamics with an explicit gradient regularizer. Observe that
√ the explicit gradient regularizer implicitly affects the
1+4τ /η−1

curvature dynamics, causing the sharpness to saturate at
≈ 78.08 instead of at 2/η = 200. We believe
τ
that similar effects may be responsible for reports in the literature (e.g. Geiping et al. (2022)) that explicit gradient
norm regularization recovers the beneficial effects of large learning rates and small batch sizes.
is optimized using a standard optimization algorithm, this could induce stronger implicit curvature regularization
which substantially affects the trajectory and the generalization performance.
Momentum Finally, we note that it is plausible that the IGR effect is negligible for vanilla gradient descent but
relevant when momentum is used, as momentum amplifies the strength of the IGR effect (Ghosh et al., 2023).
C.1.1

Implementation details

To discretize the IGR-penalized gradient flow eq. (192), we used a forward Euler scheme:
h
i
w(t+dt) = w(t) − η ϵ ∇L(w(t) ) + η2 H(w(t) )∇L(w(t) ) .

(195)

We dynamically adapt the discretization step size ϵ based on the current sharpness (which we are already measuring).
On a quadratic function L(w) = 12 Sw2 with sharpness w, the Euler method eq. (195) is convergent so long as:
ϵ≤

2
ηS + 12 η 2 S 2

.

To be on the safe side, and to try to avoid any implicit effects, we use a discretization step size of one-quarter that
threshold. In particular, at every integer time t, we compute the sharpness S(w), and set:
m = ⌈2ηS + η 2 S 2 ⌉

and

and we take m Euler steps eq. (195) with discretization step size ϵ.

88

ϵ = 1/m,

C.2

Failure mode: higher-order terms

Our theory models the objective using a local cubic Taylor approximation. Sometimes, however, a cubic Taylor
expansion is inadequate to capture the dynamics within the critical subspace, and this gives rise to a failure mode for
the central flow, which was previously discussed in Damian et al. (2023, Appendix F).
This failure mode is illustrated in Figure 28, which depicts a stretch of gradient descent where one eigenvalue is at
the edge of stability and where the cyclic EOS dynamics have collapsed to a period-2 fixed point. (This makes for a
simpler setting than the full cyclic dynamics, which helps us better illustrate the issue.) Observe that the sharpness
measured at the (second-order) midpoints between the gradient descent iterates is noticeably lower than 2/η, whereas
the sharpness along the central flow is strictly equal to 2/η. Further, observe that the actual squared displacement
between gradient descent and the central flow is noticeably different from the central flow’s prediction for this value,
σ 2 (t). This means that the central flow is applying the wrong strength of implicit sharpness regularization, which will
cause error to accumulate over the long run.
These issues arise because the loss function along the top Hessian eigenvector is not well-modeled by its cubic Taylor
expansion. In Figure 29, at various points during this stretch of training, we consider the line segment in between
two successive iterates {αwt + (1 − α)wt+1 : 0 ≤ α ≤ 1}, and we plot the curvature quantity u⊤ H(w)u along this
line, where u is the top Hessian eigenvector measured at the midpoint between the two iterates w. We also plot the
first-order Taylor approximation of this curvature quantity, S(w) + ∇S(w)⊤ (w − w), which arises from the local
cubic Taylor approximation, as well as the second-order Taylor approximation of this curvature quantity, which arises
from the local quartic Taylor approximation. Observe that the curvature along this line segment is not well-modeled
by its first-order Taylor approximation. This is an indicator that the cubic Taylor approximation which we employ in
our analysis is failing to hold within the local region that is being traversed via the oscillations.
By contrast, Figures 30 and 31 depicts a different deep learning problem where the central flow approximation is
more accurate, and where the local curvature is well-described by the cubic Taylor expansion.
Please see Damian et al. (2023, Appendix F) for an extended discussion of this issue, in the special case of one unstable
eigenvalue. In this setting, the loss function can either be super-quadratic along the top Hessian eigenvector, in which
case the real curvature lies above than its first-order Taylor approximation, and the curvature at the midpoint is less
than 2/η; or it can be sub-quadratic, in which case the real curvature lies below its first-order Taylor approximation,
and the curvature at the midpoint is greater than 2/η. When multiple eigenvalues are unstable, we expect that the loss
function could conceivably be subquadratic along some directions and superquadratic along others.
In the special case of one unstable eigenvalue, Damian et al. (2023) derived a correction to their constrained trajectory
(analogous to our central flow) which they empirically showed to match the real gradient descent trajectory even in the
variance of oscillations along
top Hessian eigenvector

top Hessian eigenvalue

215

GD (2nd order midpoints)
central flow

210
205
200
195
190
185

1800

2000
2200
step / time

2400

0.0300
0.0275
0.0250
0.0225
0.0200
0.0175
0.0150

2600

displacement2 along
top Hessian eigenvector
central flow 2(t)
1800

2000
2200
step / time

2400

2600

Figure 28: Illustrating this failure mode for the central flow approximation. This figure shows a segment of
training which suffers from the failure mode discussed here. Observe that the sharpness along the gradient descent
trajectory (measured at the second-order midpoints) is different from the sharpness of the central flow, which is locked
at 2/η. Further, the central flow poorly predicts the variance of the oscillations along the top Hessian eigenvector.
Details: a CNN is trained on a two-class subset of CIFAR-10 with logistic loss.

89

sub/super-quadratic setting. Interestingly, with this correction, the implicit regularizer still takes the form σ 2 ∇S(w)
for some σ 2 ; however, σ 2 cannot be determined solely from the local cubic Taylor approximation, and instead requires
knowledge of the exact loss function along the top Hessian eigenvector direction. It would be interesting to re-derive
this correction under the central flow framework, and to extend it to the setting of multiple unstable eigenvalues.
step 1900

step 2200

step 2500

240

240

220

220

220

200
180

curvature

240
curvature

curvature

260

200
180

160
0.2

0.4

0.6

0.8

1.0

200
180

160
0.0

real curvature
first-order
approximation
second-order
approximation

0.0

0.2

0.4

0.6

0.8

160

1.0

0.0

0.2

0.4

0.6

0.8

1.0

Figure 29: When the central flow fails, the local cubic structure poorly predicts local curvature. During the
stretch of training depicted in Figure 28, at three different steps, we plot the curvature metric u⊤ H(w)u measured
along the line segment between the current iterate and the next one: αwt + (1 − α)wt+1 , 0 ≤ α ≤ 1, where u
denotes the top Hessian eigenvector measured at the midpoint between the two iterates. Observe that this curvature
metric (blue) is poorly predicted by its linear approximation around the midpoint (orange), which is based on the
local cubic structure of the loss.
top Hessian eigenvalue

215

GD (2nd order midpoints)
central flow

210
205

displacement2 along
top Hessian eigenvector
central flow 2(t)

0.0190
0.0185

200

0.0180

195

0.0175

190
185

variance of oscillations along
top Hessian eigenvector

0.0195

0.0170
3200

3300

3400
3500
step / time

3600

3200

3300

3400
3500
step / time

3600

Figure 30: A success case for the central flow. This figure shows a segment of training which does not suffer from
this failure mode. Observe that the sharpness along the gradient descent trajectory (measured at the second-order
midpoints) is quite close to the sharpness along the central flow, which is locked at 2/η. Further, observe that the
central flow accurately predicts the variance of the oscillations along the top Hessian eigenvector. Details: a CNN is
trained on a two-class subset of CIFAR-10 with MSE loss.
step 3400

step 3500

204

204

202

202

202

200
198
196

curvature

204
curvature

curvature

step 3300

200
198
196

0.0

0.2

0.4

0.6

0.8

1.0

real curvature
first-order
approximation
second-order
approximation

200
198
196

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

Figure 31: When the central flow succeeds, the local cubic structure accurately predicts local curvature. This
figure shows the same curvature metric as Figure 29, but in the “success case” setting of Figure 30. Observe that here,
the local curvature is well-predicted by its local linearization.

90

D

Supplementary Figures
CNN

0.5

ResNet
= 0.01
= 0.0133
= 0.02
central flows

0.4
0.3
0.2
0.1
0.0

0

1000

2000
step

3000

4000

0

1000

LSTM
0.3

2000
step

3000

0.35
0.30
0.25
0.20
0.15
0.10

4000

0

1000 2000 3000 4000 5000 6000
step

Mamba
= 0.02
= 0.0267
= 0.04
central flows

0.4
0.3

0.25

0.2

0.20

0.1

0.1

0.15

1000 2000 3000 4000 5000 6000
step

0.0

0

1000 2000 3000 4000 5000 6000
step

= 0.005
= 0.01
= 0.02
central flows

0.30

0.2

0

= 0.01
= 0.0133
= 0.02
central flows

Transformer
= 0.0267
= 0.04
= 0.08
central flows

0.4

ViT
= 0.01
= 0.0133
= 0.02
central flows

0.35
0.30
0.25
0.20
0.15
0.10

0.10

0

1000 2000 3000 4000 5000 6000
step

Figure 32(a): Gradient descent: central flow predictions for loss curves (MSE). We compare the actual train loss
curve (faint colors) and its Gaussian-smoothed version (thick colors) to the central flow’s prediction eq. (23) for the
time-averaged loss curve (black dashed lines). Each subpanel is a different architecture, and each color is a different
learning rate. These plots use the mean squared error (MSE) loss; see Figure 32(b) for cross-entropy.

CNN

1.50

ResNet
= 0.005
= 0.00667
= 0.01
central flows

1.25
1.00
0.75

= 0.01
= 0.0133
= 0.02
central flows

1.25
1.00
0.75

1.00
0.75

0.50

0.50

0.25

0.25

0.25

0

1000 2000 3000 4000 5000 6000
step

0.00

0

1000 2000 3000 4000 5000 6000
step

LSTM

1.50
1.00
0.75

0.00

2.0
1.5

0.8
0.6
0.4

0.25

0.5

0.2

0.00

0.0

0

1000

2000
step

2000
step

3000

4000

3000

4000

= 0.01
= 0.0133
= 0.02
central flows

1.0

0.50

1000 2000 3000 4000 5000 6000
step

1000

Mamba
= 0.01
= 0.0133
= 0.02
central flows

2.5

1.0

0

0

Transformer
= 0.0133
= 0.02
= 0.04
central flows

1.25

= 0.01
= 0.0133
= 0.02
central flows

1.25

0.50
0.00

ViT

1.50

0.0

0

1000 2000 3000 4000 5000 6000
step

Figure 32(b): Gradient descent: central flow predictions for loss curves (CE). Similar to Figure 32(a), but for
cross-entropy loss. Here, the prediction is sometimes a bit off, especially at the end, and especially with larger
learning rates; as discussed in Section 6, the central flow tends to be somewhat less accurate with cross-entropy loss.

91

CNN

0.5

= 0.005
= 0.01
= 0.02
central flows

0.4
0.3
0.2
0.1
0.0

0

2000

4000
step

6000

ResNet

0.40
0.35
0.30
0.25
0.20
0.15
0.10

8000

0.2
0.1

0.15
0

500 1000 1500 2000 2500 3000
step

4000
step

0

1000 2000 3000 4000 5000 6000
step

Mamba
= 0.005
= 0.01
= 0.02
central flows

0.5
0.4
0.3
0.1

2000

0.30
0.20

0.2

0

0.35

Transformer
= 0.01
= 0.02
= 0.04
central flows

0.3

= 0.01
= 0.02
= 0.03
central flows

0.40

0.25

LSTM
0.4

ViT
= 0.007
= 0.01
= 0.02
central flows

6000

8000

0.0

0

2000

4000
step

6000

8000

0.40
0.35
0.30
0.25
0.20
0.15
0.10

= 0.005
= 0.01
= 0.02
central flows

0

1000

2000 3000
step

4000

5000

Figure 33(a): Scalar RMSProp: central flow predictions for loss curves (MSE). We compare the actual train loss
curve (faint colors) and its Gaussian-smoothed version (thick colors) to the central flow’s prediction eq. (116) for the
time-averaged loss curve (black dashed lines). Each subpanel is a different architecture, and each color is a different
learning rate. These plots use the mean squared error (MSE) loss; see Figure 33(b) for cross-entropy.

1.50
1.25
1.00
0.75

CNN

ResNet

= 0.003
= 0.006
= 0.01
central flows

= 0.01
= 0.02
= 0.03
central flows

1.25
1.00
0.75

1.00
0.75

0.50

0.50

0.25

0.25

0.25

0

1000

2000 3000
step

4000

5000

0.00

0

LSTM

1.50

Transformer
2.5

= 0.01
= 0.02
= 0.03
central flows

1.25
1.00
0.75

500 1000 1500 2000 2500 3000
step
= 0.01
= 0.02
= 0.03
central flows

2.0
1.5

0.00

0.5

0.4

6000

8000

0.0

0

1000 2000 3000 4000 5000 6000
step

3000

4000

Mamba
= 0.007
= 0.01
= 0.02
central flows

0.8

0.25
4000
step

2000
step

1.0
0.6

2000

1000

1.2

1.0

0

0

1.4

0.50
0.00

= 0.01
= 0.02
= 0.03
central flows

1.25

0.50

0.00

ViT

1.50

0

1000

2000 3000
step

4000

5000

Figure 33(b): Scalar RMSProp: central flow predictions for loss curves (CE). Similar to Figure 33(a) but for cross
entropy loss.

92

CNN

0.5

= 1e-05
= 2e-05
= 4e-05
central flows

0.4
0.3

0.3
0.2

0.1

0.1
0

500 1000 1500 2000 2500 3000
step

LSTM
= 1e-05
= 2e-05
= 4e-05
central flows

0.4
0.3
0.2
0.1
0

1000

2000
step

3000

4000

= 2e-05
= 4e-05
= 0.0001
central flows

0.4

0.2

0.0

ResNet

0.5

0.0

0.6
0.5
0.4
0.3
0.2
0.1
0.0

ViT

0.8

= 1e-05
= 2e-05
= 3e-05
central flows

0.6
0.4
0.2

0

500 1000 1500 2000 2500 3000
step

Transformer
= 2e-05
= 4e-05
= 0.0001
central flows

0.0

0

1000

2000
step

3000

4000

Mamba

0.5

= 1e-05
= 2e-05
= 4e-05
central flows

0.4
0.3
0.2
0.1

0

500

1000 1500
step

2000

2500

0

1000

2000 3000
step

4000

5000

Figure 34(a): RMSProp: central flow predictions for loss curves (MSE). We compare the actual train loss curve
(faint colors) and its Gaussian-smoothed version (thick colors) to the central flow’s prediction eq. (124) for the
time-averaged loss curve (black dashed lines). Each subpanel is a different architecture, and each color is a different
learning rate. These plots use the mean squared error (MSE) loss; see Figure 33(b) for cross-entropy.

CNN

ResNet
= 7e-06
= 1e-05
= 2e-05
central flows

1.25
1.00
0.75

= 1e-05
= 2e-05
= 4e-05
central flows

1.25
1.00
0.75

1.00
0.75

0.50

0.50

0.25

0.25

0.25

0

500 1000 1500 2000 2500 3000
step

LSTM
= 1e-05
= 2e-05
= 6e-05
central flows

1.25
1.00
0.75
0.50

0

0

1000

2000 3000
step

4000

5000

1000

2000
step

3000

4000

Transformer

2.5
2.0
1.5

= 1e-05
= 2e-05
= 4e-05
central flows

1.0

0.25
0.00

0.00

0.5

0

500

1000 1500
step

2000

2500

= 5e-06
= 7e-06
= 1e-05
central flows

1.25

0.50
0.00

ViT

1.50

0.00

1.4
1.2
1.0
0.8
0.6
0.4
0.2

0

1000

2000
step

3000

4000

Mamba
= 7e-06
= 1e-05
= 2e-05
central flows

0

1000

2000 3000
step

4000

5000

Figure 34(b): RMSProp: central flow predictions for loss curves (CE). Similar to Figure 34(a) but for cross-entropy
loss.

93

sharpness
(max Hessian eigenvalue)

train loss

CNN

0.4

600

0.3

0.0

0

2000

4000
step

6000

8000

0

10.0
5.0

2000

4000
step

6000

8000

central flows

2.5

0.2
0

2 = 0.9
2 = 0.99
2 = 0.999

7.5

0.4

200

0.1

12.5

0.6

400

0.2

network output on
arbitrary test example

0.8

0

2000

4000
step

6000

8000

0.0

0

2000

4000
step

6000

8000

Figure 36: Validating the Scalar RMSProp central flow across β2 values. We run both Scalar RMSProp and
its central flow at multiple values of the β2 hyperparameter (colors). Observe that the central flows (black) wellapproximate the trajectory of Scalar RMSProp. Details: a CNN is trained on a subset of CIFAR-10 with MSE loss,
η = 0.01, and bias correction.
train loss

CNN / MSE

0.5

Transformer / MSE

400

0.4

300

0.3

0

1000

step

2000

3000

0

0

1000

step

2000

3000

250

500 1000 1500 2000 2500
step

0

1000

step

2000

3000

0.0

100

0.2

2

50

0.0
500 1000 1500 2000 2500
step

0

0

500 1000 1500 2000 2500
step

1000

step
j

4

0

central flows

network output on
arbitrary test example
0.4

0.2

2 = 0.9
2 = 0.99
2 = 0.999

0.5

6

150

0

1.5

0.6

200

0.0

2.0

sharpness
(max Hessian eigenvalue)

0.4

j

j

0.2

train loss

0.6

0.6

1.0

100

0.1

network output on
arbitrary test example

0.4

200

0.2
0.0

sharpness
(max Hessian eigenvalue)

0

2000

3000

j

2 = 0.9
2 = 0.99
2 = 0.999

central flows
0

500 1000 1500 2000 2500
step

Figure 35: Validating the RMSProp central flow across β2 values. We run both RMSProp and its central flow
at multiple values of the β2 hyperparameter (colors). Observe that the central flows (black) well-approximate the
trajectories of RMSProp. Details: the top row is a CNN trained on a subset of CIFAR-10 with MSE loss, η = 2e-5,
ϵ = 1e-8, and bias correction. The bottom row is a Transformer trained on a synthetic sequence prediction task with
MSE loss, η = 4e-5, ϵ = 1e-8, and bias correction.

94

CNN / MSE
Transformer / MSE

sharpness
(max Hessian eigenvalue)

train loss
400

0.8

300

0.6

200

0.4

0.1

100

0.2

0.0

0

0.5
0.4
0.3
0.2

0

1000

step

2000

3000

0

step

2000

3000

100
0.2
500

step

1000

1500

step

2000

3000

0.6

1.5

0.4

1.0

500

step

1000

0

1000

1500

step

2000

3000

j

= 1e-05
= 0.0001
= 0.001
central flows

0.5

0.0
0

j

= 1e-06
= 1e-05
= 0.0001
central flows

j

0.2

50
0

1000

j

2.5
2.0
1.5
1.0
0.5
0.0

network output on
arbitrary test example

150

0.4

0.0

0

sharpness
(max Hessian eigenvalue)

train loss

0.6

1000

network output on
arbitrary test example

0

500

step

1000

1500

0.0

0

500

step

1000

1500

Figure 37: Validating the RMSProp central flow across ϵ values. We run both RMSProp and its central flow
at multiple values of the ϵ hyperparameter (colors). Observe that the central flows (black) well-approximate the
trajectory of RMSProp. Details: the top row is a CNN trained on a subset of CIFAR-10 with MSE loss, η = 2e-5,
ϵ = 1e-8, and bias correction. The bottom row is a Transformer trained on a synthetic sequence prediction task with
MSE loss, η = 4e-5, ϵ =1e-8, and bias correction.

train loss

2.0

500

300

400

1.5

200

0.5

100
3200

3400 3600
step / time

3800

0
3000

3200

3400 3600
step / time

3800

distance between gradient
descent and central flow

top hessian eigenvalue
0.6

250

300

1.0

0.0
3000

squared gradient norm

0.5

200

0.4

150

0.3

100
3000

3200

3400 3600
step / time

3800

0.2
3000

gradient descent
central flow

3200

3400 3600
step / time

3800

Figure 39: Large spikes degrade the accuracy of the central flow approximation. Every few hundred iterations,
there is a large spike (visible in e.g. the loss and the gradient norm), which causes the distance between gradient
descent and the central flow to jump. For reasons we do not understand, such large spikes are relatively common
during full-batch training with cross-entropy loss, especially near the end of training. Details: a CNN is trained on a
subset of CIFAR-10 with cross-entropy loss, using gradient descent with η = 0.01.

95

1.0

(b) weight-space distance
between w(t) and wt

(a) -GeLU activation function
= 1.0
= 5.0
= 10.0
= 20.0

0.8

0.35

= 1.0
= 5.0
= 10.0
= 20.0

0.30
0.25

0.6

0.20

0.4

0.15

0.2

0.10

0.0

0.05
0.2

1.00

0.75

0.50

0.25 0.00

0.25

0.50

0.75

0.00

1.00

0

500

1000

1500

2000

2500

3000

3500

4000

0.4
0.2
0.0
0.2
0

1000 2000 3000 4000
step

0

1000 2000 3000 4000

0.4
0.2
0.0
0

1000 2000 3000 4000
step

= 10.0
train loss

0.5
0.4
0.3
0.2
0.1
0

1000 2000 3000 4000

0.3
0.2
0.1
0.0
0

1000 2000 3000 4000
step

network output on test example

1000 2000 3000 4000

train loss

train loss
0

0.6

= 5.0

0.5
0.4
0.3
0.2
0.1

network output on test example

= 1.0

0.5
0.4
0.3
0.2
0.1

network output on test example

network output on test example

train loss

(c) loss and network output on test example
= 20.0

0.5
0.4
0.3
0.2
0.1
0

1000 2000 3000 4000

0

1000 2000 3000 4000
step

0.3
0.2
0.1
0.0

Figure 38: Accuracy of central flow degrades as activation function becomes less smooth. We consider networks
with the β-GeLU activation function from Dauphin et al. (2024), defined as x 7→ xΦ(βx) where Φ is the standard
Gaussian CDF. This activation interpolates between (smooth) GeLU when β = 1 and (non-smooth) ReLU when
β = ∞. Subfigure (a) plots this activation function with varying β. Subfigure (b) shows that when β is larger (i.e.
when the activation is less smooth), the approximation error between the central flow w(t) and the optimizer trajectory
wt grows faster. Subfigure (c) plots the loss curve, and the network’s output on a test example, for both the optimizer
trajectory and the central flow. Fortunately, even when β = 20, at which point β-GeLU is a very close approximation
to ReLU, the central flow accurately predicts the overall training loss curve.

96

train loss L(w(t))

sharpness S(w(t))

0.25
2 = 0.99

0.20

CNN

effective step size /
0.05

600

0.04

400

0.15
0.10

0.02

200

0.01

0.05
0

500

1000 1500 2000 2500
step

train loss L(w(t))

0.24

2 = 0.99

ResNet

0.20
0.18
0.16
0

200

400 600
step

800

1000

ViT

2 = 0.99

0.24

500

train loss L(w(t))

0.16

2 = 0.99

0.12
0.10
0.08
0.06

0

500

1000
step

1500

2 = 0.99

400 600
step

800

1000

400

0.015

0.22
200

400 600
step

800

1000 1500 2000 2500
step

Scalar RMSProp, = 0.02
Scalar RMSProp, = 0.04
central flows
ablation w/o
curvature reg

0.02
0

500

1000
step

1500

2000

0.01

0

800

1000

500

1000
step

1500

2000

effective step size /
0.045
0.040

Scalar RMSProp, = 0.01
Scalar RMSProp, = 0.02
central flows
ablation w/o
curvature reg

0.035
0.030
0.025
0.020
0

200

400 600
step

800

1000

0

200

400 600
step

800

0

1000

200

400 600
step

800

1000

effective step size /

0.030
0.025
0.020
0.015
0.010
0.005

200
400 600
step

1000 1500 2000 2500
step

0.03

400

0.18

500

0.04

600

200

0

effective step size /

800

0

Scalar RMSProp, = 0.03
Scalar RMSProp, = 0.05
central flows
ablation w/o
curvature reg

sharpness S(w(t))

sharpness S(w(t))

0.19

1000

0.005
500

1000

0.20

800

0.010

train loss L(w(t))
0.21

400 600
step

0.020

0

1000

200

effective step size /

sharpness S(w(t))

0.24

0

0

0.030

100
90
80
70
60
50

0.26

0.20

0.010

0.025

train loss L(w(t))
0.28

Transformer

200

600

2000

Scalar RMSProp, = 0.01
Scalar RMSProp, = 0.02
central flows
ablation w/o
curvature reg

0.015

200
175
150
125
100
75
50

0.14

1000 1500 2000 2500
step

0.020

0

1000 1500 2000 2500
step

500

0.025

200
0

0

effective step size /

0.22
0.20

2 = 0.99

1000 1500 2000 2500
step

sharpness S(w(t))

0.26

Mamba

500

sharpness S(w(t))

train loss L(w(t))

0.28

LSTM

0

200
180
160
140
120
100
80

0.22

Scalar RMSProp, = 0.01
Scalar RMSProp, = 0.02
central flows
ablation w/o
curvature reg

0.03

Scalar RMSProp, = 0.01
Scalar RMSProp, = 0.02
central flows
ablation w/o
curvature reg
0

200

400 600
step

800

1000

Figure 40: Implicit curvature reduction accelerates optimization for Scalar RMSProp. Starting from the
same initialization, we run the Scalar RMSProp central flow at various learning rates, as well as an ablated flow
dw
2
dt = − S(w) ∇L(w) with curvature regularization removed. These three flows all use the same step size strategy but
differ in the strength of implicit curvature regularization. Initially, the flows with higher curvature regularization often
optimize slower; however, over the longer run, they are able to take larger steps and optimize faster. Each row depicts
a different deep learning setting. All experiments use MSE loss.64

64
For discrete Scalar RMSProp, we show the train loss not at the iterates themselves but at the second-order midpoints between iterates
ŵt := 14 [2wt + wt−1 + wt+1 ], which removes most of the oscillations along the top eigenvectors. This makes the train loss directly comparable
to the flow losses.

97

CNN

= 0.0001, 2 = 0.99

train loss L(w)
0.075
0.025

ResNet

= 0.0002, 2 = 0.99

ViT

= 3e-05, 2 = 0.99

LSTM

= 0.0003, 2 = 0.95

400 600
step

800

1000

0

400 600
step

800

1000

0

200 400 600 800 1000 1200
step

1000
i/

i

)

RMSProp
central flow
ablation w/o
curvature reg

0

0.0

0

200 400 600 800 1000 1200
step

i

i/

)

RMSProp
central flow
ablation w/o
curvature reg

0.10
0.05

500
0

train loss L(w)

0.00

500 1000 1500 2000 2500
step

0

500 1000 1500 2000 2500
step

harmonic mean of
effective step sizes 1/

sharpness S(w)

0.15

(

(

1000

500 1000 1500 2000 2500
step

800

0.15

1500

0

400 600
step

harmonic mean of
effective step sizes 1/

sharpness S(w)

0.1

200

0.5

train loss L(w)

0.2

0

1.0

200 400 600 800 1000 1200
step

0.3

0.0

1.5

400

0

)

RMSProp
central flow
ablation w/o
curvature reg

harmonic mean of
effective step sizes 1/

200

(

1000

0.4

500

0.2

i

i/

)

RMSProp
central flow
ablation w/o
curvature reg

0.10
0.05
0

500

1000
step

0

1500

0

train loss L(w)

0.3

500

1000
step

0.0

1500

200

step

300

400

500

0

train loss L(w)

100

200

step

300

400

500

0.10

0

500

1000
step

1500

2000

)

0.0

0

100

200

step

300

400

(

i

500
i/

)

RMSProp
central flow
ablation w/o
curvature reg

0.050

2000
0

i/

0.075
0.025

0.05
0.00

i

RMSProp
central flow
ablation w/o
curvature reg

0.100

4000

0.15

(

harmonic mean of
effective step sizes 1/

sharpness S(w)

0.20

1500

0.2

50
100

1000
step

0.4

100

0

500

0.6

150

0.1

0

harmonic mean of
effective step sizes 1/

sharpness S(w)

0.2

0.0

Mamba

200

sharpness S(w)

800

0.1

i/

0.1

600

0.00

Transformer

200

0.2

0.0

= 0.0004, 2 = 0.99

0

i

0.2

50

train loss L(w)

0.0

= 4e-05, 2 = 0.99

(

100

0.050
0.000

sharpness S(w)

150

0.100

harmonic mean of
effective step sizes 1/

0

500

1000
step

1500

2000

0.000

0

500

1000
step

1500

2000

Figure 41: Implicit curvature reduction accelerates optimization for RMSProp. We compare RMSProp (blue) and
its central flow (black) to an ablated flow (red) which leaves out the implicit curvature regularization, and maintains
stability purely by the effect of oscillations on ν. Over time, RMSProp and the central flow navigate to lower-curvature
regions (center), where they take larger steps (right), and optimize faster (left) than the ablated flow. Each row is
a different DL setting. The left column plots the train loss,65 the middle column plots the sharpness, and the right
column plots the harmonic mean of the effective learning rates. These experiments all use MSE loss.
65
For discrete RMSProp, we show the train loss not at the iterates themselves but at the second-order midpoints between iterates
ŵt := 14 [2wt + wt−1 + wt+1 ], which removes most of the oscillations along the top eigenvectors. This makes the train loss directly
comparable to the flow losses.

98

CNN
= 1e-05

cos( (t), (w(t)))

1.00
0.99

0.999

0.90
0.85

0.97

0.80

0.96
500

1000

1500 2000
step

2500

500

1000

1500
step

= 4e-05

1.00
0.95
0.90
0.85
0.80
0.75

1.000

0.95

0.99975
0.99950

0.98

= 2e-05

1.00

2000

1.000
0.999

2500

500

1000

1500
step

2000

2500

cos( (t), (w(t)))

ResNet
= 2e-05

1.00
0.99
0.98
0.97
0.96
0.95

= 4e-05

1.00

0.99975
0.99950
0.99925

0.95

0.9975

0.90
0.85
0.80

1000

1500

2000
step

2500

500

1000

= 0.0001

1.00
0.95
0.90
0.85
0.80
0.75

1.0000

1500 2000
step

0.999
0.998
0.997

2500

500

1000

1500
step

2000

2500

cos( (t), (w(t)))

ViT
= 1e-05

1.00
0.98
0.96
0.94
0.92
0.90

= 2e-05

1.0

0.9995
0.9990

0.9

0.998

0.8
1000 1500 2000 2500 3000 3500
step

0.7

1000

2000
step

= 3e-05

1.000
0.975
0.950
0.925
0.900
0.875

1.000

3000

0.99975
0.99950
0.99925

1000

2000
step

3000

LSTM
= 1e-05

cos( (t), (w(t)))

1.000

0.999975
0.999950
0.999925

0.995
0.990

= 2e-05

1.00

0.9999
0.9998
0.9997

0.98
0.96

0.985
0.980
3000 4000 5000 6000 7000 8000
step

0.94

2000

3000
step

= 4e-05

1.00
0.98
0.96
0.94
0.92
0.90

1.000
0.998

4000

2000

4000
step

6000

8000

Transformer
= 2e-05

cos( (t), (w(t)))

1.0000
0.9995

0.998

0.9990

0.996

0.9985

0.994

0.9980

500

1000

1500
step

= 4e-05

1.000

2000

2500

0.992

= 0.0001

1.00

0.99995
0.99990

1.00000

0.99

0.99975

0.98
500

1000

1500
step

2000

2500

0.97

500

1000

1500
step

2000

2500

4000

5000

cos( (t), (w(t)))

Mamba
1.000
0.995
0.990
0.985
0.980
0.975

= 1e-05

= 2e-05

1.00
0.98

0.9998
0.9996

0.9995
0.9990

0.96
0.94

1000

2000

3000
step

4000

0.92

1000

2000

3000
step

= 4e-05

1.00
0.95
0.90
0.85
0.80
4000

5000

0.9995
0.9990
0.9985

0

1000

2000 3000
step

Figure 42(a): The EMA ν reaches stationarity during training (MSE). While running the RMSProp central flow,
beginning at the time when training enters EOS, we monitor the cosine similarity between the EMA ν(t) and the
stationary EMA ν̄(w(t)). This cosine similarity rises to high values (nearly 1) during training, implying that ν(t)
reaches stationarity.

99

cos( (t), (w(t)))

CNN
= 7e-06

1.00000
0.99995
0.99990
0.99985
0.99980
0.99975
1000

1500

= 1e-05

1.0000
0.9999
0.9998
0.9997
0.9996
0.9995

2000
step

2500

= 2e-05

1.000

1.00000

0.999

0.99995

0.998
0.997

500

1000

1500 2000
step

0.996

2500

500

1000 1500 2000 2500
step

ResNet
= 1e-05

cos( (t), (w(t)))

1.0000

= 2e-05

1.0000

0.9999

0.99999
0.99998

0.9995

0.9998

1500

2000

2500 3000
step

0.9985

3500

1.00000

0.999

0.99995

0.998

0.9990

0.9997

= 4e-05

1.000

0.997
1000

2000
step

0.996

3000

1000

2000
step

3000

ViT
= 5e-06

cos( (t), (w(t)))

1.00

0.990
0.985

0.98
0.97

0.9999
0.9998
0.9997

0.995

0.9998
0.9997

0.99

= 7e-06

1.000

0.980
1000 1500 2000 2500 3000 3500
step

1000 1500 2000 2500 3000 3500
step

1.000
0.995
0.990
0.985
0.980
0.975

= 1e-05
0.9998
0.9996

1000

2000
step

3000

LSTM
= 1e-05

cos( (t), (w(t)))

1.00000
0.99999

0.99990

0.99997

0.99985
3500 3750 4000 4250 4500 4750
step

= 6e-05

1.0000
0.9998
0.9996
0.9994
0.9992
0.9990

0.9999995
0.9999990
0.9999985

0.99995

0.99998
0.99996

= 2e-05

1.00000

2000 2500 3000 3500 4000 4500
step

1.00000
0.99999

1000

2000

3000
step

4000

5000

2000

2500

4000

5000

Transformer
= 1e-05

cos( (t), (w(t)))

1.00000
0.99998

0.9998

0.99994
1000

1500
step

0.9997

2000

500

1000

1500
step

= 4e-05

1.0000
0.9998
0.9996
0.9994
0.9992
0.9990

0.9999

0.99996
0.99992

= 2e-05

1.0000

2000

2500

500

1000

1500
step

Mamba
= 7e-06

cos( (t), (w(t)))

1.00000
0.99995
0.99990
0.99985
0.99980

1000

2000

3000
step

= 1e-05

1.00000
0.99995
0.99990
0.99985
0.99980
0.99975
4000

5000

1000

2000

3000
step

= 2e-05

1.0000
0.9999
0.9998
0.9997
0.9996
0.9995

0.999997
0.999996
0.999995

4000

5000

0.999992
0.999991

1000

2000

3000
step

Figure 42(b): The EMA ν reaches stationarity during training (cross-entropy). This figure is analogous to
Figure 42(a) but for cross-entropy loss.

100

CNN
= 1e-05

10 5
10 6
10 7
10 8
10 9
10 10
500

= 2e-05

= 4e-05
10 5

10 6

10 9

10 10
1000

1500 2000
step

2500

(t)i
(w(t))i

10 7

10 8

500

1000 1500 2000 2500
step

500

1000 1500 2000 2500
step

ResNet
= 2e-05

= 4e-05
10 6
10 9
10 12

1000

1500

2000
step

2500

500

1000

= 0.0001

10 2
10 4
10 6
10 8
10 10
10 12

10 3

10 4
10 6
10 8
10 10
10 12

1500 2000
step

(t)i
(w(t))i

2500

500

1000 1500 2000 2500
step

ViT
= 1e-05
entries of

= 2e-05

10 3

10 4

= 3e-05
10 4

10 5

10 6
10 8

10 8

10 9
1000 1500 2000 2500 3000 3500
step

(t)i
(w(t))i

10 6

10 7

1000

2000
step

3000

1000

2000
step

3000

LSTM
= 1e-05

10 4

= 2e-05

10 6

10 5

10 8

10 7

10 10

10 9

= 4e-05
10 4
(t)i
(w(t))i

10 6
10 8

3000 4000 5000 6000 7000 8000
step

2000

3000
step

4000

2000

4000
step

6000

8000

Transformer
= 2e-05

10 5
10 6
10 7
10 8
10 9
10 10

= 4e-05

10 6

10 7

1000

1500
step

2000

2500

(t)i
(w(t))i

10 8

10 9
500

= 0.0001

10 4

10 5

10 10
500

1000

1500
step

2000

2500

500

1000

1500
step

2000

2500

Mamba
= 1e-05

10 2
10 6
10 10
10 14
10 18

= 2e-05

10 1
10 5
10 9
10 13

= 4e-05
10 3
10 7
10 11
10 15
10 19

10 17
1000

2000

3000
step

4000

1000

2000

3000
step

4000

5000

(t)i
(w(t))i

0

1000

2000 3000
step

4000

5000

Figure 43(a): Stationary EMA is accurate at a coordinate-wise level (MSE). While running the RMSProp central
flow, starting at the time when training enters EOS, we plot the evolution of ten coordinates of the actual EMA ν(t)
(dots) and the stationary EMA ν(w(t)) (half-black dashed lines). Each color is a different coordinate, and the ten
coordinates are uniformly spaced throughout the network. We can see that, starting soon after training reaches EOS,
the stationary EMA ν(w(t)) becomes an excellent approximation to the real EMA ν(t), on a coordinatewise level.
We can also see that both the real EMA and the stationary EMA evolve significantly (in tandem) during this time.

101

CNN
= 7e-06

10 4

10 4

10 6

10 6

10 8

10 8

10 10

10 10
1000

1500

= 1e-05

2000
step

2500

= 2e-05
10 5
(t)i
(w(t))i

10 7
10 9

500

1000

1500 2000
step

2500

500

1000 1500 2000 2500
step

ResNet
= 1e-05

10 3
10 6
10 9
10 12
10 15
1500

2000

= 2e-05

10 3
10 5
10 7
10 9
10 11
10 13

2500 3000
step

3500

1000

2000
step

= 4e-05

10 2
10 4
10 6
10 8
10 10
10 12
3000

(t)i
(w(t))i

1000

2000
step

3000

ViT

entries of

= 5e-06

= 7e-06

10 2

= 1e-05
10 3

10 4

10 4

10 6

10 6

10 5

10 8

10 8

10 7

1000 1500 2000 2500 3000 3500
step

1000 1500 2000 2500 3000 3500
step

(t)i
(w(t))i

1000

2000
step

3000

LSTM
= 1e-05

10 5

= 2e-05

10 3

= 6e-05
10 3

10 7

10 5

10 9

10 7

10 7

10 11

10 9

10 9

3500 3750 4000 4250 4500 4750
step

10 5

2000 2500 3000 3500 4000 4500
step

(t)i
(w(t))i

1000

2000

3000
step

4000

5000

Transformer
= 1e-05

= 2e-05

= 4e-05

10 5
10 6
10 7
10 8
10 9

10 7
10 8
10 9
10 10
10 11
1000

1500
step

2000

10 4
10 5
10 6
10 7
10 8
10 9
500

1000

1500
step

2000

2500

(t)i
(w(t))i

500

1000

1500
step

2000

2500

Mamba
= 7e-06

= 1e-05

100

10 3
10 7
10 11
10 15
10 19

10 5
10 10
10 15
10 20
1000

2000

3000
step

4000

5000

1000

2000

3000
step

= 2e-05

101
10 3
10 7
10 11
10 15
10 19
4000

5000

(t)i
(w(t))i

1000

2000

3000
step

4000

5000

Figure 43(b): Stationary EMA is accurate at a coordinate-wise level (CE). Analogous to Figure 43(a), but for
cross-entropy loss.

102

CNN

cos( (t), (w(t)))

1.0000
0.9999
0.9998

2 = 0.9

1.000000
0.999995

1.00
0.95
0.90

0.9997

0.998

0.85

0.9996

0.80
500 1000 1500 2000 2500 3000
step

entries of

2 = 0.99

1.000

10 5
10 6
10 7
10 8
10 9
10 10

500 1000 1500 2000 2500 3000
step
10 5
10 6
10 7
10 8
10 9
10 10

500 1000 1500 2000 2500 3000
step

1.0
0.9
0.8
0.7
0.6
0.5
0.4

2 = 0.999

0.998
0.996

500 1000 1500 2000 2500 3000
step

10 5
10 6
10 7
10 8
10 9
10 10
500 1000 1500 2000 2500 3000
step

(t)i
(w(t))i

500 1000 1500 2000 2500 3000
step

Figure 44: Assessing how β2 impacts the convergence of ν to stationarity. For multiple values of β2 (columns),
we monitor the closeness between ν(t) and the stationary value ν(w(t)) over time. In particular, the top row reports
the cosine similarity between these two vectors, and the bottom row compares ten individual coordinates (colors).
As one might expect, we see that when β2 is smaller, ν(t) converges faster to ν(w(t)) and the ultimate similarity is
higher. Conversely, when β2 is at the highest value of 0.999, some of the coordinates are noticeably off (e.g. the teal,
brown, and orange coordinates.) Details: a CNN is trained on a subset of CIFAR-10 using MSE loss at η = 2e-5,
β2 ∈ {0.9, 0.99, 0.999}, ϵ =1e-8, and bias correction.

103

CNN

L(w) 2P 1

= 1e-05

= 2e-05

= 4e-05

10 3

10 3

10 3

10 4

10 4

10 4

10 5
500

1000

1500 2000
steps

10 5

2500

500 1000 1500 2000 2500
steps

stationary
preconditioner
w/o second term

10 5

500 1000 1500 2000 2500
steps

ResNet

L(w) 2P 1

= 2e-05

= 4e-05

= 0.0001

10 3

10 3

10 3

10 4

10 4

10 4

1000

1500

2000
steps

2500

500

stationary
preconditioner
w/o second term

1000 1500 2000 2500
steps

500 1000 1500 2000 2500
steps

optimization speed
L(w) 2P 1

ViT
= 1e-05

= 2e-05

10 4

= 3e-05

10 4

1000

2000
steps

3000

stationary
preconditioner
w/o second term

10 4

1000

2000
steps

3000

1000

2000
steps

3000

LSTM
= 1e-05

L(w) 2P 1

10 2

= 2e-05

10 2

10 3

10 3

10 3

10 4

10 4

10 4

10 5

3000 4000 5000 6000 7000 8000
steps

10 5

2000

= 4e-05

10 2

3000
steps

stationary
preconditioner
w/o second term

10 5

4000

2000

4000
steps

6000

8000

Transformer

L(w) 2P 1

= 2e-05

= 4e-05

= 0.0001

10 2

10 2

10 2

10 3

10 3

10 3

10 4

10 4

10 4

500

1000

1500
steps

2000

2500

500

1000 1500
steps

2000

stationary
preconditioner
w/o second term

2500

500

1000 1500
steps

2000

2500

Mamba

L(w) 2P 1

= 1e-05

= 2e-05

= 4e-05

10 2

10 2

10 2

10 3

10 3

10 3

10 4

10 4

10 4

1000

2000

3000
steps

4000

1000

2000

3000
steps

4000

5000

stationary
preconditioner
w/o second term

0

1000 2000 3000 4000 5000
steps

Figure 45(a): RMSProp stationary preconditioner is suboptimal (MSE). We compare the RMSProp stationary
preconditioner, defined as the solution to the optimization problem eq. (35), to an alternative preconditioner defined as
the solution to eq. (36), a similar optimization problem but without the second term in the objective. We assess each
preconditioner P by reporting ∥∇L(w)∥2P − 1 , the instantaneous rate of decrease in the loss under the preconditioned
gradient flow with preconditioner P . Observe that this value is higher under the alternative preconditioner (orange)
than under the RMSProp stationary preconditioner (blue), meaning that the alternative preconditioner would decrease
the loss faster. The gap between the two preconditioners tends to be smaller when η is larger, which is reasonable
because the second term in eq. (35) is proportional to η12 .
104

CNN

L(w) 2P 1

= 7e-06

= 1e-05

= 2e-05

10 2

10 2

10 2

10 3

10 3

10 3

10 4

10 4

10 4

10 5

10 5
1000

1500

2000
steps

2500

stationary
preconditioner
w/o second term

10 5

500

1000

1500 2000
steps

2500

500 1000 1500 2000 2500
steps

ResNet

L(w) 2P 1

= 1e-05

= 2e-05

= 4e-05

10 2

10 2

10 2

10 3

10 3

10 3

10 4

10 4

10 4

10 5
1500

10 5
2000

2500 3000
steps

stationary
preconditioner
w/o second term

10 5

3500

1000

2000
steps

3000

1000

2000
steps

3000

optimization speed
L(w) 2P 1

ViT
= 5e-06

10 2

= 7e-06

10 2

10 3

10 3

10 3

10 4

10 4

10 4

1000

2000

steps

3000

1000

2000
steps

= 1e-05

10 2

3000

stationary
preconditioner
w/o second term

1000

2000
steps

3000

LSTM
= 1e-05

L(w) 2P 1

10 1

= 2e-05

10 1

10 2

10 2

10 2

10 3

10 3

10 3

10 4

10 4

10 4

10 5

10 5

3500 3750 4000 4250 4500 4750
steps

= 6e-05

10 1

2000 2500 3000 3500 4000 4500
steps

10 5

stationary
preconditioner
w/o second term

1000

2000

3000
steps

4000

5000

Transformer

L(w) 2P 1

= 1e-05

= 2e-05

= 4e-05

10 1

10 1

10 1

10 2

10 2

10 2

10 3

10 3

10 3

1000

1500
steps

2000

500

1000

1500
steps

2000

2500

stationary
preconditioner
w/o second term

500

1000 1500
steps

2000

2500

Mamba

L(w) 2P 1

= 7e-06

= 1e-05

= 2e-05

10 1

10 1

10 1

10 2

10 2

10 2

10 3

10 3

10 3

10 4

10 4

1000

2000

3000
steps

4000

5000

1000

2000

3000
steps

4000

5000

10 4

stationary
preconditioner
w/o second term

1000

2000 3000
steps

4000

5000

Figure 45(b): RMSProp stationary preconditioner is suboptimal (cross-entropy). This figure is analogous to
Figure 45(a), but for cross-entropy loss.

105

CNN
rate of loss
decrease -dL/dt

= 1e-05

= 2e-05

= 4e-05

10 4

10 4

10 4

10 5

10 5

10 5

500

1000

1500 2000
steps

2500

real
estimate w/
curvature reg
estimate w/o
curvature reg

500 1000 1500 2000 2500
steps

500 1000 1500 2000 2500
steps

ResNet
rate of loss
decrease -dL/dt

= 2e-05

= 4e-05

= 0.0001

10 4

10 4

10 4

10 5

10 5

10 5

1000

1500

2000
steps

2500

500

real
estimate w/
curvature reg
estimate w/o
curvature reg

1000 1500 2000 2500
steps

500 1000 1500 2000 2500
steps

ViT
rate of loss
decrease -dL/dt

= 1e-05

= 2e-05

= 3e-05

10 4

10 4

10 4

10 5

10 5

10 5

1000

2000
steps

3000

1000

2000
steps

real
estimate w/
curvature reg
estimate w/o
curvature reg

3000

1000

2000
steps

3000

LSTM
rate of loss
decrease -dL/dt

= 1e-05

= 2e-05

= 4e-05

10 4

10 4

10 4

10 5

10 5

10 5

3000 4000 5000 6000 7000 8000
steps

2000

3000
steps

real
estimate w/
curvature reg
estimate w/o
curvature reg

4000

2000

4000
steps

6000

8000

Transformer
rate of loss
decrease -dL/dt

= 2e-05

= 4e-05

= 0.0001

10 4

10 4

10 4

10 5

10 5

10 5

500

1000

1500
steps

2000

2500

500

real
estimate w/
curvature reg
estimate w/o
curvature reg

1000 1500 2000 2500
steps

500

1000 1500 2000 2500
steps

Mamba
rate of loss
decrease -dL/dt

= 1e-05

= 2e-05

10 4

10 5

= 4e-05

10 4

1000

2000

3000
steps

4000

10 5

real
estimate w/
curvature reg
estimate w/o
curvature reg

10 4

1000

2000

3000
steps

4000

5000

10 5

0

1000 2000 3000 4000 5000
steps

Figure 46(a): Stationary flow accurately predicts the instantaneous speed of optimization (MSE). The stationary
flow eq. (37), which incorporates an implicit curvature regularizer, predicts (black) the rate of loss decrease − dL
dt
(blue) more accurately than a naive estimate ∥∇L(w)∥2 −1
(in red) which uses the stationary preconditioner but
P

(w)

does not incorporate curvature regularization. Observe that the gap between the two estimates is larger when η is
larger, suggesting that, like Scalar RMSProp, the implicit regularization of RMSProp increases in strength with η.

106

CNN
rate of loss
decrease -dL/dt

= 7e-06

= 1e-05

= 2e-05

10 3

10 3

10 3

10 4

10 4

10 4

10 5
1000

1500

2000
steps

10 5
500

2500

real
estimate w/
curvature reg
estimate w/o
curvature reg

10 5
1000

1500 2000
steps

2500

500 1000 1500 2000 2500
steps

rate of loss
decrease -dL/dt

ResNet
= 1e-05

10 3

= 2e-05

10 3

10 4

10 4

10 4

10 5

10 5

10 5

1500

2000

2500 3000
steps

3500

1000

2000
steps

= 4e-05

10 3

real
estimate w/
curvature reg
estimate w/o
curvature reg

3000

1000

2000
steps

3000

rate of loss
decrease -dL/dt

ViT
= 5e-06

10 3

= 7e-06

10 3

10 4

10 4
1000

2000

steps

3000

= 1e-05

10 3

real
estimate w/
curvature reg
estimate w/o
curvature reg

10 4
1000

2000
steps

3000

1000

2000
steps

3000

LSTM
rate of loss
decrease -dL/dt

= 1e-05

= 2e-05

= 6e-05

10 3

10 3

10 3

10 4

10 4

10 4

10 5

3500

4000
steps

10 5

4500

2000

3000

4000

steps

real
estimate w/
curvature reg
estimate w/o
curvature reg

10 5

1000

2000

3000
steps

4000

5000

rate of loss
decrease -dL/dt

Transformer
= 1e-05

10 3

= 2e-05

10 3

10 3

6 × 10 4

6 × 10 4

6 × 10 4

4 × 10 4
3 × 10 4

4 × 10 4
3 × 10 4

4 × 10 4
3 × 10 4

1000

1500
2000
steps

500

1000

1500
steps

2000

2500

= 4e-05
real
estimate w/
curvature reg
estimate w/o
curvature reg
500 1000 1500 2000 2500
steps

Mamba
rate of loss
decrease -dL/dt

= 7e-06

= 1e-05

10 4

= 2e-05

10 4
1000

2000

3000
steps

4000

5000

real
estimate w/
curvature reg
estimate w/o
curvature reg

10 4
1000

2000

3000
steps

4000

5000

1000 2000 3000 4000 5000
steps

Figure 46(b): Stationary flow accurately predicts the instantaneous speed of optimization (cross-entropy). Same
as Figure 46(a), but with cross-entropy loss.

107

train loss

2.4

0.04
0.03
0.02
2000

2050

2100 2150
step
j

2.25

2200

200

2.0

190

2250

2000

2050

2100 2150
step

2200

2250

network output on
arbitrary test example

j

2.00

0.20

1.75

0.15

1.50

0.10
2000

2050

2100 2150
step

2200

top Hessian eigenvalue

2.2

1.8

0.01

1.25

top 3 effective Hessian eigenvalues

2250

2000

2050

2100 2150
step

2200

2250

180

0.20
0.15
0.10
0.05
0.00

2000

2050

2100 2150
step

2200

2250

weight-space distance to RMSProp

2000

2050

2100 2150
step

2200

RMSProp
central flow
stationary flow
fully-adaptive flow

2250

Figure 47(a): Stationary flow can be accurate over moderate timescales. Starting at a point during training when ν
has reached stationarity, we run the stationary flow eq. (37) (in green) alongside both RMSProp (in blue) and the
central flow (in black). As a baseline, we also run an ablated version of the stationary flow (in red) which adapts using
the stationary ν but does not implicitly regularize curvature. Observe that the stationary flow accurately tracks the
central flow (and, in turn, RMSProp), whereas the baseline is a worse approximation. This experiment uses a CNN
trained on a subset of CIFAR-10 using MSE loss with hyperparameters η = 4e-05, β2 = 0.99, and ϵ =1e-8.
train loss

2.75
2.50
2.25
2.00
1.75

0.08
0.06
800

850

900

step
j

1.25
1.00
0.75
0.50
0.25

950

1000

1050

top 4 effective Hessian eigenvalues
200
180
160
140
800

850

900

step

950

1000

1050

network output on
arbitrary test example

j

850

900

step

950

1000

1050

800

0.15

0.85
0.80
0.75
0.70
800

top Hessian eigenvalue

850

900

step

950

1000

1050

weight-space distance to RMSProp

RMSProp
central flow
stationary flow
fully-adaptive flow

0.10
0.05
800

850

900

step

950

1000

1050

0.00

800

850

900

step

950

1000

1050

Figure 47(b): Stationary flow can be accurate over moderate timescales. Same as Figure 47(a), but using a
Transformer trained on a synthetic sequence task using MSE loss with hyperparameters η = 1e-4, β2 = 0.95, and
ϵ =1e-8.

108

E

Bulk Experimental Data

This section contains the bulk experimental data from our central flow experiments:
• Section E.1 contains gradient descent experiments. See Figure 48 for a fully annotated example of a gradient
descent trajectory.
• Section E.2 contains Scalar RMSProp experiments. See Figure 51 for a fully annotated example of a Scalar
RMSProp trajectory.
• Section E.3 contains RMSProp experiments. See Figure 54 for a fully annotated example of a RMSProp
trajectory.

109

E.1

Gradient Descent
train loss
gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

0.35
0.30
0.25
0.20
0.15
0

1000 2000 3000 4000 5000 6000
step

gradient norm2

8

gradient descent
gradient descent (average)
central flow
central flow prediction

6
4
2
0

0

300
250
200
150
100
50
0

0.4
0.2
0.0
0.2

0

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

gradient descent (midpoints)
central flow
1000 2000 3000 4000 5000 6000
step

network outputs on test example 1
0.75
0.50
0.25
0.00
0.25
0.50

real vs. predicted oscillation covariance

0.00012
0.00010
0.00008
0.00006
0.00004
0.00002
0.00000

empirical variance
along (t) eigenvectors
(t) eigenvalues

0

1000 2000 3000 4000 5000 6000
step

test accuracy (%)

0

network outputs on test example 0
0.6

gradient descent
central flow
stable flow (top 1)

65
60
55
50
45
40

1000 2000 3000 4000 5000 6000
step
gradient descent
central flow

top Hessian eigenvalues

gradient descent
central flow

0

1000 2000 3000 4000 5000 6000
step

distance to gradient descent
1.0
0.8
0.6
0.4
0.2
0.0

0.8
0.6
0.4
0.2
0.0
0.2

central flow
stable flow

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2
gradient descent
central flow

0

1000 2000 3000 4000 5000 6000
step

Figure 48: Annotated example of a gradient descent experiment. Using gradient descent with η = 2/200, we train a ViT on
a subset of CIFAR-10. The central flow (black) accurately models the trajectory of gradient descent (blue), whereas gradient
flow (red) takes a different path. As described in Section B.1, we terminate gradient flow once the sharpness gets too high.
Top left: The loss along the central flow (solid black) decreases monotonically, whereas the loss along the gradient descent
trajectory (light blue) behaves non-monotonically once the dynamics enter EOS. While the gradient descent loss is higher than
the central flow loss, the central flow can accurately predict the time-averaged loss along the gradient descent trajectory, using
eq. (73) (dashed black); this can be seen to match the empirical time average of the gradient descent loss curve (dark blue).
Finally, the train loss along gradient flow (in red) decreases faster, because it follows a different, unregularized path.
Top center: We plot the top three Hessian eigenvalues under gradient descent (colors) and under the central flow (black). Under
GD, the top Hessian eigenvalues equilibrate around 2/η; under the central flow they are fixed exactly at 2/η. In red, we plot the
top Hessian eigenvalue under the gradient flow, which rises beyond 2/η. Note that for GD, we report the Hessian eigenvalues at
the second-order midpoints (see Section B.1), rather than at the iterates themselves, as this makes for clearer plots.
Top right: We show that the central flow’s Σ(t) accurately predicts the covariance of the oscillations. In black, we plot the
nonzero eigenvalues of Σ(t); the number is always the same as the number of Hessian eigenvalues at 2/η. In faint colors, we
plot the squared magnitude of the displacement between gradient descent and the central flow along each eigenvector of Σ(t).
In thick colors, we plot the time-averages of these displacements, i.e. the empirical variance of the oscillations along each
eigenvector of Σ(t). Observe that the eigenvalues of Σ(t) accurately predict the instantaneous variance of the oscillations along
the corresponding eigenvectors, as we expect from eq. (76).
Middle left: We plot the squared gradient norm along the gradient descent trajectory (light blue) and its empirical time-average
(dark blue). In dashed black, we plot the central flow’s prediction eq. (74) for the time-averaged squared gradient norm along the
trajectory; this prediction is quite accurate. In solid black, we plot the squared gradient norm along the central flow, which is
much smaller, indicating that that most of the gradient norm comes from the oscillations.
Middle center: We plot the test accuracy under gradient descent (blue) and the central flow (black). For gradient descent, we
report the test accuracy at second-order midpoints, as this removes much of the oscillations. Because the central flow matches
the gradient descent trajectory, the test accuracy is nearly the same across both trajectories.
Middle right: The Euclidean distance in weight space between gradient descent and the central flow (black) stays small over
time, indicating that these two trajectories stay close. By contrast, the distance between gradient descent and the gradient flow
(red) grows rapidly once the dynamics enter EOS.
Bottom row: We show the network’s final-layer predictions on three arbitrary examples. Under gradient descent (colors) these
predictions oscillate due to the oscillations in weight space. Under the central flow (black), the predictions evolve smoothly
while still following the same macroscopic path.

110

train loss
gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

0.4
0.3
0.2

200

2000
step

3000

4000

0

gradient descent
central flow
stable flow (top 1)
0

1000

gradient norm2

4

0

30

2000
step

3000

4000

gradient descent
central flow

0.50

0.25
3000

2000
step

3000

4000

central flow
stable flow

0.4
gradient descent (midpoints)
central flow
0

1000

2000
step

3000

4000

0.2
0.0

0

network outputs on test example 1
1.0

gradient descent
central flow

0.5
2000
step

1000

0.6

0.0

0.00
1000

0

distance to gradient descent

0.5

0.25

0

0

0.8

network outputs on test example 0
0.75

4000

50
40

1000

3000

60

2
0

2000
step

2

test accuracy (%)

gradient descent
gradient descent (average)
central flow
central flow prediction

6

empirical variance
along (t) eigenvectors
(t) eigenvalues

4

100
1000

1e 5

6

0.1
0

real vs. predicted oscillation covariance

top Hessian eigenvalues

300

4000

0

1000

2000
step

3000

1000

2000
step

3000

4000

network outputs on test example 2
0.8
0.6
0.4
0.2
0.0
0.2

gradient descent
central flow

4000

0

1000

2000
step

3000

4000

Figure 49.1: Gradient descent central flow for a CNN with MSE loss, η = 0.005.

train loss
gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

0.4
0.3
0.2
0.1
0.0

1000

2000
step

3000

4000

150

6

100

gradient descent
central flow
stable flow (top 1)

0

0

1000

3

3000

4000

network outputs on test example 0
gradient descent
central flow

0.75
0.50
0.25
0.00
0.25

3000

0

1000

30

1.0

4000

2000
step

3000

4000

central flow
stable flow

0.75
0.50
gradient descent (midpoints)
central flow
0

1000

2000
step

3000

4000

network outputs on test example 1
gradient descent
central flow

0.5
2000
step

0

1.00

0.0

1000

2

distance to gradient descent

0.5

0

4

1.25

40
2000
step

4000

50

1
1000

3000

60

2

0

2000
step

empirical variance
along (t) eigenvectors
(t) eigenvalues

test accuracy (%)

gradient descent
gradient descent (average)
central flow
central flow prediction

4

0

8

gradient norm2

5

1e 5

200

50
0

real vs. predicted oscillation covariance

top Hessian eigenvalues

0

1000

2000
step

3000

4000

0.25
0.00

0.8
0.6
0.4
0.2
0.0
0.2

0

1000

2000
step

3000

4000

network outputs on test example 2
gradient descent
central flow

0

1000

2000
step

3000

4000

Figure 49.2: Gradient descent central flow for a CNN with MSE loss, η = 0.006666.

111

train loss
gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

0.4
0.3
0.2
0.1
0.0

0

1000

2000
step

3000

4000

150
125
100
75
50
25
0

top Hessian eigenvalues

gradient descent
central flow
stable flow (top 1)
0

1000

gradient norm2

2
1
0

1000

2000
step

4000

3000

4000

50

1.0

30

0

1000

2000
step

3000

0.0

4000

network outputs on test example 1

0.0

0.5
2000
step

3000

4000

0

1000

2000
step

3000

3000

4000

central flow
stable flow

0

1000

2000
step

3000

4000

gradient descent
central flow

0.8
0.6
0.4
0.2
0.0
0.2

0.0

0.5

2000
step

network outputs on test example 2

gradient descent
central flow

0.5

1000

1000

0.5

gradient descent (midpoints)
central flow

1.0

gradient descent
central flow

0

0

distance to gradient descent
1.5

network outputs on test example 0
0.5

3000

60

40
0

2000
step

empirical variance
along (t) eigenvectors
(t) eigenvalues

test accuracy (%)

gradient descent
gradient descent (average)
central flow
central flow prediction

3

real vs. predicted oscillation covariance
0.00012
0.00010
0.00008
0.00006
0.00004
0.00002
0.00000

4000

0

1000

2000
step

3000

4000

Figure 49.3: Gradient descent central flow for a CNN with MSE loss, η = 0.01.

train loss

0.35
0.30
0.25
0.20
0.15
0.10

gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

1000

2000
step

3000

4000

gradient norm2
2.0
1.5
1.0

0

gradient descent
central flow
stable flow (top 1)
0

1000

2000
step

3000

4000

test accuracy (%)

2000
step

3000

4000

network outputs on test example 0
gradient descent
central flow

0.4
0.3
0.2
0.1
0

1000

2000
step

3000

4000

45

1.00
0.75
0.50
0.25
0.00
0.25

0.5
0.0

0

1000

2000
step

3000

4000

distance to gradient descent
central flow
stable flow

0.3

60

0.2

50
1000

1.0

0.4

55

0

empirical variance
along (t) eigenvectors
(t) eigenvalues

1.5

65

0.5
0.0

200

70

gradient descent
gradient descent (average)
central flow
central flow prediction

1e 5

2.0

100
0

real vs. predicted oscillation covariance

top Hessian eigenvalues

300

gradient descent (midpoints)
central flow
0

1000 2000 3000 4000 5000 6000
step

0.1
0.0

0

1000

2000
step

3000

4000

network outputs on test example 1

network outputs on test example 2

gradient descent
central flow

gradient descent
central flow

0.4
0.2
0.0

0

1000

2000
step

3000

4000

0

1000

2000
step

3000

Figure 49.4: Gradient descent central flow for a ResNet with MSE loss, η = 0.01.

112

4000

train loss
gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

0.3
0.2

2000
step

3000

gradient norm2
1.5

0

1000

2000
step

3000

4000

2000
step

3000

4000

45

0

1000 2000 3000 4000 5000 6000
step

2000
step

3000

4000

central flow
stable flow

0.2
0.0

0

network outputs on test example 1
1.0

gradient descent
central flow

1000

0.4
gradient descent (midpoints)
central flow

network outputs on test example 0
0.5
0.4
0.3
0.2
0.1

0

0.6

50
1000

0

distance to gradient descent

55

0

1

0.8

60

0.5

2

test accuracy (%)

65

1.0
0.0

gradient descent
central flow
stable flow (top 1)

70

gradient descent
gradient descent (average)
central flow
central flow prediction

2.0

0

empirical variance
along (t) eigenvectors
(t) eigenvalues

3

100

4000

1e 5

4

150

0.1
1000

5

200

50
0

real vs. predicted oscillation covariance

top Hessian eigenvalues

1000

2000
step

3000

4000

network outputs on test example 2

gradient descent
central flow

gradient descent
central flow

0.6
0.4

0.5

0.2
0.0
0

1000

2000
step

3000

4000

0.0
0

1000

2000
step

3000

4000

0

1000

2000
step

3000

4000

Figure 49.5: Gradient descent central flow for a ResNet with MSE loss, η = 0.013333.

train loss
gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

0.3
0.2

0

1000

2000
step

3000

4000

gradient norm2
1.5
1.0

gradient descent
central flow
stable flow (top 1)
0

1000

2000
step

3000

4000

2000
step

3000

4000

gradient descent
central flow

0.6

1000

2000 3000
step

4000

5000

0.2

gradient descent
central flow

0.0

0.0
0

1000

2000
step

3000

4000

0.5

1000

2000
step

3000

4000

central flow
stable flow

0.5
0.0

network outputs on test example 1

0.5

0.4

0

distance to gradient descent

gradient descent (midpoints)
central flow
0

1.0

0

1.0

50
1000

2

1.5

55

0

4

test accuracy (%)

60

network outputs on test example 0

0.2

0

empirical variance
along (t) eigenvectors
(t) eigenvalues

6

65

0.5
0.0

100

70

gradient descent
gradient descent (average)
central flow
central flow prediction

1e 5

8

50

0.1

real vs. predicted oscillation covariance

top Hessian eigenvalues

150

0

1000

2000
step

3000

4000

0

1000

2000
step

3000

network outputs on test example 2
0.8
0.6
0.4
0.2
0.0
0.2

gradient descent
central flow

0

1000

2000
step

3000

Figure 49.6: Gradient descent central flow for a ResNet with MSE loss, η = 0.02.

113

4000

4000

train loss
gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

0.35
0.30
0.25
0.20
0.15
0

1000 2000 3000 4000 5000 6000
step

gradient norm2

8

gradient descent
gradient descent (average)
central flow
central flow prediction

6
4
2
0

0

300
250
200
150
100
50
0

top Hessian eigenvalues

gradient descent
central flow
stable flow (top 1)
0

1000 2000 3000 4000 5000 6000
step

gradient descent (midpoints)
central flow
0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 0

network outputs on test example 1

gradient descent
central flow

gradient descent
central flow

0.4
0.2
0.0
0.2

0

1000 2000 3000 4000 5000 6000
step

empirical variance
along (t) eigenvectors
(t) eigenvalues

0

1000 2000 3000 4000 5000 6000
step

test accuracy (%)

65
60
55
50
45
40

1000 2000 3000 4000 5000 6000
step

0.6

real vs. predicted oscillation covariance

0.00012
0.00010
0.00008
0.00006
0.00004
0.00002
0.00000

0.75
0.50
0.25
0.00
0.25
0.50

0

distance to gradient descent
1.0
0.8
0.6
0.4
0.2
0.0

central flow
stable flow

0

0.8
0.6
0.4
0.2
0.0
0.2

1000 2000 3000 4000 5000 6000
step

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2
gradient descent
central flow

0

1000 2000 3000 4000 5000 6000
step

Figure 49.7: Gradient descent central flow for a ViT with MSE loss, η = 0.01.
train loss
0.35

gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

0.30
0.25
0.20
0

1000 2000 3000 4000 5000 6000
step

gradient norm2
gradient descent
gradient descent (average)
central flow
central flow prediction

6
4

100

gradient descent
central flow
stable flow (top 1)

0

0

1000 2000 3000 4000 5000 6000
step

60
55
gradient descent (midpoints)
central flow

45
0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 0
gradient descent
central flow

0

1000 2000 3000 4000 5000 6000
step

0
1.00
0.75
0.50
0.25
0.00
0.25

real vs. predicted oscillation covariance

0.000150
0.000125
0.000100
0.000075
0.000050
0.000025
0.000000

empirical variance
along (t) eigenvectors
(t) eigenvalues

0

1000 2000 3000 4000 5000 6000
step

test accuracy (%)

65

50

2

0.8
0.6
0.4
0.2
0.0
0.2
0.4

150

50

0.15

0

top Hessian eigenvalues
200

1000 2000 3000 4000 5000 6000
step

network outputs on test example 1
gradient descent
central flow

0

1000 2000 3000 4000 5000 6000
step

distance to gradient descent
1.0
0.8
0.6
0.4
0.2
0.0

0.8
0.6
0.4
0.2
0.0
0.2

central flow
stable flow

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2
gradient descent
central flow

0

1000 2000 3000 4000 5000 6000
step

Figure 49.8: Gradient descent central flow for a ViT with MSE loss, η = 0.013333.

114

train loss

0.8

gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

0.6
0.4
0.2
0

1000 2000 3000 4000 5000 6000
step

150
125
100
75
50
25
0

top Hessian eigenvalues

gradient descent
gradient descent (average)
central flow
central flow prediction

4
3
2
0

0

1000 2000 3000 4000 5000 6000
step

test accuracy (%)
65
60
55
gradient descent (midpoints)
central flow

45
0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 0
gradient descent
central flow

1.0
0.5
0.0
0.5
0

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.0002
gradient descent
central flow
stable flow (top 1)

50

1

real vs. predicted oscillation covariance

0.0003

gradient norm2
5

0.0004

0

1000 2000 3000 4000 5000 6000
step

0.0001
0.0000

1.2
1.0
0.8
0.6
0.4
0.2
0.0

0

distance to gradient descent
central flow
stable flow

0

network outputs on test example 1

1.00
0.75
0.50
0.25
0.00
0.25
0.50

gradient descent
central flow

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

1000 2000 3000 4000 5000 6000
step

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2
gradient descent
central flow

0.75
0.50
0.25
0.00
0.25
0.50

0

1000 2000 3000 4000 5000 6000
step

Figure 49.9: Gradient descent central flow for a ViT with MSE loss, η = 0.02.
train loss

0.4

gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

0.3
0.2

top Hessian eigenvalues
100
80
60
40

gradient descent
central flow
stable flow (top 1)

20

0.1
0

1000 2000 3000 4000 5000 6000
step

0

0

1000 2000 3000 4000 5000 6000
step

gradient norm2
gradient descent
gradient descent (average)
central flow
central flow prediction

2.0

test accuracy (%)

real vs. predicted oscillation covariance
0.00030
0.00025
0.00020
0.00015
0.00010
0.00005
0.00000

0.6

1.0

60

0.4

0.5

40

0.0

0.8
0.6
0.4
0.2
0.0
0.2

0

1000 2000 3000 4000 5000 6000
step

gradient descent (midpoints)
central flow
0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 0

network outputs on test example 1

gradient descent
central flow

gradient descent
central flow

0

1000 2000 3000 4000 5000 6000
step

1.00
0.75
0.50
0.25
0.00
0.25

0

1000 2000 3000 4000 5000 6000
step

distance to gradient descent

0.8

80

1.5

empirical variance
along (t) eigenvectors
(t) eigenvalues

central flow
stable flow

0.2
0.0

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2
gradient descent
central flow

0.6
0.4
0.2
0.0

0

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

Figure 49.10: Gradient descent central flow for a LSTM with MSE loss, η = 0.01333.

115

train loss

0.4

gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

0.3
0.2

60
40
gradient descent
central flow
stable flow (top 1)

20

0.1
0

1000 2000 3000 4000 5000 6000
step

gradient norm2

2.0

gradient descent
gradient descent (average)
central flow
central flow prediction

1.5
1.0

0

0

1000 2000 3000 4000 5000 6000
step

80

gradient descent
central flow

0

1000 2000 3000 4000 5000 6000
step

distance to gradient descent
central flow
stable flow

1.0
gradient descent (midpoints)
central flow
0

1.25
1.00
0.75
0.50
0.25
0.00
0.25

0

1.5

1000 2000 3000 4000 5000 6000
step

network outputs on test example 0
1.00
0.75
0.50
0.25
0.00
0.25

empirical variance
along (t) eigenvectors
(t) eigenvalues

2.0

40
0

real vs. predicted oscillation covariance
0.0006
0.0005
0.0004
0.0003
0.0002
0.0001
0.0000

test accuracy (%)

100

60

0.5
0.0

top Hessian eigenvalues

1000 2000 3000 4000 5000 6000
step

network outputs on test example 1
gradient descent
central flow

0.5
0.0

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2

0.8

gradient descent
central flow

0.6
0.4
0.2
0.0
0.2

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

Figure 49.11: Gradient descent central flow for a LSTM with MSE loss, η = 0.02.
train loss
gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

0.3
0.2

top Hessian eigenvalues
30

10

0.0

0

0

1000 2000 3000 4000 5000 6000
step

gradient norm2

1.25

gradient descent
gradient descent (average)
central flow
central flow prediction

1.00
0.75
0.50
0.25
0.00
0

0.00075
gradient descent
central flow
stable flow (top 1)
0

1000 2000 3000 4000 5000 6000
step

gradient descent (midpoints)
central flow
0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 1
gradient descent
central flow

1.0
0.5
0.0

1000 2000 3000 4000 5000 6000
step

0.00000

0

1000 2000 3000 4000 5000 6000
step

distance to gradient descent
central flow
stable flow

2

gradient descent
central flow

0

0.00025

3

network outputs on test example 0
1.00
0.75
0.50
0.25
0.00
0.25

0.00050

test accuracy (%)

100
90
80
70
60
50
40

1000 2000 3000 4000 5000 6000
step

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.00100

20

0.1

real vs. predicted oscillation covariance
0.00125

0

1000 2000 3000 4000 5000 6000
step

1
0

1.00
0.75
0.50
0.25
0.00
0.25

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2
gradient descent
central flow

0

1000 2000 3000 4000 5000 6000
step

Figure 49.12: Gradient descent central flow for a LSTM with MSE loss, η = 0.04.

116

train loss
gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

0.4
0.3
0.2

top Hessian eigenvalues

150

0

1000 2000 3000 4000 5000 6000
step

100

gradient norm2
gradient descent
gradient descent (average)
central flow
central flow prediction

0.8
0.6
0.4

0

0

1000 2000 3000 4000 5000 6000
step

0

gradient descent
central flow

0.1

0.1
0.2
0

0

1000 2000 3000 4000 5000 6000
step

distance to gradient descent
central flow
stable flow

0.4

80

0.3
0.2
gradient descent (midpoints)
central flow
0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 1

0.15

gradient descent
central flow

0.10
0.05

0.0

0

0.5

1000 2000 3000 4000 5000 6000
step

network outputs on test example 0

2

test accuracy (%)

100

40

0.0

empirical variance
along (t) eigenvectors
(t) eigenvalues

4
gradient descent
central flow
stable flow (top 1)

60

0.2

1e 5

6

50

0.1

real vs. predicted oscillation covariance
8

0.1
0.0

0.2

0.0

0.05

0.1
0

1000 2000 3000 4000 5000 6000
step

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2
gradient descent
central flow

0.1

0.00

1000 2000 3000 4000 5000 6000
step

0

0

1000 2000 3000 4000 5000 6000
step

Figure 49.13: Gradient descent central flow for a Transformer with MSE loss, η = 0.01.
train loss
gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

0.4
0.3
0.2

top Hessian eigenvalues

0.000125

80

0.000100

60

0.000075

40

0.1

gradient descent
central flow
stable flow (top 1)

20
0

1000 2000 3000 4000 5000 6000
step

gradient norm2
gradient descent
gradient descent (average)
central flow
central flow prediction

1.0
0.8
0.6
0.4
0.2
0.0
0

0

gradient descent
central flow

0.1

0

1000 2000 3000 4000 5000 6000
step

test accuracy (%)

100
90
80
70
60
50
40

1000 2000 3000 4000 5000 6000
step

network outputs on test example 0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 1
gradient descent
central flow

0.10

0

1000 2000 3000 4000 5000 6000
step

0.10

0.000000

0

1000 2000 3000 4000 5000 6000
step

distance to gradient descent

1.0

central flow
stable flow

0.2
0.0

0.2

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2
gradient descent
central flow

0.1
0.0

0.05

0.2

0.000025

0.4
gradient descent (midpoints)
central flow

0.00

0.1

0.000050

0.6

0
0.15

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.8

0.05

0.0

real vs. predicted oscillation covariance

100

0.1
0

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

Figure 49.14: Gradient descent central flow for a Transformer with MSE loss, η = 0.013333.

117

train loss

top Hessian eigenvalues

gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

0.4
0.3
0.2

60

0.00020

40

0.00015

0.1

20

0.0

0

0

1000 2000 3000 4000 5000 6000
step

gradient norm2
gradient descent
gradient descent (average)
central flow
central flow prediction

1.0
0.8
0.6
0.4
0.2
0.0
0

gradient descent
central flow
stable flow (top 1)
0

1000 2000 3000 4000 5000 6000
step

gradient descent (midpoints)
central flow
0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 0

network outputs on test example 1

gradient descent
central flow

gradient descent
central flow

0.0
0.1
0.2
0

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.00010
0.00005
0.00000

0

1000 2000 3000 4000 5000 6000
step

test accuracy (%)

100
90
80
70
60
50
40

1000 2000 3000 4000 5000 6000
step

0.1

real vs. predicted oscillation covariance

0.00025

0.1

distance to gradient descent
1.50
1.25
1.00
0.75
0.50
0.25
0.00

central flow
stable flow

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2
gradient descent
central flow

0.2
0.1

0.0

0.0

0.1

0.1

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

0.2

0

1000 2000 3000 4000 5000 6000
step

Figure 49.15: Gradient descent central flow for a Transformer with MSE loss, η = 0.02.

train loss
gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

0.30
0.25

0

1000 2000 3000 4000 5000 6000
step

400

gradient norm2

3

gradient descent
gradient descent (average)
central flow
central flow prediction

2

0

0

1000 2000 3000 4000 5000 6000
step

1000 2000 3000 4000 5000 6000
step
gradient descent
central flow

0.75

0.3

0

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

central flow
stable flow

0.1
0.0

network outputs on test example 1
gradient descent
central flow

0.6

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2
gradient descent
central flow

1.00
0.75
0.50
0.25

0.0

0.00

1000 2000 3000 4000 5000 6000
step

0.2
gradient descent (midpoints)
central flow

0.2

0.25

0

distance to gradient descent

64

0.4

0.50

0

66

network outputs on test example 0
1.00

2

0.4

60
0

empirical variance
along (t) eigenvectors
(t) eigenvalues

test accuracy (%)

68

58

1e 6

4
gradient descent
central flow
stable flow (top 1)

62

1

real vs. predicted oscillation covariance
8
6

200

0.20

0

top Hessian eigenvalues

600

0.00
0

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

Figure 49.16: Gradient descent central flow for a Mamba with MSE loss, η = 0.01.

118

train loss

0.275
0.250
0.225
0.200
0.175
0.150

gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

top Hessian eigenvalues

300

1000 2000 3000 4000 5000 6000
step

200

0

1.5
1.0

0

1000 2000 3000 4000 5000 6000
step

0.0

66
64
gradient descent (midpoints)
central flow

60
0

1000 2000 3000 4000 5000 6000
step

58

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 0

network outputs on test example 1

gradient descent
central flow

gradient descent
central flow

1.00
0.75

0.6
0.4

0.50

1000 2000 3000 4000 5000 6000
step

distance to gradient descent
0.5
0.4
0.3
0.2
0.1
0.0

central flow
stable flow

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2
gradient descent
central flow

1.00
0.75

0.00

0.2
0

0

0.25

0.0

0.00

0.0

0.50

0.2

0.25

0.5

test accuracy (%)
68

62

0.5

empirical variance
along (t) eigenvectors
(t) eigenvalues

1.0
gradient descent
central flow
stable flow (top 1)

gradient norm2
gradient descent
gradient descent (average)
central flow
central flow prediction

1e 5

1.5

100
0

real vs. predicted oscillation covariance
2.0

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

Figure 49.17: Gradient descent central flow for a Mamba with MSE loss, η = 0.013333.

train loss
gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

0.25
0.20

0.10

0

1000 2000 3000 4000 5000 6000
step

gradient norm2
gradient descent
gradient descent (average)
central flow
central flow prediction

1.00
0.75
0.50

0

0

0

1000 2000 3000 4000 5000 6000
step

67.5

0.75
0.50

gradient descent (midpoints)
central flow
0

1000 2000 3000 4000 5000 6000
step
gradient descent
central flow

0.50

1000 2000 3000 4000 5000 6000
step

central flow
stable flow

0.1
0.0

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2
gradient descent
central flow

1.00
0.75
0.50
0.25
0.00

0.25
0

1000 2000 3000 4000 5000 6000
step

distance to gradient descent

network outputs on test example 1
0.75

0.00

0.00

0

0.2

0.25

0.25

0

0.3

65.0

network outputs on test example 0

2

test accuracy (%)

70.0

1000 2000 3000 4000 5000 6000
step
gradient descent
central flow

empirical variance
along (t) eigenvectors
(t) eigenvalues

4
gradient descent
central flow
stable flow (top 1)

60.0

0.00

1.00

100

62.5

0.25

1e 5

6

50

0.15

real vs. predicted oscillation covariance

top Hessian eigenvalues

150

0

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

Figure 49.18: Gradient descent central flow for a Mamba with MSE loss, η = 0.02.

119

train loss
1.25
1.00
0.75
0.50
0.25
0.00

60
50
40
30
20
10
0

gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

0

1000 2000 3000 4000 5000 6000
step

gradient norm2
gradient descent
gradient descent (average)
central flow
central flow prediction

600
500
400
300
200
100
0

top Hessian eigenvalues

0.00010
gradient descent
central flow
stable flow (top 1)
0

1000 2000 3000 4000 5000 6000
step

test accuracy (%)

70

0.6

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 1
gradient descent
central flow

10
0

1000 2000 3000 4000 5000 6000
step

1000 2000 3000 4000 5000 6000
step

distance to gradient descent
central flow
stable flow

0.4
gradient descent (midpoints)
central flow

gradient descent
central flow

5

0

1.0

50

network outputs on test example 0

0

0.00000

0.8

1000 2000 3000 4000 5000 6000
step

5

0.00005

60

30

10

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.00015

40
0

real vs. predicted oscillation covariance

5

0.2
0.0

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2

10

gradient descent
central flow

5

0

0

5

5
10

10

0

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

Figure 50.1: Gradient descent central flow for a CNN with CE loss, η = 0.005.
train loss

4

gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

3
2

top Hessian eigenvalues
400

0

0

0

1000 2000 3000 4000 5000 6000
step

gradient norm2

50

gradient descent
gradient descent (average)
central flow
central flow prediction

40
30
20

gradient descent
central flow
stable flow (top 1)
0

1000 2000 3000 4000 5000 6000
step

test accuracy (%)

70
60
50
40

10
0

0.00015

200
100

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 0
gradient descent
central flow

10
0
10
20
0

1000 2000 3000 4000 5000 6000
step

30

10
5
0
5
10
15
20

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.00020

300

1

real vs. predicted oscillation covariance

0.00025

gradient descent (midpoints)
central flow
0

1000 2000 3000 4000 5000 6000
step

0.00010
0.00005
0.00000

1.50
1.25
1.00
0.75
0.50
0.25
0.00

network outputs on test example 1
gradient descent
central flow

0

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

distance to gradient descent
central flow
stable flow

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2
gradient descent
central flow

10
5
0
5
10
15
0

1000 2000 3000 4000 5000 6000
step

Figure 50.2: Gradient descent central flow for a CNN with CE loss, η = 0.006666.

120

train loss

5

gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

4
3
2
1
0

0

1000 2000 3000 4000 5000 6000
step

gradient norm2
gradient descent
gradient descent (average)
central flow
central flow prediction

30
20

300
250
200
150
100
50
0

network outputs on test example 0
gradient descent
central flow

0

1000 2000 3000 4000 5000 6000
step

0.0000

0

1000 2000 3000 4000 5000 6000
step

test accuracy (%)

distance to gradient descent
central flow
stable flow

2.0
1.5
1.0

30

10
5
0
5
10
15
20

0

0.0001

50

0

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.0002
gradient descent
central flow
stable flow (top 1)

60

40
1000 2000 3000 4000 5000 6000
step

real vs. predicted oscillation covariance

0.0004
0.0003

70

10
0

top Hessian eigenvalues

gradient descent (midpoints)
central flow
0

1000 2000 3000 4000 5000 6000
step

0.5
0.0

0

network outputs on test example 1

15
10
5
0
5
10
15

gradient descent
central flow

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2
gradient descent
central flow

10
5
0
5
10
15

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

Figure 50.3: Gradient descent central flow for a CNN with CE loss, η = 0.01.
train loss
gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

1.25
1.00
0.75
0.50
0.25
0.00

0

1000 2000 3000 4000 5000 6000
step

gradient norm2
gradient descent
gradient descent (average)
central flow
central flow prediction

6
4

300
250
200
150
100
50
0

0

1000 2000 3000 4000 5000 6000
step

1000 2000 3000 4000 5000 6000
step

network outputs on test example 1
gradient descent
central flow

10
5

10
0

1000 2000 3000 4000 5000 6000
step

distance to gradient descent
central flow
stable flow

1.0
gradient descent (midpoints)
central flow

5

4

1000 2000 3000 4000 5000 6000
step

2.5

0

2

0

test accuracy (%)

0

network outputs on test example 0

0

0.00000

1.5

1000 2000 3000 4000 5000 6000
step

2

0.00002

60

45

gradient descent
central flow

0.00004

2.0

0

6

gradient descent
central flow
stable flow (top 1)

65

50

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.00006

55

0

real vs. predicted oscillation covariance

0.00010
0.00008

70

2

4

top Hessian eigenvalues

0

1000 2000 3000 4000 5000 6000
step

0.5
0.0

7.5
5.0
2.5
0.0
2.5
5.0

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2
gradient descent
central flow

0

1000 2000 3000 4000 5000 6000
step

Figure 50.4: Gradient descent central flow for a ResNet with CE loss, η = 0.01.

121

train loss
gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

1.25
1.00
0.75
0.50

0

1000 2000 3000 4000 5000 6000
step

gradient norm2
gradient descent
gradient descent (average)
central flow
central flow prediction

6
4

100

gradient descent
central flow
stable flow (top 1)

0

0

1000 2000 3000 4000 5000 6000
step

65
60
50

gradient descent (midpoints)
central flow

45
0

4
2
0
2
4
6

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 0

network outputs on test example 1

gradient descent
central flow

gradient descent
central flow

0

real vs. predicted oscillation covariance
0.000150
0.000125
0.000100
0.000075
0.000050
0.000025
0.000000

empirical variance
along (t) eigenvectors
(t) eigenvalues

0

1000 2000 3000 4000 5000 6000
step

test accuracy (%)

70

55

2
0

150

50

0.25
0.00

top Hessian eigenvalues
200

10
5
0
5
10

distance to gradient descent
3.0
2.5
2.0
1.5
1.0
0.5
0.0

central flow
stable flow

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2
gradient descent
central flow

5
0
5

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

Figure 50.5: Gradient descent central flow for a ResNet with CE loss, η = 0.013333.
train loss
1.25

gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

1.00
0.75
0.50
0.25
0.00

6
5
4
3
2
1
0

4
2
0
2
4
6

0

1000 2000 3000 4000 5000 6000
step

150
125
100
75
50
25
0

top Hessian eigenvalues

0.0002
gradient descent
central flow
stable flow (top 1)
0

1000 2000 3000 4000 5000 6000
step

60

1000 2000 3000 4000 5000 6000
step

distance to gradient descent

45

central flow
stable flow

2
gradient descent (midpoints)
central flow
0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 1

gradient descent
central flow

gradient descent
central flow

10
5
0
5
10

1000 2000 3000 4000 5000 6000
step

0

3

65

network outputs on test example 0

0

0.0000

test accuracy (%)

50
1000 2000 3000 4000 5000 6000
step

0.0001

70

55

0

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.0003

gradient norm2
gradient descent
gradient descent (average)
central flow
central flow prediction

real vs. predicted oscillation covariance

0

1000 2000 3000 4000 5000 6000
step

1
0

7.5
5.0
2.5
0.0
2.5
5.0
7.5

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2
gradient descent
central flow

0

1000 2000 3000 4000 5000 6000
step

Figure 50.6: Gradient descent central flow for a ResNet with CE loss, η = 0.02.

122

train loss

2.0

gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

1.5
1.0
0.5
0.0

0

1000

2000
step

3000

4000

gradient norm2
40

20
10
0

0

1000

2000
step

3000

gradient descent
central flow

0

1000

2000
step

3000

real vs. predicted oscillation covariance
empirical variance
along (t) eigenvectors
(t) eigenvalues

0.0003
0.0002
gradient descent
central flow
stable flow (top 1)
0

1000

2000
step

3000

4000

test accuracy (%)

0.0001
0.0000

0

1000

2000
step

3000

4000

distance to gradient descent

1.0

central flow
stable flow

0.8
0.6
0.4
gradient descent (midpoints)
central flow

4000

network outputs on test example 0
4
2
0
2
4
6

top Hessian eigenvalues

65
60
55
50
45
40

gradient descent
gradient descent (average)
central flow
central flow prediction

30

300
250
200
150
100
50
0

0

1000

2000
step

3000

4000

0.2
0.0

0

network outputs on test example 1

6
4
2
0
2
4
6

gradient descent
central flow

1000

2000
step

3000

4000

network outputs on test example 2
gradient descent
central flow

4
2
0
2
4

4000

0

1000

2000
step

3000

4000

0

1000

2000
step

3000

4000

Figure 50.7: Gradient descent central flow for a ViT with CE loss, η = 0.01.
train loss
2.0

top Hessian eigenvalues

gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

1.5
1.0

0.0

0

1000

2000
step

3000

4000

gradient norm2
30
25
20
15
10
5
0

6
4
2
0
2
4
6

0.0003

100
50

gradient descent
central flow
stable flow (top 1)
0

1000

2000
step

3000

4000

test accuracy (%)

65

gradient descent
gradient descent (average)
central flow
central flow prediction

3000

55

0.6

4000

0

1000

2000
step

3000

4000

1000

2000
step

1000

2000
step

3000

4000

distance to gradient descent
central flow
stable flow

3000

4000

0.2
0.0

network outputs on test example 1
gradient descent
central flow

5.0
2.5

2

5.0

4
2000
step

3000

4000

2000
step

3000

0

1000

2000
step

3000

Figure 50.8: Gradient descent central flow for a ViT with CE loss, η = 0.013333.

123

4000

gradient descent
central flow

2
0

1000

1000

network outputs on test example 2

2.5

0

0

4

0.0

7.5

0

0.4
gradient descent (midpoints)
central flow
0

network outputs on test example 0
gradient descent
central flow

0.0000

0.8

40
2000
step

0.0001

60

45
1000

0.0002

1.0

50

0

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.0004

150

0.5
0

real vs. predicted oscillation covariance

0.0005

200

4000

train loss

2.5

gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

2.0
1.5
1.0
0.5
0.0

0

1000

2000
step

3000

4000

gradient norm2
15
10

1000

2000
step

3000

0

1000

gradient descent
central flow

4
2
0
2
4
6

0

1000

2000
step

3000

2000
step

3000

4000

test accuracy (%)

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.0002
0.0000

0

1000

0.8

55

0.6

2000
step

3000

4000

distance to gradient descent

1.0

60

central flow
stable flow

0.4
gradient descent (midpoints)
central flow

4000

network outputs on test example 0

real vs. predicted oscillation covariance

0.0004
gradient descent
central flow
stable flow (top 1)

45
0

0.0008
0.0006

50

5
0

top Hessian eigenvalues

65

gradient descent
gradient descent (average)
central flow
central flow prediction

20

150
125
100
75
50
25
0

0

1000

2000
step

3000

4000

0.2
0.0

0

network outputs on test example 1

6
4
2
0
2
4
6

gradient descent
central flow

4000

0

1000

2000
step

3000

4000

1000

2000
step

3000

4000

network outputs on test example 2
gradient descent
central flow

4
2
0
2
4
6

0

1000

2000
step

3000

4000

Figure 50.9: Gradient descent central flow for a ViT with CE loss, η = 0.02.

1.4
1.2
1.0
0.8
0.6
0.4
0.2

train loss
gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

0

1000 2000 3000 4000 5000 6000
step
gradient descent
gradient descent (average)
central flow
central flow prediction

8
6
4
2

6

0

100

0

gradient descent
central flow
stable flow (top 1)

network outputs on test example 0
gradient descent
central flow

2

0

1000 2000 3000 4000 5000 6000
step

test accuracy (%)

90
80
70
60
50
40
30

1000 2000 3000 4000 5000 6000
step

4

1000 2000 3000 4000 5000 6000
step

1000 2000 3000 4000 5000 6000
step

1000 2000 3000 4000 5000 6000
step

distance to gradient descent

1.0

central flow
stable flow

0.2
0.0

network outputs on test example 1
gradient descent
central flow

4

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2
gradient descent
central flow

2
0
2
4

2
0

0

0.4
gradient descent (midpoints)
central flow

0

2

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.6

0
6

real vs. predicted oscillation covariance
0.00030
0.00025
0.00020
0.00015
0.00010
0.00005
0.00000

0.8

2

0
4

150

50

gradient norm2

0

top Hessian eigenvalues
200

0

1000 2000 3000 4000 5000 6000
step

6

0

1000 2000 3000 4000 5000 6000
step

Figure 50.10: Gradient descent central flow for a LSTM with CE loss, η = 0.01333.

124

train loss
1.25
1.00
0.75
0.50
0.25
0.00

gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

0

1000 2000 3000 4000 5000 6000
step

gradient norm2
gradient descent
gradient descent (average)
central flow
central flow prediction

8
6

150
125
100
75
50
25
0

2

40
0

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 0
gradient descent
central flow

7.5
5.0
2.5
0.0
2.5
5.0
0

1000 2000 3000 4000 5000 6000
step

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.0004
gradient descent
central flow
stable flow (top 1)
1000 2000 3000 4000 5000 6000
step

gradient descent (midpoints)
central flow
0

8
6
4
2
0
2
4

0.0002
0.0000

0

1000 2000 3000 4000 5000 6000
step

test accuracy (%)

80
60

real vs. predicted oscillation covariance
0.0006

100

4
0

top Hessian eigenvalues

1000 2000 3000 4000 5000 6000
step

distance to gradient descent
2.5
2.0
1.5
1.0
0.5
0.0

central flow
stable flow

0

network outputs on test example 1
gradient descent
central flow

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2
gradient descent
central flow

4
2
0
2
4

0

1000 2000 3000 4000 5000 6000
step

6

0

1000 2000 3000 4000 5000 6000
step

Figure 50.11: Gradient descent central flow for a LSTM with CE loss, η = 0.02.
train loss

2.5

gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

2.0
1.5
1.0

0

1000 2000 3000 4000 5000 6000
step

gradient norm2
gradient descent
gradient descent (average)
central flow
central flow prediction

5
4
3
2

10

40

0

0

network outputs on test example 0
gradient descent
central flow

0

1000 2000 3000 4000 5000 6000
step

test accuracy (%)

80
60

5
0
5
0

1000 2000 3000 4000 5000 6000
step

gradient descent (midpoints)
central flow
0

10.0
7.5
5.0
2.5
0.0
2.5
5.0

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.0010
gradient descent
central flow
stable flow (top 1)

100

1000 2000 3000 4000 5000 6000
step

real vs. predicted oscillation covariance
0.0015

40

1
0

60

20

0.5
0.0

top Hessian eigenvalues

1000 2000 3000 4000 5000 6000
step

network outputs on test example 1
gradient descent
central flow

0

1000 2000 3000 4000 5000 6000
step

0.0005
0.0000

1.2
1.0
0.8
0.6
0.4
0.2
0.0

7.5
5.0
2.5
0.0
2.5
5.0

0

1000 2000 3000 4000 5000 6000
step

distance to gradient descent
central flow
stable flow

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2
gradient descent
central flow

0

1000 2000 3000 4000 5000 6000
step

Figure 50.12: Gradient descent central flow for a LSTM with CE loss, η = 0.04.

125

train loss

2.5

gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

2.0
1.5
1.0
0.5
0.0

0

1000

2000
step

3000

4000

gradient norm2
10
8
6
4
2
0

300
250
200
150
100
50
0

top Hessian eigenvalues

0.00010
gradient descent
central flow
stable flow (top 1)
0

1000

2000
step

3000

2
4
1000

2000
step

3000

0

1000

2000
step

3000

4000

central flow
stable flow

0.6
0.4
gradient descent (midpoints)
central flow

4000

0

0.00000

distance to gradient descent

60

gradient descent
central flow

0

4000

80

0

network outputs on test example 0
2

3000

0.8

20
1000

2000
step

0.00005

test accuracy (%)

40
0

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.00015

100

gradient descent
gradient descent (average)
central flow
central flow prediction

real vs. predicted oscillation covariance

1000

2000
step

3000

4000

network outputs on test example 1
1
0
1
2
3
4

gradient descent
central flow

0.2
0.0

0

1000

2000
step

3000

4000

network outputs on test example 2

2

gradient descent
central flow

1
0
1
2

4000

0

1000

2000
step

3000

4000

0

1000

2000
step

3000

4000

Figure 50.13: Gradient descent central flow for a Transformer with CE loss, η = 0.01.
train loss

2.5

top Hessian eigenvalues

gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

2.0
1.5
1.0
0.5
0.0

1000

2000
step

3000

4000

gradient norm2

100

gradient descent
central flow
stable flow (top 1)

8
6

0

0

1000

40

gradient descent (midpoints)
central flow

20
2000
step

3000

4000

60

2
1000

3000

80

4

0

2000
step

test accuracy (%)

100

gradient descent
gradient descent (average)
central flow
central flow prediction

10

0

150

50
0

real vs. predicted oscillation covariance

200

4000

0

1000

2000
step

3000

4000

network outputs on test example 0

network outputs on test example 1

gradient descent
central flow

gradient descent
central flow

2
0
2
4
0

1000

2000
step

3000

4000

1
0
1
2
3
4

0.00025
0.00020
0.00015
0.00010
0.00005
0.00000

1.2
1.0
0.8
0.6
0.4
0.2
0.0

2

empirical variance
along (t) eigenvectors
(t) eigenvalues

0

1000

2000
step

3000

4000

distance to gradient descent
central flow
stable flow

0

1000

2000
step

3000

4000

network outputs on test example 2
gradient descent
central flow

1
0
1
2

0

1000

2000
step

3000

4000

3

0

1000

2000
step

3000

4000

Figure 50.14: Gradient descent central flow for a Transformer with CE loss, η = 0.013333.

126

train loss

2.5

gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

2.0
1.5
1.0
0.5
0.0

0

1000

2000
step

3000

4000

gradient norm2
8
6

gradient descent
central flow
stable flow (top 1)
0

1000

3000

2000
step

3000

4000

4000

gradient descent (midpoints)
central flow
0

1000

2000
step

3000

4000

gradient descent
central flow

0
2
4

2000
step

3000

4000

0

1000

2000
step

3000

4000

central flow
stable flow

1.0

network outputs on test example 1

1000

0.0000

distance to gradient descent

gradient descent
central flow

0

0.0001

1.5

network outputs on test example 0
2
1
0
1
2
3

0.0002

test accuracy (%)

20
2000
step

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.0003

40
1000

real vs. predicted oscillation covariance

0.0004

60

2
0

0.0005

80

4
0

top Hessian eigenvalues

100

gradient descent
gradient descent (average)
central flow
central flow prediction

10

150
125
100
75
50
25
0

0

1000

2000
step

3000

0.5
0.0

0

1000

2000
step

3000

4000

network outputs on test example 2

2
1
0
1
2
3

gradient descent
central flow

4000

0

1000

2000
step

3000

4000

Figure 50.15: Gradient descent central flow for a Transformer with CE loss, η = 0.02.
train loss
gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

1.0
0.8
0.6
0.4
0.2
0.0

0

1000 2000 3000 4000 5000 6000
step

300
250
200
150
100
50
0

top Hessian eigenvalues

gradient descent
central flow
stable flow (top 1)
0

1000 2000 3000 4000 5000 6000
step

gradient norm2
12
10
8
6
4
2
0

gradient descent
gradient descent (average)
central flow
central flow prediction

70

0.6

1000 2000 3000 4000 5000 6000
step
gradient descent
central flow

0

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

distance to gradient descent
0.8

gradient descent (midpoints)
central flow
0

7.5
5.0
2.5
0.0
2.5
5.0

central flow
stable flow

0.4

60

network outputs on test example 0
7.5
5.0
2.5
0.0
2.5
5.0
7.5

empirical variance
along (t) eigenvectors
(t) eigenvalues

test accuracy (%)
75

65

0

real vs. predicted oscillation covariance
0.00012
0.00010
0.00008
0.00006
0.00004
0.00002
0.00000

1000 2000 3000 4000 5000 6000
step

0.2
0.0

network outputs on test example 1
gradient descent
central flow

0

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2
gradient descent
central flow

7.5
5.0
2.5
0.0
2.5
5.0
0

1000 2000 3000 4000 5000 6000
step

Figure 50.16: Gradient descent central flow for a Mamba with CE loss, η = 0.01.

127

train loss

1.0

gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

0.8
0.6
0.4

0

1000 2000 3000 4000 5000 6000
step

gradient norm2
10

gradient descent
gradient descent (average)
central flow
central flow prediction

8
6
4

gradient descent
central flow
stable flow (top 1)
0

1000 2000 3000 4000 5000 6000
step

test accuracy (%)

80
75
70

gradient descent (midpoints)
central flow

60
0

10

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 0

network outputs on test example 1

gradient descent
central flow

gradient descent
central flow

5

5
0

0

0

1000 2000 3000 4000 5000 6000
step

0.00005
0.00000

1.2
1.0
0.8
0.6
0.4
0.2
0.0

0

1000 2000 3000 4000 5000 6000
step

distance to gradient descent
central flow
stable flow

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2

10

gradient descent
central flow

5
0

5

5

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.00010

100

65

2
0

150

0

real vs. predicted oscillation covariance
0.00015

50

0.2
0.0

top Hessian eigenvalues
200

5

10

0

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

Figure 50.17: Gradient descent central flow for a Mamba with CE loss, η = 0.013333.
train loss
gradient descent
gradient descent (average)
central flow
central flow prediction
stable flow

0.8
0.6
0.4
0.2
0.0

0

1000 2000 3000 4000 5000 6000
step

150
125
100
75
50
25
0

top Hessian eigenvalues

gradient descent
central flow
stable flow (top 1)
0

1000 2000 3000 4000 5000 6000
step

gradient norm2
gradient descent
gradient descent (average)
central flow
central flow prediction

8
6
4

0

10

80
75

gradient descent
central flow

5

gradient descent (midpoints)
central flow
0

10

distance to gradient descent

1000 2000 3000 4000 5000 6000
step

network outputs on test example 1
gradient descent
central flow

5

0.0

10

0

5

5

5

1000 2000 3000 4000 5000 6000
step

10

0

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example 2
gradient descent
central flow

5

0

0

central flow
stable flow

0.5

0

10

1000 2000 3000 4000 5000 6000
step

1.0

70

1000 2000 3000 4000 5000 6000
step

network outputs on test example 0

0

1.5

60
0

empirical variance
along (t) eigenvectors
(t) eigenvalues

test accuracy (%)

65

2

real vs. predicted oscillation covariance
0.00030
0.00025
0.00020
0.00015
0.00010
0.00005
0.00000

10

0

1000 2000 3000 4000 5000 6000
step

Figure 50.18: Gradient descent central flow for a Mamba with CE loss, η = 0.02.

128

E.2

Scalar RMSProp
train loss

0.40

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

0.35
0.30
0.25

1e 5

Scalar RMSProp
central flow
stable flow (top 1)

3
2

0

1000

2000 3000
step

4000

5000

gradient norm2
3

5000

Scalar RMSProp
central flow

5000

0

Scalar RMSProp
central flow
stable flow (top 1)
0

1000

2000 3000
step

4000

66

2000 3000
step

4000

5000

0.01
0

1000 2000 3000 4000 5000
step

distance to Scalar RMSProp

0.25

central flow
stable flow

0.20
0.15

62

0.10

60
1000 2000 3000 4000 5000
step

1000

Scalar RMSProp
central flow

5000

test accuracy (%)

58

0

effective step size /

64

0

0.0

top hessian eigenvalues

network outputs on test example

0.8
0.6
0.4
0.2
0.0
0.2

4000

0.02

200
4000

2000 3000
step

600

1
2000 3000
step

1000

0.03

400

1000

1.0

800

2

0

0

1000

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

4

0

empirical variance
along (t) eigenvectors
(t) eigenvalues

1.5

0.5

1

0.20

0

real vs. predicted oscillation covariance

top effective hessian eigenvalues

4

Scalar RMSProp (midpoints)
central flow
0

1000 2000 3000 4000 5000
step

0.05
0.00

0

1000

2000 3000
step

4000

5000

Figure 51: Annotated example of a Scalar RMSProp experiment. Using Scalar RMSProp with η = 2/400,
β2 = 0.99, and bias correction, we train a Mamba network on a synthetic sequence prediction task with MSE loss.
The central flow (black) accurately models the long-term trajectory of Scalar RMSProp (blue), whereas the stable flow
(red) takes a different path. As described in Section B.1, we terminate the stable flow once the effective sharpness
gets too high.
Top left: See Figure 48 caption. The central flow’s prediction for the time-averaged loss is given by eq. (116).
Top center: We plot the top several eigenvalues of the effective Hessian √ην H(w) under both Scalar RMSProp
(colors) and its central flow (dashed black). Under Scalar RMSProp, these eigenvalues equilibrate around the critical
threshold 2, whereas under the central flow they are fixed exactly at 2. We also plot the top eigenvalue under the
“stable flow” baseline (red), and this increases far above 2.
Top right: See Figure 48 caption. This plot is validating eq. (118).
Middle left: See Figure 48 caption. The central flow’s prediction for the time-average is given by eq. (117).
Middle center: We plot the top several eigenvalues of the “raw” Hessian H(w), under both Scalar RMSProp (colors)
and the central flow (dashed black). These evolve throughout training, even as top eigenvalues of the effective Hessian
are equilibrating at the critical threshold (top center). In red, we plot the top Hessian eigenvalue under the stable flow.
√
Middle right: We plot the effective step size η/ ν under both Scalar RMSProp (blue) and the central flow (dashed
black). This effective step size oscillates under Scalar RMSProp, but varies smoothly under the central flow.
Bottom left: We show the network’s final-layer predictions on an arbitrary example. Under Scalar RMSProp
(colors) these predictions oscillate due to the oscillations in weight space. Under the central flow (dashed black), the
predictions evolve smoothly while following the same macroscopic path.
Bottom center: See Figure 48 caption.
Bottom right: See Figure 48 caption.

129

train loss
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

0.4
0.3
0.2

1e 5

Scalar RMSProp
central flow
stable flow (top 1)

3

0

2000

4000
step

6000

8000

0

0.2
0

2000

6

4000
step

6000

8000

0

Scalar RMSProp
central flow
stable flow (top 1)
0

2000

4000
step

6000

8000

0.0

60

40

0.2
0

2000

4000
step

6000

8000

6000

8000

0.02
0.01
0.00

0

Scalar RMSProp (midpoints)
central flow

30

0

2000

4000
step

6000

2000

4000
step

6000

8000

distance to Scalar RMSProp

50

0.2

4000
step

Scalar RMSProp
central flow

test accuracy (%)

Scalar RMSProp
central flow

0.4

2000

0.03

network outputs on test example
0.6

0

effective step size /

1000

2
4000
step

0.0

0.04

500
2000

8000

1500

4

0

6000

top hessian eigenvalues

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

8

0.8
0.4

gradient norm2

10

empirical variance
along (t) eigenvectors
(t) eigenvalues

1.0
0.6

2
1

0.1

0

real vs. predicted oscillation covariance

top effective hessian eigenvalues

4

8000

1.25
1.00
0.75
0.50
0.25
0.00

central flow
stable flow

0

2000

4000
step

6000

8000

Figure 52.1: Scalar RMSProp central flow for a CNN with MSE loss, η = 0.003, β2 = 0.99, and bias correction.

train loss
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

0.4
0.3
0.2
0.0

0

4000
step

6000

8000

7.5

1
0

2000

2.5

200
2000

4000
step

6000

8000

Scalar RMSProp
central flow

0.4
0.2
0.0
0.2
2000

4000
step

6000

0

0

2000

0

8000

6000

8000

effective step size /

Scalar RMSProp
central flow
stable flow (top 1)
0

2000

4000
step

6000

8000

test accuracy (%)

Scalar RMSProp
central flow

0.02
0.00

0

1.00

50

0.75

2000

4000
step

6000

8000

distance to Scalar RMSProp

1.25

60

30

4000
step

0.04

central flow
stable flow

0.50

40
0

8000

0.06

network outputs on test example
0.6

6000

600
400

0

4000
step

800

5.0
0.0

empirical variance
along (t) eigenvectors
(t) eigenvalues

4

top hessian eigenvalues

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

10.0

1e 5

2

gradient norm2

12.5

5
3

2
1

2000

Scalar RMSProp
central flow
stable flow (top 1)

3

0.1
0

real vs. predicted oscillation covariance

top effective hessian eigenvalues

4

Scalar RMSProp (midpoints)
central flow
0

2000

4000
step

6000

8000

0.25
0.00

0

2000

4000
step

6000

8000

Figure 52.2: Scalar RMSProp central flow for a CNN with MSE loss, η = 0.006, β2 = 0.99, and bias correction.

130

train loss
0.4
0.3
0.2

0

2000

4000
step

6000

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

4
3
2
1
0

0

2000

4000
step

6000

8000

0.00010
0.00005
0

2000

2000

4000
step

6000

8000

0.00000

0

2000

4000
step

6000

8000

effective step size /
Scalar RMSProp
central flow

0.125
0.100
0.075
Scalar RMSProp
central flow
stable flow (top 1)
0

2000

4000
step

6000

0.050
0.025

8000

test accuracy (%)

0

0.8

50

0.6

2000

4000
step

6000

8000

distance to Scalar RMSProp

1.0

60

central flow
stable flow

0.4

40
0

6000

top hessian eigenvalues

network outputs on test example
0.75
0.50
0.25
0.00
0.25
0.50

4000
step

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.00015

1

300
250
200
150
100
50
0

Scalar RMSProp
central flow

real vs. predicted oscillation covariance

0.00020

2

0

8000

gradient norm2

5

Scalar RMSProp
central flow
stable flow (top 1)

3

0.1
0.0

top effective hessian eigenvalues

4

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

Scalar RMSProp (midpoints)
central flow

8000

0

2000

4000
step

6000

8000

0.2
0.0

0

2000

4000
step

6000

8000

Figure 52.3: Scalar RMSProp central flow for a CNN with MSE loss, η = 0.01, β2 = 0.99, and bias correction.

train loss
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

0.3
0.2
0.1

Scalar RMSProp
central flow
stable flow (top 1)

3
2
1

0

500 1000 1500 2000 2500 3000
step

0

0

500 1000 1500 2000 2500 3000
step

gradient norm2
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

1.5
1.0
0.5
0.0

500 1000 1500 2000 2500 3000
step

network outputs on test example
Scalar RMSProp
central flow

0.4

200

0.020

0

Scalar RMSProp
central flow
stable flow (top 1)
0

500 1000 1500 2000 2500 3000
step

0.1

50

0.015
0.010
0

test accuracy (%)

500 1000 1500 2000 2500 3000
step

distance to Scalar RMSProp
central flow
stable flow

1.5

65
55

Scalar RMSProp
central flow

500 1000 1500 2000 2500 3000
step

70

0.2

1.0
Scalar RMSProp (midpoints)
central flow

45
500 1000 1500 2000 2500 3000
step

0

effective step size /
0.025

60

0

empirical variance
along (t) eigenvectors
(t) eigenvalues

top hessian eigenvalues

0.3

0.0

1e 5

1.25
1.00
0.75
0.50
0.25
0.00

300

100
0

real vs. predicted oscillation covariance

top effective hessian eigenvalues

4

0

2000

4000
step

6000

8000

0.5
0.0

0

500 1000 1500 2000 2500 3000
step

Figure 52.4: Scalar RMSProp central flow for a ResNet with MSE loss, η = 0.01, β2 = 0.99, and bias correction.

131

train loss
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

0.35
0.30
0.25
0.20
0.10

Scalar RMSProp
central flow
stable flow (top 1)

3
2

0

500 1000 1500 2000 2500 3000
step

0

0

500 1000 1500 2000 2500 3000
step

gradient norm2

1.5

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

1.0
0.5
0.0
0

500 1000 1500 2000 2500 3000
step

network outputs on test example
Scalar RMSProp
central flow

0.4

2

0

0

500 1000 1500 2000 2500 3000
step

effective step size /
0.030

0.020
Scalar RMSProp
central flow
stable flow (top 1)
0

500 1000 1500 2000 2500 3000
step

55

0.1

50

0.010

0

500 1000 1500 2000 2500 3000
step

distance to Scalar RMSProp
1.5

central flow
stable flow

1.0
Scalar RMSProp (midpoints)
central flow

45
500 1000 1500 2000 2500 3000
step

0.015

test accuracy (%)

65
60

Scalar RMSProp
central flow

0.025

70

0.2

0

empirical variance
along (t) eigenvectors
(t) eigenvalues

top hessian eigenvalues
250
200
150
100
50
0

0.3

0.0

1e 5

3

1

1

0.15

real vs. predicted oscillation covariance

top effective hessian eigenvalues

4

0

1000 2000 3000 4000 5000
step

0.5
0.0

0

500 1000 1500 2000 2500 3000
step

Figure 52.5: Scalar RMSProp central flow for a ResNet with MSE loss, η = 0.02, β2 = 0.99, and bias correction.
train loss

0.35

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

0.30
0.25
0.20
0.15

top effective hessian eigenvalues

4

Scalar RMSProp
central flow
stable flow (top 1)

3
2
1

0.10
0

500 1000 1500 2000 2500 3000
step

0

0

500 1000 1500 2000 2500 3000
step

gradient norm2
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

1.5
1.0
0.5
0.0

0.6

0

500 1000 1500 2000 2500 3000
step

network outputs on test example
Scalar RMSProp
central flow

0.4

500 1000 1500 2000 2500 3000
step

500 1000 1500 2000 2500 3000
step

effective step size /

Scalar RMSProp
central flow
stable flow (top 1)
0

0.03
0.02

500 1000 1500 2000 2500 3000
step

0

test accuracy (%)

70

500 1000 1500 2000 2500 3000
step

distance to Scalar RMSProp
central flow
stable flow

1.25

65

45

Scalar RMSProp
central flow

0.04

1.00
0.75
0.50

50
0

0

0.05

55

0.0

empirical variance
along (t) eigenvectors
(t) eigenvalues

top hessian eigenvalues
150
125
100
75
50
25
0

60

0.2

real vs. predicted oscillation covariance

0.000150
0.000125
0.000100
0.000075
0.000050
0.000025
0.000000

Scalar RMSProp (midpoints)
central flow
0

1000 2000 3000 4000 5000
step

0.25
0.00

0

500 1000 1500 2000 2500 3000
step

Figure 52.6: Scalar RMSProp central flow for a ResNet with MSE loss, η = 0.03, β2 = 0.99, and bias correction.

132

train loss
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

0.35
0.30
0.25

Scalar RMSProp
central flow
stable flow (top 1)

3

0

1000 2000 3000 4000 5000 6000
step

gradient norm2
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

0

0.001
0

1000 2000 3000 4000 5000 6000
step

800

Scalar RMSProp
central flow

0

Scalar RMSProp
central flow

0.01

1000 2000 3000 4000 5000 6000
step

0

0.2

40

1000 2000 3000 4000 5000 6000
step

distance to Scalar RMSProp
1.0

60
50

central flow
stable flow

0.8
0.6
0.4
Scalar RMSProp (midpoints)
central flow

30
0

effective step size /

test accuracy (%)

0.4

0.0

1000 2000 3000 4000 5000 6000
step

0.02
Scalar RMSProp
central flow
stable flow (top 1)

network outputs on test example
0.6

0

0.03

600

0

0.000

top hessian eigenvalues

1000

200
1000 2000 3000 4000 5000 6000
step

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.004
0.002

400

0

real vs. predicted oscillation covariance
0.005
0.003

2
1

0.20

12.5
10.0
7.5
5.0
2.5
0.0

top effective hessian eigenvalues

4

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

0.2
0.0

0

1000 2000 3000 4000 5000 6000
step

Figure 52.7: Scalar RMSProp central flow for a ViT with MSE loss, η = 0.01, β2 = 0.99, and bias correction.
train loss

0.40

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

0.35
0.30
0.25
0.20
0.15

0

1000 2000 3000 4000 5000 6000
step

top effective hessian eigenvalues

4

Scalar RMSProp
central flow
stable flow (top 1)

3

0.004

1

0.002

0

0

1000 2000 3000 4000 5000 6000
step

10
8
6

top hessian eigenvalues
400

2

100

0

0

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example
Scalar RMSProp
central flow

0.6
0.4
0.2
0.0
0.2
0

1000 2000 3000 4000 5000 6000
step

65
60
55
50
45
40
35

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.000

0

1000 2000 3000 4000 5000 6000
step

effective step size /

0.04

Scalar RMSProp
central flow

0.03

300
200

4

real vs. predicted oscillation covariance

0.006

2

gradient norm2
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

0.008

Scalar RMSProp
central flow
stable flow (top 1)
0

0.02
0.01

1000 2000 3000 4000 5000 6000
step

0

test accuracy (%)

1000 2000 3000 4000 5000 6000
step

distance to Scalar RMSProp
central flow
stable flow

1.0
0.8
0.6
0.4
Scalar RMSProp (midpoints)
central flow
0

1000 2000 3000 4000 5000 6000
step

0.2
0.0

0

1000 2000 3000 4000 5000 6000
step

Figure 52.8: Scalar RMSProp central flow for a ViT with MSE loss, η = 0.02, β2 = 0.99, and bias correction.

133

train loss

0.5
0.4
0.3

Scalar RMSProp
central flow
stable flow (top 1)

3
2
1

0.2
0

0

1000 2000 3000 4000 5000
step

gradient norm2

8

4

0

1000 2000 3000 4000 5000
step

150

0

network outputs on test example

1.00
0.75
0.50
0.25
0.00
0.25
0.50

Scalar RMSProp
central flow

0

0

1000 2000 3000 4000 5000
step
Scalar RMSProp
central flow

0.03
Scalar RMSProp
central flow
stable flow (top 1)

50
1000 2000 3000 4000 5000
step

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.04

100

0

real vs. predicted oscillation covariance

effective step size /

200

2

0.012
0.010
0.008
0.006
0.004
0.002
0.000

top hessian eigenvalues

250

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

6

0

top effective hessian eigenvalues

4

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

0

1000 2000 3000 4000 5000
step

0.02
0.01

0

test accuracy (%)

65
60
55
50
45
40

distance to Scalar RMSProp

Scalar RMSProp (midpoints)
central flow

1000 2000 3000 4000 5000 6000
step

0

1000 2000 3000 4000 5000 6000
step

1000 2000 3000 4000 5000 6000
step

1.0
0.8
0.6
0.4
0.2
0.0

central flow
stable flow

0

1000 2000 3000 4000 5000
step

Figure 52.9: Scalar RMSProp central flow for a ViT with MSE loss, η = 0.03, β2 = 0.99, and bias correction.

train loss

0.4

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

0.3
0.2
0.1
0

2000

4000
step

6000

8000

3

4

1

2

0

1.5

2000

0.5

100
2000

4000
step

6000

8000

network outputs on test example

0.2

6000

8000

Scalar RMSProp
central flow

0.0

0

Scalar RMSProp
central flow
stable flow (top 1)
0

2000

4000
step

6000

8000

4000
step

6000

8000

effective step size /
Scalar RMSProp
central flow

4000
step

6000

8000

0.10
0.05
0.00

0

test accuracy (%)

100

2000

4000
step

6000

8000

distance to Scalar RMSProp
central flow
stable flow

2.0

80

1.5
1.0
Scalar RMSProp (midpoints)
central flow

0.4
2000

2000

0.15

40
0

0

0.20

60

0.2

0

0.25

300
200

0

4000
step

400

1.0
0.0

0

empirical variance
along (t) eigenvectors
(t) eigenvalues

6

2

top hessian eigenvalues

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

2.0

1e 5

Scalar RMSProp
central flow
stable flow (top 1)

gradient norm2

2.5

real vs. predicted oscillation covariance

top effective hessian eigenvalues

4

0

2000

4000
step

6000

8000

0.5
0.0

0

2000

4000
step

6000

8000

Figure 52.10: Scalar RMSProp central flow for a LSTM with MSE loss, η = 0.01, β2 = 0.99, and bias correction.

134

train loss

0.4

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

0.3
0.2

Scalar RMSProp
central flow
stable flow (top 1)

3
2
1

0.1
0

2000

4000
step

6000

8000

0

0

2000

gradient norm2

2.5
1.5

150

0.3

0.5

50
4000
step

6000

8000

network outputs on test example

0.2

Scalar RMSProp
central flow

0.0

0

0.4

40
2000

4000
step

6000

0

2000

4000
step

6000

8000

0

2000

4000
step

6000

8000

effective step size /
Scalar RMSProp
central flow

0.1
0.0

0

test accuracy (%)

Scalar RMSProp (midpoints)
central flow

8000

0

2000

4000
step

6000

2000

4000
step

6000

8000

distance to Scalar RMSProp

80
60

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.2
Scalar RMSProp
central flow
stable flow (top 1)

100

0.2

0

8000

0.4

100

2000

6000

real vs. predicted oscillation covariance

0.00030
0.00025
0.00020
0.00015
0.00010
0.00005
0.00000

200

1.0

0

4000
step

top hessian eigenvalues

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

2.0

0.0

top effective hessian eigenvalues

4

8000

1.50
1.25
1.00
0.75
0.50
0.25
0.00

central flow
stable flow

0

2000

4000
step

6000

8000

Figure 52.11: Scalar RMSProp central flow for a LSTM with MSE loss, η = 0.02, β2 = 0.99, and bias correction.
train loss
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

0.3
0.2
0.1
0.0

top effective hessian eigenvalues

4

Scalar RMSProp
central flow
stable flow (top 1)

3
2

2000

4000
step

6000

8000

0

1.0

0

2000

0.2

4000
step

4000
step

6000

8000

network outputs on test example
Scalar RMSProp
central flow

0.0
0.2
0.4
0

2000

4000
step

6000

0.0000

0

2000

8000

0

Scalar RMSProp
central flow
stable flow (top 1)

100
90
80
70
60
50
40

0

2000

4000
step

6000

8000

test accuracy (%)

Scalar RMSProp (midpoints)
central flow
0

2000

4000
step

6000

4000
step

6000

8000

effective step size /

60
20

2000

8000

80
40

0

6000

top hessian eigenvalues

0.5
0.0

0.0006
0.0002

gradient norm2
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

1.5

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.0008
0.0004

1
0

real vs. predicted oscillation covariance

8000

Scalar RMSProp
central flow

0.6
0.5
0.4
0.3
0.2
0.1
0.0

1.50
1.25
1.00
0.75
0.50
0.25
0.00

0

2000

4000
step

6000

8000

distance to Scalar RMSProp
central flow
stable flow

0

2000

4000
step

6000

8000

Figure 52.12: Scalar RMSProp central flow for a LSTM with MSE loss, η = 0.03, β2 = 0.99, and bias correction.

135

train loss

0.5

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

0.4
0.3
0.2
0.1
0.0

0

2000

4000
step

6000

8000

2

3

4

1

2
0

2000

4000
step

2000

4000
step

6000

8000

network outputs on test example
Scalar RMSProp
central flow

0.0

6000

8000

0.2

0.04

200

0.03
Scalar RMSProp
central flow
stable flow (top 1)

0

0

2000

4000
step

6000

0.4
4000
step

6000

2000

4000
step

6000

8000

Scalar RMSProp
central flow

0.02
0.01

8000

test accuracy (%)

100

0

4

60

3

2000

4000
step

6000

8000

distance to Scalar RMSProp

5

80

central flow
stable flow

2
Scalar RMSProp (midpoints)
central flow

20
2000

0

effective step size /

40

0

0

300

100
0

6

2

0

empirical variance
along (t) eigenvectors
(t) eigenvalues

8

top hessian eigenvalues

1
0

1e 6

Scalar RMSProp
central flow
stable flow (top 1)

gradient norm2
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

3

real vs. predicted oscillation covariance

top effective hessian eigenvalues

4

8000

0

2000

4000
step

6000

8000

1
0

0

2000

4000
step

6000

8000

Figure 52.13: Scalar RMSProp central flow for a Transformer with MSE loss, η = 0.01, β2 = 0.99, and bias
correction.

train loss
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

0.4
0.3
0.2
0.1
0

2000

4000
step

6000

8000

3

2

1

1

0

0

2000

0.4

4000
step

0.2
0.0
4000
step

6000

8000

network outputs on test example
Scalar RMSProp
central flow

0.0
0.2

0.04

0

Scalar RMSProp
central flow
stable flow (top 1)
0

2000

2000

4000
step

6000

8000

6000

8000

test accuracy (%)

100

2

4000
step

6000

8000

8000

Scalar RMSProp
central flow

0.01

60

2000

6000

effective step size /

0

2000

4000
step

6000

8000

distance to Scalar RMSProp

4

Scalar RMSProp (midpoints)
central flow

4000
step

0.02

3

0

2000

0.03

80

20
0

4000
step

0

0.06

150

40

0.4

0

0.05

50
2000

8000

200
100

0

6000

empirical variance
along (t) eigenvectors
(t) eigenvalues

3

2

top hessian eigenvalues

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

0.6

1e 5

Scalar RMSProp
central flow
stable flow (top 1)

gradient norm2
0.8

real vs. predicted oscillation covariance

top effective hessian eigenvalues

4

central flow
stable flow

1
0

0

2000

4000
step

6000

8000

Figure 52.14: Scalar RMSProp central flow for a Transformer with MSE loss, η = 0.02, β2 = 0.99, and bias
correction.

136

train loss
0.4
0.3
0.2

real vs. predicted oscillation covariance

Scalar RMSProp
central flow
stable flow (top 1)

3
2
1

0.1
0.0

top effective hessian eigenvalues

4

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

0

2000

4000
step

6000

gradient norm2

0.8

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

0.6
0.4
0.2
0.0
0

2000

4000
step

6000

8000

network outputs on test example

0.1
0.0
0.1
0.2
0.3
0.4

0

8000

120
100
80
60
40
20
0

0

2000

6000

8000

4000
step

6000

2000

4000
step

6000

8000

Scalar RMSProp
central flow

0.08
0.06
Scalar RMSProp
central flow
stable flow (top 1)
0

2000

4000
step

6000

0.04

8000

test accuracy (%)

80

Scalar RMSProp (midpoints)
central flow

40
2000

0

effective step size /

60

0

empirical variance
along (t) eigenvectors
(t) eigenvalues

top hessian eigenvalues

100

Scalar RMSProp
central flow

4000
step

0.00012
0.00010
0.00008
0.00006
0.00004
0.00002
0.00000

8000

0

2000

4000
step

6000

8000

0.02

3.0
2.5
2.0
1.5
1.0
0.5
0.0

0

2000

4000
step

6000

8000

distance to Scalar RMSProp
central flow
stable flow

0

2000

4000
step

6000

8000

Figure 52.15: Scalar RMSProp central flow for a Transformer with MSE loss, η = 0.03, β2 = 0.99, and bias
correction.

train loss

0.40

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

0.35
0.30
0.25

1e 5

Scalar RMSProp
central flow
stable flow (top 1)

3
2

0

1000

2000 3000
step

4000

5000

gradient norm2
3
1

200
2000 3000
step

4000

5000

0

Scalar RMSProp
central flow

4000

5000

1000

2000 3000
step

4000

5000

Scalar RMSProp
central flow

0.02
Scalar RMSProp
central flow
stable flow (top 1)
0

1000

2000 3000
step

4000

0.01

5000

66

0

1000 2000 3000 4000 5000
step

distance to Scalar RMSProp

0.25

central flow
stable flow

0.20
0.15

62

0.10

60
1000 2000 3000 4000 5000
step

0

0.03

64

0

0.0

effective step size /

test accuracy (%)

58

1.0

top hessian eigenvalues

network outputs on test example

0.8
0.6
0.4
0.2
0.0
0.2

2000 3000
step

600
400

1000

1000

800

2

0

0

1000

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

4

0

empirical variance
along (t) eigenvectors
(t) eigenvalues

1.5

0.5

1

0.20

0

real vs. predicted oscillation covariance

top effective hessian eigenvalues

4

Scalar RMSProp (midpoints)
central flow
0

1000 2000 3000 4000 5000
step

0.05
0.00

0

1000

2000 3000
step

4000

5000

Figure 52.16: Scalar RMSProp central flow for a Mamba with MSE loss, η = 0.007, β2 = 0.99, and bias correction.

137

train loss
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

0.30
0.25
0.20
0.15

Scalar RMSProp
central flow
stable flow (top 1)

3

0

1000

2000 3000
step

4000

5000

1.0

0

0

1000

2000 3000
step

2000 3000
step

Scalar RMSProp
central flow
stable flow (top 1)

4000

5000

0

0

1000

2000 3000
step

4000

5000

4000

5000

0.02
0.01
0

1000

test accuracy (%)

4000

5000

central flow
stable flow

0.20

66

58

2000 3000
step

distance to Scalar RMSProp

68

0.15
0.10
Scalar RMSProp (midpoints)
central flow

60
4000

2000 3000
step

Scalar RMSProp
central flow

5000

62

2000 3000
step

1000

0.03

64

1000

0

effective step size /

200

Scalar RMSProp
central flow

0

0

0.04

network outputs on test example

0.8
0.6
0.4
0.2
0.0
0.2

5000

300

100
1000

4000

top hessian eigenvalues

0.5
0

4

empirical variance
along (t) eigenvectors
(t) eigenvalues

2

gradient norm2
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

1.5

6 1e 5

2
1

2.0

0.0

real vs. predicted oscillation covariance

top effective hessian eigenvalues

4

0

2000

4000
step

6000

8000

0.05
0.00

0

1000

2000 3000
step

4000

5000

Figure 52.17: Scalar RMSProp central flow for a Mamba with MSE loss, η = 0.01, β2 = 0.99, and bias correction.
train loss
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

0.30
0.25
0.20
0.15
0.10

0

1000

2000 3000
step

4000

5000

top effective hessian eigenvalues

4

Scalar RMSProp
central flow
stable flow (top 1)

3

0.00010

1

0.00005

0

0

1000

0

1000

2000 3000
step

4000

5000

Scalar RMSProp
central flow

0

4000

5000

0.00000

0

1000

Scalar RMSProp
central flow
stable flow (top 1)
0

1000

1000 2000 3000 4000 5000
step

2000 3000
step

4000

5000

Scalar RMSProp (midpoints)
central flow
0

4000

5000

Scalar RMSProp
central flow

0.07
0.06
0.05
0.04
0.03
0.02

0

test accuracy (%)
70
68
66
64
62
60
58

2000 3000
step

effective step size /

top hessian eigenvalues
125
100
75
50
25
0

network outputs on test example
0.8
0.6
0.4
0.2
0.0
0.2

2000 3000
step

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.00015

2

gradient norm2
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

1.25
1.00
0.75
0.50
0.25
0.00

real vs. predicted oscillation covariance

1000 2000 3000 4000 5000 6000
step

1000 2000 3000 4000 5000
step

distance to Scalar RMSProp
0.5
0.4
0.3
0.2
0.1
0.0

central flow
stable flow

0

1000

2000 3000
step

4000

5000

Figure 52.18: Scalar RMSProp central flow for a Mamba with MSE loss, η = 0.02, β2 = 0.99, and bias correction.

138

train loss
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

1.0
0.5
0.0

0

1000

2000 3000
step

4000

5000

Scalar RMSProp
central flow
stable flow (top 1)

3

1.0
0.5
0

1000

0.4

2000 3000
step

2000 3000
step

4000

5000

Scalar RMSProp
central flow

0

Scalar RMSProp
central flow
stable flow (top 1)
0

1000

2000 3000
step

5

40

4000

5000

5000

0.02
0.00

0

1000

2000 3000
step

4000

5000

distance to Scalar RMSProp
central flow
stable flow

3
2
Scalar RMSProp (midpoints)
central flow

10
4000

2000 3000
step

Scalar RMSProp
central flow

test accuracy (%)
50

2000 3000
step

4000

60

0

1000

1000

0.04

400

network outputs on test example

0

0

0.06

200

5

0.0

effective step size /

600

0.0
1000

5000

top hessian eigenvalues

0.2

0

4000

empirical variance
along (t) eigenvectors
(t) eigenvalues

1.5

1
0

1e 6

2.0

2

gradient norm2
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

0.6

real vs. predicted oscillation covariance

top effective hessian eigenvalues

4

5000

0

2000

4000
step

6000

8000

1
0

0

1000

2000 3000
step

4000

5000

Figure 53.1: Scalar RMSProp central flow for a CNN with CE loss, η = 0.003, β2 = 0.99, and bias correction.

train loss
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

1.0
0.5
0.0

1e 5

Scalar RMSProp
central flow
stable flow (top 1)

3
2
1

0

1000

2000 3000
step

4000

5000

0

0

1000

gradient norm2

2.5
1.5

2000 3000
step

Scalar RMSProp
central flow
stable flow (top 1)

200
2000 3000
step

4000

5000

0

0

1000

network outputs on test example
Scalar RMSProp
central flow

5
0
5

2000 3000
step

0

1000

2000 3000
step

4000

5000

5000

Scalar RMSProp
central flow

0

test accuracy (%)

1000 2000 3000 4000 5000
step

distance to Scalar RMSProp
4

50

3

central flow
stable flow

2
Scalar RMSProp (midpoints)
central flow

10
1000 2000 3000 4000 5000
step

4000

0.125
0.100
0.075
0.050
0.025
0.000

60

40
0

empirical variance
along (t) eigenvectors
(t) eigenvalues

effective step size /

400

0.5
1000

5000

600

1.0

0

4000

1.0
0.8
0.6
0.4
0.2
0.0

top hessian eigenvalues

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

2.0

0.0

real vs. predicted oscillation covariance

top effective hessian eigenvalues

4

0

2000

4000
step

6000

8000

1
0

0

1000

2000 3000
step

4000

5000

Figure 53.2: Scalar RMSProp central flow for a CNN with CE loss, η = 0.006, β2 = 0.99, and bias correction.

139

train loss
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

1.5
1.0
0.5
0.0

1e 5

Scalar RMSProp
central flow
stable flow (top 1)

3
2

0

1000

2000 3000
step

4000

5000

0

2

1
0

1000

2000 3000
step

1000

2000 3000
step

4000

5000

Scalar RMSProp
central flow

5
5

400

0.2
Scalar RMSProp
central flow
stable flow (top 1)

0

0

1000

2000 3000
step

4000

5000

1000

2000 3000
step

0

1000

4000

0

2000

4000
step

6000

4000

5000

0.1
0.0

0

1000

2000 3000
step

4000

5000

distance to Scalar RMSProp

Scalar RMSProp (midpoints)
central flow

5000

2000 3000
step

Scalar RMSProp
central flow

test accuracy (%)
60

40

10
0

0

effective step size /

50

0

15

5000

0.3

network outputs on test example
10

4000

600

200
0

3

top hessian eigenvalues

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

4

empirical variance
along (t) eigenvectors
(t) eigenvalues

4
2

1

gradient norm2

0

real vs. predicted oscillation covariance

top effective hessian eigenvalues

4

8000

5
4
3
2
1
0

central flow
stable flow

0

1000

2000 3000
step

4000

5000

Figure 53.3: Scalar RMSProp central flow for a CNN with CE loss, η = 0.01, β2 = 0.99, and bias correction.

train loss
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

1.2
1.0
0.8
0.6
0.4

0

500

1000 1500 2000 2500 3000
step

Scalar RMSProp
central flow
stable flow (top 1)

3

5
4
3
2
1
0

1

2

0

0

500

1000 1500 2000 2500 3000
step

500

1000 1500 2000 2500 3000
step

network outputs on test example
Scalar RMSProp
central flow

1

500

400

0.03
Scalar RMSProp
central flow
stable flow (top 1)

0

0

500

0.02
0.01

1000 1500 2000 2500 3000
step

65
55

Scalar RMSProp
central flow

0

test accuracy (%)

70

1000 1500 2000 2500 3000
step

effective step size /
0.04

1

500 1000 1500 2000 2500 3000
step

distance to Scalar RMSProp
central flow
stable flow

1.5
1.0

50

Scalar RMSProp (midpoints)
central flow

45
500 1000 1500 2000 2500 3000
step

0

top hessian eigenvalues

60

0

0

600

0

2

empirical variance
along (t) eigenvectors
(t) eigenvalues

6
4

200
0

1e 5

2

gradient norm2
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

real vs. predicted oscillation covariance

top effective hessian eigenvalues

4

0

500 1000 1500 2000 2500 3000
step

0.5
0.0

0

500

1000 1500 2000 2500 3000
step

Figure 53.4: Scalar RMSProp central flow for a ResNet with CE loss, η = 0.01, β2 = 0.99, and bias correction.

140

train loss
1.2
1.0
0.8
0.6
0.4
0.2

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

0

500 1000 1500 2000 2500 3000
step

top effective hessian eigenvalues

4

Scalar RMSProp
central flow
stable flow (top 1)

3

0.00010

1

0.00005

0

0

500 1000 1500 2000 2500 3000
step

6
5
4
3
2
1
0

500 1000 1500 2000 2500 3000
step

network outputs on test example

3
2
1
0
1
2
3

Scalar RMSProp
central flow

500 1000 1500 2000 2500 3000
step

effective step size /
0.08

200

0.06
Scalar RMSProp
central flow
stable flow (top 1)

0

0

test accuracy (%)

70
65

Scalar RMSProp (midpoints)
central flow

50
45

Scalar RMSProp
central flow

0.04
0.02

500 1000 1500 2000 2500 3000
step

55

500 1000 1500 2000 2500 3000
step

0

top hessian eigenvalues

60

0

0.00000

300

100
0

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.00015

2

gradient norm2
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

real vs. predicted oscillation covariance

0.00020

0

500 1000 1500 2000 2500 3000
step

0
1.50
1.25
1.00
0.75
0.50
0.25
0.00

500 1000 1500 2000 2500 3000
step

distance to Scalar RMSProp
central flow
stable flow

0

500 1000 1500 2000 2500 3000
step

Figure 53.5: Scalar RMSProp central flow for a ResNet with CE loss, η = 0.02, β2 = 0.99, and bias correction.
train loss
1.2
1.0
0.8
0.6
0.4
0.2
0.0

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

0

500 1000 1500 2000 2500 3000
step

top effective hessian eigenvalues

4

Scalar RMSProp
central flow
stable flow (top 1)

3

0.0002

1

0.0001

0

0

500 1000 1500 2000 2500 3000
step

5
4
3
2
1
0

4

150
Scalar RMSProp
central flow
stable flow (top 1)

50
500 1000 1500 2000 2500 3000
step

0

0

2

65
60

2

55

4

50
500 1000 1500 2000 2500 3000
step

500 1000 1500 2000 2500 3000
step

effective step size /
0.150
0.125
0.100
0.075
0.050
0.025

Scalar RMSProp
central flow

0

test accuracy (%)
70

0

0

0

500 1000 1500 2000 2500 3000
step

network outputs on test example
Scalar RMSProp
central flow

0.0000

top hessian eigenvalues
200

100

0

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.0003

2

gradient norm2
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

real vs. predicted oscillation covariance

0.0004

Scalar RMSProp (midpoints)
central flow
0

500 1000 1500 2000 2500 3000
step

500 1000 1500 2000 2500 3000
step

distance to Scalar RMSProp
1.2
1.0
0.8
0.6
0.4
0.2
0.0

central flow
stable flow

0

500 1000 1500 2000 2500 3000
step

Figure 53.6: Scalar RMSProp central flow for a ResNet with CE loss, η = 0.03, β2 = 0.99, and bias correction.

141

train loss

1.4

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

1.2
1.0
0.8

0

1000

3
2

2000
step

3000

4000

0

150

0.015

0.005
0

1000

2000
step

3000

4000

0.000

0

1000

2000
step

3000

4000

effective step size /

top hessian eigenvalues
4000

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

200

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.020

0.010

gradient norm2

250

Scalar RMSProp
central flow

0.06

3000

0.04

2000

100

Scalar RMSProp
central flow
stable flow (top 1)

1000

50
0

real vs. predicted oscillation covariance

Scalar RMSProp
central flow
stable flow (top 1)

1

0.6
0.4

top effective hessian eigenvalues

4

0

1000

2000
step

3000

4000

network outputs on test example

3
2
1
0
1
2
3

Scalar RMSProp
central flow

0

0

1000

2000
step

3000

4000

test accuracy (%)

65
60

Scalar RMSProp (midpoints)
central flow

40
3000

1000

2000
step

3000

4000

distance to Scalar RMSProp
central flow
stable flow

0.4

45
2000
step

0

0.6

50

1000

0.00

0.8

55

0

0.02

4000

0

1000

2000
step

3000

4000

0.2
0.0

0

1000

2000
step

3000

4000

Figure 53.7: Scalar RMSProp central flow for a ViT with CE loss, η = 0.01, β2 = 0.99, and bias correction.
train loss

1.4
1.2
1.0
0.8
0.6
0.4
0.2

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

top effective hessian eigenvalues

4
3
2
1

0

1000

2000
step

3000

4000

0

0

1000

gradient norm2
125
100
75
50
25
0

real vs. predicted oscillation covariance

Scalar RMSProp
central flow
stable flow (top 1)

1000

2000
step

3000

4000

network outputs on test example
Scalar RMSProp
central flow

4
2

1500
1250
1000
750
500
250
0

2

50

4

45
2000
step

3000

4000

0

1000

2000
step

3000

4000

empirical variance
along (t) eigenvectors
(t) eigenvalues

0

1000

2000
step

3000

4000

effective step size /

0.12
0.10
0.08
0.06
0.04
0.02
0.00

Scalar RMSProp
central flow

0

test accuracy (%)

1000

2000
step

3000

4000

distance to Scalar RMSProp
central flow
stable flow

0.6

60
55

1000

4000

Scalar RMSProp
central flow
stable flow (top 1)

65

0

0

3000

top hessian eigenvalues

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

0

2000
step

0.030
0.025
0.020
0.015
0.010
0.005
0.000

0.4
Scalar RMSProp (midpoints)
central flow
0

1000

2000
step

3000

4000

0.2
0.0

0

1000

2000
step

3000

4000

Figure 53.8: Scalar RMSProp central flow for a ViT with CE loss, η = 0.02, β2 = 0.99, and bias correction.

142

train loss
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

2.0
1.5
1.0

0

1000

2000
step

3000

4000

gradient norm2
80

3

40

0

0.01
0

1000

2000
step

1000

2000
step

3000

4000

Scalar RMSProp
central flow

0

2000
step

3000

1000

2000
step

3000

4000

effective step size /

Scalar RMSProp
central flow
stable flow (top 1)
0

1000

2000
step

3000

4000

test accuracy (%)

65

Scalar RMSProp
central flow

0.05
0.00

60
55

0.4

4000

Scalar RMSProp (midpoints)
central flow
0

1000

2000
step

3000

0

4000

1000

2000
step

3000

4000

distance to Scalar RMSProp

0.8
0.6

45
1000

0

0.10

50
0

0.00

0.15

400

network outputs on test example

6
4
2
0
2
4
6

4000

top hessian eigenvalues

200
0

3000

600

20

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.03
0.02

2

800

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

60

0

real vs. predicted oscillation covariance

Scalar RMSProp
central flow
stable flow (top 1)

1

0.5
0.0

top effective hessian eigenvalues

4

central flow
stable flow

0.2
0.0

0

1000

2000
step

3000

4000

Figure 53.9: Scalar RMSProp central flow for a ViT with CE loss, η = 0.03, β2 = 0.99, and bias correction.

train loss
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

1.25
1.00
0.75
0.50
0.25
0

2000

4000
step

6000

8000

3

4

1

2

0

15

2000

250
4000
step

6000

8000

network outputs on test example

0

8000

Scalar RMSProp
central flow

2

0

0

0

2000

4000
step

6000

8000

effective step size /

0.4

Scalar RMSProp
central flow

0.3

750

5
2000

6000

1000
500

0

4000
step

1250

10
0

0

empirical variance
along (t) eigenvectors
(t) eigenvalues

6

2

top hessian eigenvalues

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

20

1e 5

Scalar RMSProp
central flow
stable flow (top 1)

gradient norm2

25

real vs. predicted oscillation covariance

top effective hessian eigenvalues

4

0.2
Scalar RMSProp
central flow
stable flow (top 1)
0

2000

4000
step

6000

8000

test accuracy (%)

100

0.1
0.0

0

4000
step

6000

8000

distance to Scalar RMSProp

1.5

80

2000

central flow
stable flow

1.0

60
4
6

Scalar RMSProp (midpoints)
central flow

40
0

2000

4000
step

6000

8000

0

2000

4000
step

6000

8000

0.5
0.0

0

2000

4000
step

6000

8000

Figure 53.10: Scalar RMSProp central flow for a LSTM with CE loss, η = 0.01, β2 = 0.99, and bias correction.

143

train loss
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

1.25
1.00
0.75
0.50

Scalar RMSProp
central flow
stable flow (top 1)

3
2

0

2000

4000
step

6000

8000

0

20

0

2000

15
10
5
0

0

2000

4000
step

6000

8000

network outputs on test example

0
1
2
3
4
5

Scalar RMSProp
central flow

0

2000

4000
step

6000

4000
step

6000

8000

top hessian eigenvalues
600
500
400
300
200
100
0

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.0004
0.0003
0.0001

gradient norm2
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

real vs. predicted oscillation covariance

0.0002

1

0.25
0.00

top effective hessian eigenvalues

4

0.0000

0

2000

4000
step

6000

8000

effective step size /

0.8

Scalar RMSProp
central flow

0.6
0.4
Scalar RMSProp
central flow
stable flow (top 1)
0

2000

4000
step

6000

8000

0.2
0.0

0

test accuracy (%)

100
90
80
70
60
50
40

2000

4000
step

6000

8000

distance to Scalar RMSProp
central flow
stable flow

1.25
1.00
0.75
0.50
Scalar RMSProp (midpoints)
central flow

8000

0

2000

4000
step

6000

8000

0.25
0.00

0

2000

4000
step

6000

8000

Figure 53.11: Scalar RMSProp central flow for a LSTM with CE loss, η = 0.02, β2 = 0.99, and bias correction.
train loss
1.25
1.00
0.75
0.50
0.25
0.00

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

top effective hessian eigenvalues

4

Scalar RMSProp
central flow
stable flow (top 1)

3

0.0004

1
0

2000

4000
step

6000

8000

0.0002
0

2000

gradient norm2
15.0
12.5
10.0
7.5
5.0
2.5
0.0

0

0

2000

4000
step

6000

8000

Scalar RMSProp
central flow

2
3
4
2000

4000
step

6000

8000

200

network outputs on test example

0

6000

300

Scalar RMSProp
central flow
stable flow (top 1)

100

1

5

4000
step

top hessian eigenvalues

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

8000

0

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.0008
0.0006

2

0

real vs. predicted oscillation covariance

0.0010

0

2000

4000
step

6000

8000

0.0000

0

2000

4000
step

6000

6000

8000

Scalar RMSProp
central flow

0

2000

4000
step

6000

8000

distance to Scalar RMSProp

Scalar RMSProp (midpoints)
central flow
0

4000
step

effective step size /

1.2
1.0
0.8
0.6
0.4
0.2
0.0

test accuracy (%)

100
90
80
70
60
50

2000

8000

1.2
1.0
0.8
0.6
0.4
0.2
0.0

central flow
stable flow

0

2000

4000
step

6000

8000

Figure 53.12: Scalar RMSProp central flow for a LSTM with CE loss, η = 0.03, β2 = 0.99, and bias correction.

144

train loss

2.5

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

2.0
1.5
1.0
0.0

Scalar RMSProp
central flow
stable flow (top 1)

3
2

0

1000 2000 3000 4000 5000 6000
step

0

0

1000 2000 3000 4000 5000 6000
step

gradient norm2
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

6
4
2
0

1000 2000 3000 4000 5000 6000
step

network outputs on test example

0
1
2
3
4
5

Scalar RMSProp
central flow

2

0

0

1000 2000 3000 4000 5000 6000
step

effective step size /
0.075

400

0

Scalar RMSProp
central flow

0.100

600

Scalar RMSProp
central flow
stable flow (top 1)
0

1000 2000 3000 4000 5000 6000
step

test accuracy (%)

100

0.050
0.025
0.000

0

1000 2000 3000 4000 5000 6000
step

distance to Scalar RMSProp

6

80

central flow
stable flow

4

60
40

Scalar RMSProp (midpoints)
central flow

20
0

empirical variance
along (t) eigenvectors
(t) eigenvalues

top hessian eigenvalues

200
0

1e 5

3

1

1

0.5

real vs. predicted oscillation covariance

top effective hessian eigenvalues

4

1000 2000 3000 4000 5000 6000
step

0

2000

4000
step

6000

8000

2
0

0

1000 2000 3000 4000 5000 6000
step

Figure 53.13: Scalar RMSProp central flow for a Transformer with CE loss, η = 0.01, β2 = 0.99, and bias correction.
train loss

2.5

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

2.0
1.5
1.0
0.0

0

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

6
4

0

0
1
2
3
4
5
6

0.000025
0

1000 2000 3000 4000 5000 6000
step

top hessian eigenvalues

0

1000 2000 3000 4000 5000 6000
step

network outputs on test example
Scalar RMSProp
central flow

0

1000 2000 3000 4000 5000 6000
step

effective step size /
Scalar RMSProp
central flow

0.05
0.00

0

test accuracy (%)

100

1000 2000 3000 4000 5000 6000
step

distance to Scalar RMSProp
central flow
stable flow

5
4

80

3
2
Scalar RMSProp (midpoints)
central flow

40
1000 2000 3000 4000 5000 6000
step

1000 2000 3000 4000 5000 6000
step

0.10
Scalar RMSProp
central flow
stable flow (top 1)

60

0

0

0.15

200

0

0.000000

0.20

300

100

2

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.000100
0.000050

gradient norm2

8

real vs. predicted oscillation covariance
0.000125
0.000075

2
1

1000 2000 3000 4000 5000 6000
step

Scalar RMSProp
central flow
stable flow (top 1)

3

0.5
0

top effective hessian eigenvalues

4

0

2000

4000
step

6000

8000

1
0

0

1000 2000 3000 4000 5000 6000
step

Figure 53.14: Scalar RMSProp central flow for a Transformer with CE loss, η = 0.02, β2 = 0.99, and bias correction.

145

3.0
2.5
2.0
1.5
1.0
0.5
0.0

train loss

Scalar RMSProp
central flow
stable flow (top 1)

3
2
1

0

1000 2000 3000 4000 5000 6000
step

gradient norm2

8

4

0

0

1000 2000 3000 4000 5000 6000
step

Scalar RMSProp
central flow
stable flow (top 1)

50

network outputs on test example

0

0

1000 2000 3000 4000 5000 6000
step

1000 2000 3000 4000 5000 6000
step
Scalar RMSProp
central flow

20
0

0

test accuracy (%)

100

Scalar RMSProp
central flow

2

0

0

40

150

1000 2000 3000 4000 5000 6000
step

empirical variance
along (t) eigenvectors
(t) eigenvalues

60

100

0

real vs. predicted oscillation covariance

effective step size /

200

2

0.00030
0.00025
0.00020
0.00015
0.00010
0.00005
0.00000

top hessian eigenvalues

250

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

6

0

top effective hessian eigenvalues

4

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

1000 2000 3000 4000 5000 6000
step

distance to Scalar RMSProp
central flow
stable flow

2.5
2.0

80

1.5

4

60

6

1.0
Scalar RMSProp (midpoints)
central flow

40
0

1000 2000 3000 4000 5000 6000
step

0

2000

4000
step

6000

0.5
0.0

0

1000 2000 3000 4000 5000 6000
step

Figure 53.15: Scalar RMSProp central flow for a Transformer with CE loss, η = 0.03, β2 = 0.99, and bias correction.

train loss
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

1.2
1.0
0.8

0

1000

3
2

2000 3000
step

4000

5000

0

15

0

1000

2000 3000
step

4000

5000

network outputs on test example

2

Scalar RMSProp
central flow

0

2000 3000
step

1000

0

0

1000

2000 3000
step

4000

4000

5000

4000

5000

Scalar RMSProp
central flow

0.005

5000

test accuracy (%)

68

0

0.20

64

0.15

1000

2000 3000
step

4000

5000

distance to Scalar RMSProp

0.25

66

58

2000 3000
step

0.010
Scalar RMSProp
central flow
stable flow (top 1)

central flow
stable flow

0.10
Scalar RMSProp (midpoints)
central flow

60
1000

0

effective step size /

62

2
0

0

0.015

500
2000 3000
step

5000

1000

5
1000

4000

1500

10

0

2

top hessian eigenvalues

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

20

empirical variance
along (t) eigenvectors
(t) eigenvalues

3

1

gradient norm2

25

4

1e 5

Scalar RMSProp
central flow
stable flow (top 1)

1

0.6

0

real vs. predicted oscillation covariance

top effective hessian eigenvalues

4

0

2000

4000
step

6000

8000

0.05
0.00

0

1000

2000 3000
step

4000

5000

Figure 53.16: Scalar RMSProp central flow for a Mamba with CE loss, η = 0.007, β2 = 0.99, and bias correction.

146

train loss
1.2

Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

1.0
0.8

real vs. predicted oscillation covariance

top effective hessian eigenvalues

4

Scalar RMSProp
central flow
stable flow (top 1)

3

0

1000

2000 3000
step

4000

5000

20
15

0

0

1000

250

0

0

2000 3000
step

4000

5000

Scalar RMSProp
central flow

Scalar RMSProp
central flow
stable flow (top 1)

2000 3000
step

4000

5000

2000 3000
step

4000

5000

effective step size /
Scalar RMSProp
central flow

0

1000

2000 3000
step

4000

0.005

5000

0

64

0.15

58

1000

2000 3000
step

4000

5000

distance to Scalar RMSProp

0.25
0.20

60

1000

0.010

66
62

0

0.015

test accuracy (%)

4
1000

0

0.025

68

2

0

4

0.020

network outputs on test example

0

5000

750

5

2

4000

1000
500

1000

2000 3000
step

top hessian eigenvalues

10

0

empirical variance
along (t) eigenvectors
(t) eigenvalues

2

gradient norm2
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

1e 5

2
1

0.6

6

central flow
stable flow

0.10
Scalar RMSProp (midpoints)
central flow
0

2000

4000
step

6000

8000

0.05
0.00

0

1000

2000 3000
step

4000

5000

Figure 53.17: Scalar RMSProp central flow for a Mamba with CE loss, η = 0.01, β2 = 0.99, and bias correction.
train loss
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction
stable flow

1.0
0.8
0.6
0.4
0

1000

2000 3000
step

4000

5000

top effective hessian eigenvalues

4

Scalar RMSProp
central flow
stable flow (top 1)

3

0.00010

1

0.00005

0

0

1000

15
10

2

2000 3000
step

4000

network outputs on test example
Scalar RMSProp
central flow

0
2
4
0

1000 2000 3000 4000 5000
step

70
68
66
64
62
60
58

1000

2000 3000
step

4000

5000

effective step size /
Scalar RMSProp
central flow

0.03

300

5000

0

0.05

400

0

0.00000

0.04

Scalar RMSProp
central flow
stable flow (top 1)

100
1000

5000

top hessian eigenvalues

200

0

4000

500

5
0

2000 3000
step

empirical variance
along (t) eigenvectors
(t) eigenvalues

0.00015

2

gradient norm2
Scalar RMSProp
Scalar RMSProp (average)
central flow
central flow prediction

real vs. predicted oscillation covariance

0.00020

0

1000

2000 3000
step

4000

0.02
0.01

5000

test accuracy (%)

0

1000 2000 3000 4000 5000
step

distance to Scalar RMSProp

0.5

central flow
stable flow

0.4
0.3
0.2
Scalar RMSProp (midpoints)
central flow
0

1000 2000 3000 4000 5000
step

0.1
0.0

0

1000

2000 3000
step

4000

5000

Figure 53.18: Scalar RMSProp central flow for a Mamba with CE loss, η = 0.02, β2 = 0.99, and bias correction.

147

E.3

RMSProp
train loss

0.5

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

0.4
0.3
0.2

top effective hessian eigenvalues

4

RMSProp
central flow
stable flow (top 1)

3
2

0

500 1000 1500 2000 2500 3000

0

8
6

0

500 1000 1500 2000 2500 3000

top hessian eigenvalues

2

200

0

0

500 1000 1500 2000 2500 3000

0.4
0.3
0.2

0.0

500 1000 1500 2000 2500 3000

test accuracy (%)

70
65

RMSProp (midpoints)
central flow

45
0

500 1000 1500 2000 2500 3000

0

coordinates of
10 5

500 1000 1500 2000 2500 3000

distance to RMSProp
RMSProp
central flow

central flow
stable flow

4

10 8

3

10 11

2
1

10 17
0

500 1000 1500 2000 2500 3000

50

10 14

0.1

0

55

network outputs on test example
RMSProp
central flow

0.00000

60

600
400

0

RMSProp
central flow
stable flow (top 1)

800

4

0.00020

0.00005

1000

RMSProp
RMSProp (average)
central flow
central flow prediction

empirical variance
along each mode
predicted variances

0.00025

0.00010

gradient norm2

10

real vs. predicted oscillation covariance

0.00015

1

0.1

0.00030

0

500 1000 1500 2000 2500 3000

0

0

500 1000 1500 2000 2500 3000

Figure 54: Annotated example of a RMSProp experiment. Using RMSProp with η = 2 × 10−5 , β2 = 0.99,
ϵ = 10−8 and bias correction, we train a ResNet on a subset of CIFAR-10 with MSE loss. The central flow (black)
accurately models the long-term trajectory of RMSProp (blue), whereas the stable flow (red) takes a different path.
As described in Section B.1, we terminate the stable flow once the effective sharpness gets sufficiently large.
Top left: See Figure 48 caption. The central flow’s prediction for the time-averaged
i is given by eq. (124).
h loss
η
Top center: We plot the top several eigenvalues of the effective Hessian diag √ν H(w) under both RMSProp
(colors) and its central flow (dashed black). Under RMSProp, these eigenvalues equilibrate around the critical
threshold 2, whereas under the central flow they are fixed exactly at 2. We also plot the top eigenvalue under the
“stable flow” baseline (red), which increases far above 2.
Top right: We show that the central flow accurately predicts the covariance of the oscillations. In particular, we
show that each nonzero eigenvalue λi (t) of P (t)1/2 Σ(t) P (t)1/2 accurately predicts the P -whitened variance of
oscillations along the corresponding eigenvector vi (t), as we expect from eq. (126). In black, we plot the nonzero
eigenvalues of P (t)1/2 Σ(t) P (t)1/2 . In faint colors, we plot the squared magnitude of the P -whitened displacement
between RMSProp and the central flow along each eigenvector vi (t) (see eq. (126)), and in thick colors, we plot the
time-averages of these displacements, i.e. the empirical variances of the oscillations. Observe that each eigenvalue
accurately predicts the variance of the oscillations along the corresponding eigenvector.
Middle left: See Figure 48 caption. The central flow’s prediction for the time-average is given by eq. (125).
Middle center: See Figure 51 caption. The central flow’s prediction for the time-averaged loss is given by eq. (124).
Middle right: See Figure 48 caption.
Bottom left: See Figure 51 caption.
Bottom center: We plot four arbitrary coordinates of the EMA ν under both RMSProp (colors) and the central flow
(dashed black). The coordinates of ν are oscillatory along the RMSProp trajectory, while varying smoothly along the
central flow.
Bottom right: See Figure 48 caption.

148

train loss

0.5

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

0.4
0.3
0.2
0.1
0

500 1000 1500 2000 2500 3000

top effective hessian eigenvalues

4

RMSProp
central flow
stable flow (top 1)

3

0.0002

1

0.0001

0

0

500 1000 1500 2000 2500 3000

0.6
0.4
0.2
0.0
0

500 1000 1500 2000 2500 3000

network outputs on test example
RMSProp
central flow

0.4

empirical variance
along each mode
predicted variances

0.0003

2

gradient norm2
RMSProp
RMSProp (average)
central flow
central flow prediction

real vs. predicted oscillation covariance

0.0004

0.0000

0

500 1000 1500 2000 2500 3000

top hessian eigenvalues
600
500
400
300
200
100
0

RMSProp
central flow
stable flow (top 1)

test accuracy (%)
60
50
40

0

RMSProp (midpoints)
central flow

500 1000 1500 2000 2500 3000

0

coordinates of

10 5

500 1000 1500 2000 2500 3000

distance to RMSProp
RMSProp
central flow

10 7

central flow
stable flow

2.5
2.0
1.5

0.2

10 9

0.0

1.0
0.5

10 11

0.2

0

500 1000 1500 2000 2500 3000

0

500 1000 1500 2000 2500 3000

0.0

0

500 1000 1500 2000 2500 3000

Figure 55.1: RMSProp central flow for a CNN with MSE loss, η = 1e-05, β2 = 0.99, ϵ = 1e-08, and bias correction.
train loss
RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

0.4
0.3
0.2

0.0

0

RMSProp
RMSProp (average)
central flow
central flow prediction

1.5

0

500 1000 1500 2000 2500 3000

0.6

0

500 1000 1500 2000 2500 3000

RMSProp
central flow
stable flow (top 1)

300

test accuracy (%)
60
50

200
40

100
0

500 1000 1500 2000 2500 3000

0

0

RMSProp
central flow

coordinates of

0.2
0.0
0.2
0

500 1000 1500 2000 2500 3000

10 5
10 6
10 7
10 8
10 9
10 10
10 11

RMSProp (midpoints)
central flow

500 1000 1500 2000 2500 3000

network outputs on test example

0.4

0.4

0.00000

top hessian eigenvalues
400

0.5
0.0

0.00100

0.00025

500

1.0

empirical variance
along each mode
predicted variances

0.00125

0.00050

gradient norm2
2.0

real vs. predicted oscillation covariance

0.00075

2
1

500 1000 1500 2000 2500 3000

RMSProp
central flow
stable flow (top 1)

3

0.1
0

top effective hessian eigenvalues

4

RMSProp
central flow

0

500 1000 1500 2000 2500 3000

distance to RMSProp

2.5

central flow
stable flow

2.0
1.5
1.0
0.5

0

500 1000 1500 2000 2500 3000

0.0

0

500 1000 1500 2000 2500 3000

Figure 55.2: RMSProp central flow for a CNN with MSE loss, η = 2e-05, β2 = 0.99, ϵ = 1e-08, and bias correction.

149

train loss
0.4
0.3
0.2

0

0

500 1000 1500 2000 2500 3000

1.5

0.0005
0

500 1000 1500 2000 2500 3000
RMSProp
central flow
stable flow (top 1)

200
150

0

500 1000 1500 2000 2500 3000

network outputs on test example
RMSProp
central flow

0.6
0.4
0.2
0.0
0.2
0.4
0

500 1000 1500 2000 2500 3000

test accuracy (%)
60

RMSProp (midpoints)
central flow

40

50
0

0

50

100

0.5

0.0000

top hessian eigenvalues
250

1.0

empirical variance
along each mode
predicted variances

0.0020

0.0010

gradient norm2
RMSProp
RMSProp (average)
central flow
central flow prediction

real vs. predicted oscillation covariance

0.0015

2
1

2.0

0.0

RMSProp
central flow
stable flow (top 1)

3

0.1
0.0

top effective hessian eigenvalues

4

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

0

500 1000 1500 2000 2500 3000

coordinates of

10 4
10 5
10 6
10 7
10 8
10 9
10 10

500 1000 1500 2000 2500 3000

RMSProp
central flow

0

500 1000 1500 2000 2500 3000

distance to RMSProp

2.5

central flow
stable flow

2.0
1.5
1.0
0.5

0

500 1000 1500 2000 2500 3000

0.0

0

500 1000 1500 2000 2500 3000

Figure 55.3: RMSProp central flow for a CNN with MSE loss, η = 4e-05, β2 = 0.99, ϵ = 1e-08, and bias correction.
train loss

0.5

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

0.4
0.3
0.2

RMSProp
central flow
stable flow (top 1)

3
2

0

500 1000 1500 2000 2500 3000

0

8
6

0

500 1000 1500 2000 2500 3000

top hessian eigenvalues

2

200
500 1000 1500 2000 2500 3000

0

0.4
0.3
0.2

0.0

500 1000 1500 2000 2500 3000

500 1000 1500 2000 2500 3000

test accuracy (%)

70
65

RMSProp (midpoints)
central flow

45
0

500 1000 1500 2000 2500 3000

0

coordinates of
10 5

500 1000 1500 2000 2500 3000

distance to RMSProp
RMSProp
central flow

central flow
stable flow

4

10 8

3

10 11

2
1

10 17
0

0

50

10 14

0.1

0.00000

55

network outputs on test example
RMSProp
central flow

0.00020

60

600
400

0

RMSProp
central flow
stable flow (top 1)

800

4

empirical variance
along each mode
predicted variances

0.00025

0.00005

1000

RMSProp
RMSProp (average)
central flow
central flow prediction

real vs. predicted oscillation covariance

0.00010

gradient norm2

10

0.00030

0.00015

1

0.1

0

top effective hessian eigenvalues

4

0

500 1000 1500 2000 2500 3000

0

0

500 1000 1500 2000 2500 3000

Figure 55.4: RMSProp central flow for a ResNet with MSE loss, η = 2e-05, β2 = 0.99, ϵ = 1e-08, and bias correction.

150

train loss
RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

0.4
0.3
0.2

top effective hessian eigenvalues

4

RMSProp
central flow
stable flow (top 1)

3
2

0

500 1000 1500 2000 2500 3000

0

6

0.00100

0.00025
0

500 1000 1500 2000 2500 3000

0.00000

RMSProp
central flow
stable flow (top 1)

800
600

test accuracy (%)
65
60

400

55

2

200

50

0

0

500 1000 1500 2000 2500 3000
RMSProp
central flow

0.4

0

500 1000 1500 2000 2500 3000

coordinates of
RMSProp
central flow

10 3
10 6

0

4

0.2

10 12

2

10 15

1

0

central flow
stable flow

5

10 9

0.0

500 1000 1500 2000 2500 3000

distance to RMSProp

6

0.3
0.1

RMSProp (midpoints)
central flow

45

network outputs on test example
0.5

500 1000 1500 2000 2500 3000

70

4

0

0

top hessian eigenvalues

RMSProp
RMSProp (average)
central flow
central flow prediction

8

empirical variance
along each mode
predicted variances

0.00125

0.00050

gradient norm2

10

real vs. predicted oscillation covariance

0.00075

1

0.1

0.00150

3

500 1000 1500 2000 2500 3000

0

500 1000 1500 2000 2500 3000

0

0

500 1000 1500 2000 2500 3000

Figure 55.5: RMSProp central flow for a ResNet with MSE loss, η = 4e-05, β2 = 0.99, ϵ = 1e-08, and bias correction.
train loss
0.4

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

0.3
0.2
0.1
0.0

RMSProp
central flow
stable flow (top 1)

3

0

500 1000 1500 2000 2500 3000

0

0

500 1000 1500 2000 2500 3000
RMSProp
central flow
stable flow (top 1)

400
300

3
2
1
500 1000 1500 2000 2500 3000

network outputs on test example
RMSProp
central flow

0.4
0.2
0.0
0

500 1000 1500 2000 2500 3000

500 1000 1500 2000 2500 3000

0

test accuracy (%)
70
65
55
50

100

0.6

0

60

200

0

0.000

top hessian eigenvalues

RMSProp
RMSProp (average)
central flow
central flow prediction

4

empirical variance
along each mode
predicted variances

0.003

0.001

gradient norm2

5

real vs. predicted oscillation covariance

0.002

2
1

6

0

top effective hessian eigenvalues

4

RMSProp (midpoints)
central flow

45
0

500 1000 1500 2000 2500 3000

coordinates of

10 2
10 4
10 6
10 8
10 10
10 12
10 14

RMSProp
central flow

0

500 1000 1500 2000 2500 3000

distance to RMSProp

8

central flow
stable flow

6
4
2

0

500 1000 1500 2000 2500 3000

0

0

500 1000 1500 2000 2500 3000

Figure 55.6: RMSProp central flow for a ResNet with MSE loss, η = 0.0001, β2 = 0.99, ϵ = 1e-08, and bias
correction.

151

train loss

0.8
0.6
0.4

2
1

0

1000

2000

3000

4000

gradient norm2
60
50
40
30
20
10
0

RMSProp
central flow
stable flow (top 1)

3

0.2
0.0

top effective hessian eigenvalues

4

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

1000

0

1000

2000

3000

4000

0.4
0.0
0.2

3000

1000

30
1000

2000

3000

0

0.6
0.4

10 9

0.2
2000

2000

3000

4000

3000

4000

central flow
stable flow

0.8

10 7

1000

1000

distance to RMSProp

10 5

0

4000

RMSProp (midpoints)
central flow

4000

RMSProp
central flow

4000

3000

50

500
0

2000

60

40

10 3

0.2

2000

0

test accuracy (%)

1000
0

empirical variance
along each mode
predicted variances

coordinates of

RMSProp
central flow

1000

4000

1500

0.6

0

3000

RMSProp
central flow
stable flow (top 1)

2000

network outputs on test example

0.4

2000

0.00150
0.00125
0.00100
0.00075
0.00050
0.00025
0.00000

top hessian eigenvalues

2500

RMSProp
RMSProp (average)
central flow
central flow prediction

0

0

real vs. predicted oscillation covariance

0.0

0

1000

2000

3000

4000

Figure 55.7: RMSProp central flow for a ViT with MSE loss, η = 7e-06, β2 = 0.95, ϵ = 1e-08, and bias correction.
train loss
RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

0.6
0.4
0.2
0.0

RMSProp
central flow
stable flow (top 1)

3
2

0

1000

2000

3000

4000

gradient norm2

30

0

1000

2000

3000

4000

RMSProp
central flow
stable flow (top 1)

200
2000

3000

4000

0

RMSProp
central flow

0

1000

2000

3000

3000

4000

4000

RMSProp (midpoints)
central flow
0

RMSProp
central flow

10 3

1000

2000

3000

4000

distance to RMSProp
central flow
stable flow

1.0
0.8
0.6
0.4

10 9
2000

3000

50

4000

10 7

1000

2000

60

coordinates of
10 5

0

1000

30

network outputs on test example
0.8
0.6
0.4
0.2
0.0
0.2

0

40

400

1000

0.0000

test accuracy (%)

600

0

0.0020

top hessian eigenvalues

800

10

empirical variance
along each mode
predicted variances

0.0025

0.0005

1000

20

real vs. predicted oscillation covariance

0.0010

1200

RMSProp
RMSProp (average)
central flow
central flow prediction

40

0

0.0030

0.0015

1

50

0

top effective hessian eigenvalues

4

0.2
0

1000

2000

3000

4000

0.0

0

1000

2000

3000

4000

Figure 55.8: RMSProp central flow for a ViT with MSE loss, η = 1e-05, β2 = 0.95, ϵ = 1e-08, and bias correction.

152

train loss
0.6
0.5
0.4
0.3
0.2
0.1
0.0

0

1000

2000

3000

top effective hessian eigenvalues

4

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

RMSProp
central flow
stable flow (top 1)

3

0.002

1

0.001
0

1000

gradient norm2

40

3000

4000

0.000

0

1000

top hessian eigenvalues

RMSProp
RMSProp (average)
central flow
central flow prediction

30

2000

empirical variance
along each mode
predicted variances

0.003

2

0

4000

real vs. predicted oscillation covariance
0.004

RMSProp
central flow
stable flow (top 1)

600

20

400

10

200

0

0

2000

3000

4000

test accuracy (%)
60
50
40

0

1000

2000

3000

4000

0

1000

RMSProp
central flow

0.75

2000

3000

4000

10 3

0.25

10 7

0.00

10 9

RMSProp
central flow

0.25
1000

2000

3000

4000

0

1000

2000

1000

2000

3000

4000

distance to RMSProp

10 5

0.50

0

0

coordinates of

network outputs on test example

1.00

RMSProp (midpoints)
central flow

30

3000

4000

1.2
1.0
0.8
0.6
0.4
0.2
0.0

central flow
stable flow

0

1000

2000

3000

4000

Figure 55.9: RMSProp central flow for a ViT with MSE loss, η = 2e-05, β2 = 0.95, ϵ = 1e-08, and bias correction.

train loss
RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

0.4
0.3
0.2

real vs. predicted oscillation covariance

top effective hessian eigenvalues

4

1e 6

RMSProp
central flow
stable flow (top 1)

3

0

1000

2000

3000

4000

0

2
1
0

1000

gradient norm2
0.2

2000

3000

4000

top hessian eigenvalues

RMSProp
RMSProp (average)
central flow
central flow prediction

0.3

4
3

2
1

0.1

RMSProp
central flow
stable flow (top 1)

200
150

0

60

0.0

50

40

1000

2000

3000

4000

0

0

1000

RMSProp
central flow

0.1

2000

3000

RMSProp
central flow

10 6
10 8

0.2

10 9

0.4

0.3

10 10

0.2

3000

4000

2000

4000

6000

8000

central flow
stable flow

1.0

0.1

2000

4000

distance to RMSProp

1.2

0.0

1000

3000

RMSProp (midpoints)
central flow
0

10 7

0

2000

test accuracy (%)

4000

coordinates of

network outputs on test example

1000

80

100

0

0

100

0.1

0.1

empirical variance
along each mode
predicted variances

5

0.8
0.6

0

1000

2000

3000

4000

0.0

0

1000

2000

3000

4000

Figure 55.10: RMSProp central flow for a LSTM with MSE loss, η = 1e-05, β2 = 0.99, ϵ = 1e-08, and bias
correction.

153

train loss
0.4
0.3

top effective hessian eigenvalues

4

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

RMSProp
central flow
stable flow (top 1)

3
2

0.2

0

1000

2000

3000

4000

0

0

1000

0.6
0.4
0.2
0.0
0

1000

2000

3000

4000

network outputs on test example

1500
1250
1000
750
500
250
0

10 7

0.1

10 8

0.2

10 9

0.3

10 10
1000

2000

4000

3000

0.00000

0

1000

2000

3000

4000

test accuracy (%)

100
80
60
40

0

1000

2000

3000

RMSProp (midpoints)
central flow

4000

0

coordinates of

1000

2000

3000

4000

distance to RMSProp
RMSProp
central flow

10 6

0.0

0

3000

RMSProp
central flow
stable flow (top 1)

10 5

RMSProp
central flow

0.1

2000

top hessian eigenvalues

RMSProp
RMSProp (average)
central flow
central flow prediction

0.00015

0.00005

gradient norm2

0.8

empirical variance
along each mode
predicted variances

0.00020

0.00010

1

0.1

real vs. predicted oscillation covariance

central flow
stable flow

1.5
1.0
0.5

4000

0

1000

2000

3000

4000

0.0

0

1000

2000

3000

4000

Figure 55.11: RMSProp central flow for a LSTM with MSE loss, η = 2e-05, β2 = 0.99, ϵ = 1e-08, and bias
correction.
train loss
RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

0.4
0.3
0.2

top effective hessian eigenvalues

4

RMSProp
central flow
stable flow (top 1)

3

0

1000

2000

3000

4000

0

0.0002
0

1000

4
3

1500
1000

1

500
0

1000

2000

3000

4000

network outputs on test example
RMSProp
central flow

0.1
0.0
0.1
0.2
0.3
0

1000

2000

3000

3000

4000

RMSProp
central flow
stable flow (top 1)

2000

2
0

2000

top hessian eigenvalues

RMSProp
RMSProp (average)
central flow
central flow prediction

4000

0

0.0008
0.0004

gradient norm2
5

empirical variance
along each mode
predicted variances

0.0006

2
1

0.1

real vs. predicted oscillation covariance
0.0010

0.0000

0

1000

2000

3000

4000

test accuracy (%)

100
80
60
40

0

1000

2000

3000

RMSProp (midpoints)
central flow

4000

0

coordinates of

10 4
10 5
10 6
10 7
10 8
10 9
10 10

2000

4000

6000

8000

distance to RMSProp
RMSProp
central flow

central flow
stable flow

1.5
1.0
0.5

0

1000

2000

3000

4000

0.0

0

1000

2000

3000

4000

Figure 55.12: RMSProp central flow for a LSTM with MSE loss, η = 4e-05, β2 = 0.99, ϵ = 1e-08, and bias
correction.

154

train loss

0.6
0.5
0.4

1

0.1

0

1000

1500

2000

2500

gradient norm2

6

0

500

1000

1500

2000

2500

RMSProp
central flow
stable flow (top 1)

1000

1500

2000

2500

0

500

0.1
0.2
0

500

1000

1500

10 5
10 6
10 7
10 8
10 9
10 10
10 11

0.0

2000

2500

RMSProp (midpoints)
central flow

2000

2500

0

coordinates of

RMSProp
central flow

1500

80

20

network outputs on test example
0.1

1000

40

50
500

500

60

100

0

0

test accuracy (%)

150

0

0.00000

top hessian eigenvalues

200

2
0

0.00002

250

4

empirical variance
along each mode
predicted variances

0.00008
0.00004

300

RMSProp
RMSProp (average)
central flow
central flow prediction

8

real vs. predicted oscillation covariance
0.00010
0.00006

2

0.2
500

RMSProp
central flow
stable flow (top 1)

3

0.3

0

top effective hessian eigenvalues

4

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

500

1000 1500 2000 2500

distance to RMSProp
RMSProp
central flow

central flow
stable flow

1.25
1.00
0.75
0.50
0.25

1000 1500 2000 2500

0

500

1000 1500 2000 2500

0.00

0

500

1000

1500

2000

2500

Figure 55.13: RMSProp central flow for a Transformer with MSE loss, η = 2e-05, β2 = 0.95, ϵ = 1e-08, and bias
correction.
train loss

0.6

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

0.5
0.4
0.3
0.2

RMSProp
central flow
stable flow (top 1)

3

0

500

1000

1500

2000

2500

0

0.0001
0

500

6

1500

2000

2500

RMSProp
central flow
stable flow (top 1)

300

0

500

1000

1500

2000

2500

network outputs on test example
RMSProp
central flow

0.1
0.0
0.1

0

0

500

0

500

1000 1500 2000 2500

1000

1500

2000

2500

test accuracy (%)

100
80

RMSProp (midpoints)
central flow

1000

1500

2000

2500

0

coordinates of

10 4

RMSProp
central flow

10 6

0

500

500

1000 1500 2000 2500

distance to RMSProp

10 10

0.3

500

20

10 8

0.2

0

40

100

2

0.0000

60

200

4

0

1000

top hessian eigenvalues

RMSProp
RMSProp (average)
central flow
central flow prediction

empirical variance
along each mode
predicted variances

0.0004

0.0002

gradient norm2
8

real vs. predicted oscillation covariance

0.0005

0.0003

2
1

0.1
0.0

top effective hessian eigenvalues

4

1000 1500 2000 2500

1.50
1.25
1.00
0.75
0.50
0.25
0.00

central flow
stable flow

0

500

1000

1500

2000

2500

Figure 55.14: RMSProp central flow for a Transformer with MSE loss, η = 4e-05, β2 = 0.95, ϵ = 1e-08, and bias
correction.

155

train loss
RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

0.5
0.4
0.3
0.2
0.0

0

1000

1500

2000

4

0

500

0

0

500

1000

1000

1500

2000

60
40

50

20
500

1000

0.1
0.2
0.3
1000

1500

2000

1500

2000

RMSProp (midpoints)
central flow
0

RMSProp
central flow

10 4
10 5
10 6
10 7
10 8
10 9
10 10

1000

test accuracy (%)

2000

coordinates of

0.0

500

1500

500

80

100

0

0

100

150

0

RMSProp
central flow

0

0.0000

2000

RMSProp
central flow
stable flow (top 1)

200

network outputs on test example
0.1

1500

250

2

empirical variance
along each mode
predicted variances

0.0020

0.0005

top hessian eigenvalues

RMSProp
RMSProp (average)
central flow
central flow prediction

real vs. predicted oscillation covariance

0.0010

gradient norm2
6

0.0025

0.0015

2

0.1
500

RMSProp
central flow
stable flow (top 1)

3

1
0

top effective hessian eigenvalues

4

500

1000

1500

2000

2500

distance to RMSProp

8

central flow
stable flow

6
4
2

2500

0

500

1000

1500

2000

2500

0

0

500

1000

1500

2000

Figure 55.15: RMSProp central flow for a Transformer with MSE loss, η = 0.0001, β2 = 0.95, ϵ = 1e-08, and bias
correction.
train loss

0.5

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

0.4
0.3

0.1

0

2000

3000

4000

5000

6

0

1000

2

2000
0

1000

2000

3000

4000

5000

RMSProp
central flow

0.4

0

5000

0

1000

10 8

0.1

10 10

0.0

10 12
1000 2000 3000 4000 5000

2000

3000

4000

1000

2000

3000

4000

5000

test accuracy (%)

70
60

RMSProp (midpoints)
central flow

5000

RMSProp
central flow

10 4
10 6

0

30

10 2

0.2

0.00000

40

coordinates of

0.3

0

4000

50

network outputs on test example
0.5

3000

RMSProp
central flow
stable flow (top 1)

6000
4000

0

2000

8000

4

0.00020

0.00005

top hessian eigenvalues

RMSProp
RMSProp (average)
central flow
central flow prediction

empirical variance
along each mode
predicted variances

0.00025

0.00010

gradient norm2
8

real vs. predicted oscillation covariance

0.00015

2
1

1000

RMSProp
central flow
stable flow (top 1)

3

0.2
0

top effective hessian eigenvalues

4

0

1000 2000 3000 4000 5000

distance to RMSProp

0.6

central flow
stable flow

0.5
0.4
0.3
0.2
0.1

0

1000 2000 3000 4000 5000

0.0

0

1000

2000

3000

4000

5000

Figure 55.16: RMSProp central flow for a Mamba with MSE loss, η = 1e-05, β2 = 0.99, ϵ = 1e-08, and bias
correction.

156

train loss

0.5

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

0.4
0.3
0.2

top effective hessian eigenvalues

4

RMSProp
central flow
stable flow (top 1)

3
2

0

1000

2000

3000

4000

5000

0

0

1000

2000

3000

4000

5000

RMSProp
central flow
stable flow (top 1)

5000
4000

3000

4000

5000

0

0.4
0.2

10 8

0

1000

2000

3000

4000

5000

0

1000 2000 3000 4000 5000

0

1000 2000 3000 4000 5000

distance to RMSProp
RMSProp
central flow

10 12
0

5000

RMSProp (midpoints)
central flow

45

10 10

0.0

4000

65

50

10 2
10 4
10 6

3000

70

coordinates of

RMSProp
central flow

2000

55

network outputs on test example

0.6

1000

60

3000
1000

2000

0

test accuracy (%)

2000

1000

0.0000

top hessian eigenvalues

RMSProp
RMSProp (average)
central flow
central flow prediction

0

0.0006

0.0002

gradient norm2
15.0
12.5
10.0
7.5
5.0
2.5
0.0

empirical variance
along each mode
predicted variances

0.0004

1

0.1

real vs. predicted oscillation covariance
0.0008

1000 2000 3000 4000 5000

0.6
0.5
0.4
0.3
0.2
0.1
0.0

central flow
stable flow

0

1000

2000

3000

4000

5000

Figure 55.17: RMSProp central flow for a Mamba with MSE loss, η = 2e-05, β2 = 0.99, ϵ = 1e-08, and bias
correction.
train loss

0.5

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

0.4
0.3
0.2
0.1
0

1000

2000

3000

4000

5000

gradient norm2
12
10
8
6
4
2
0

1000

2000

3000

3

1

0.0005

0

0

1000

2000

3000

4000

5000

5000

RMSProp
central flow

RMSProp
central flow
stable flow (top 1)

0

1000

2000

0.0
1000 2000 3000 4000 5000

4000

5000

RMSProp (midpoints)
central flow
0

1000 2000 3000 4000 5000

distance to RMSProp

0.8

central flow
stable flow

0.6
0.4
0.2

10 11
0

3000

70

5000

RMSProp
central flow

10 3

10 9

4000

coordinates of

10 1

10 7

3000

2000

80

50

0.2

1000

test accuracy (%)

500

10 5

0

top hessian eigenvalues

60

0.4

0.2

0.0000

1000
0

empirical variance
along each mode
predicted variances

0.0015
0.0010

1500

4000

real vs. predicted oscillation covariance

2

2000

network outputs on test example
0.6

RMSProp
central flow
stable flow (top 1)

2500

RMSProp
RMSProp (average)
central flow
central flow prediction

0

top effective hessian eigenvalues

4

0

1000 2000 3000 4000 5000

0.0

0

1000

2000

3000

4000

5000

Figure 55.18: RMSProp central flow for a Mamba with MSE loss, η = 4e-05, β2 = 0.99, ϵ = 1e-08, and bias
correction.

157

train loss
RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

1.25
1.00
0.75
0.50

RMSProp
central flow
stable flow (top 1)

3

0

500 1000 1500 2000 2500 3000

0

0.05

1
0

500 1000 1500 2000 2500 3000
RMSProp
central flow
stable flow (top 1)

250
200
100

0.05

50
0

500 1000 1500 2000 2500 3000

network outputs on test example

4

RMSProp
central flow

2
0

0

500 1000 1500 2000 2500 3000

0

2

0

60

RMSProp (midpoints)
central flow

500 1000 1500 2000 2500 3000

coordinates of

10 4

RMSProp
central flow

10 6

0

500 1000 1500 2000 2500 3000

distance to RMSProp

2.5

central flow
stable flow

2.0
1.5
1.0

10 10

4

test accuracy (%)

40

10 8

0

0

50

150

0.00
0.10

empirical variance
along each mode
predicted variances

4

top hessian eigenvalues

RMSProp
RMSProp (average)
central flow
central flow prediction

0.10

1e 5

2

gradient norm2
0.15

5
3

2
1

0.25

real vs. predicted oscillation covariance

top effective hessian eigenvalues

4

0.5

500 1000 1500 2000 2500 3000

0

500 1000 1500 2000 2500 3000

0.0

0

500 1000 1500 2000 2500 3000

Figure 56.1: RMSProp central flow for a CNN with CE loss, η = 7e-06, β2 = 0.95, ϵ = 1e-08, and bias correction.
train loss
RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

1.25
1.00
0.75

top effective hessian eigenvalues

4

RMSProp
central flow
stable flow (top 1)

3

1

0.00

0

0

500 1000 1500 2000 2500 3000

gradient norm2
RMSProp
RMSProp (average)
central flow
central flow prediction

0.15
0.10
0.05

0.000025
0

500 1000 1500 2000 2500 3000
RMSProp
central flow
stable flow (top 1)

200

50

0.10

0

0

500 1000 1500 2000 2500 3000

network outputs on test example
RMSProp
central flow

5.0
2.5
0.0

0

0

500 1000 1500 2000 2500 3000

test accuracy (%)
60

RMSProp (midpoints)
central flow

500 1000 1500 2000 2500 3000

0

coordinates of

10 4
10 6

0

500 1000 1500 2000 2500 3000

distance to RMSProp
RMSProp
central flow

10 10

5.0

500 1000 1500 2000 2500 3000

40

10 8

2.5

0

50

150

0.05

0.000000

top hessian eigenvalues

250

100

0.000100
0.000050

300

0.00

empirical variance
along each mode
predicted variances

0.000125
0.000075

2

0.50
0.25

real vs. predicted oscillation covariance

500 1000 1500 2000 2500 3000

6
5
4
3
2
1
0

central flow
stable flow

0

500 1000 1500 2000 2500 3000

Figure 56.2: RMSProp central flow for a CNN with CE loss, η = 1e-05, β2 = 0.95, ϵ = 1e-08, and bias correction.

158

train loss
1.00
0.75

top effective hessian eigenvalues

4

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

1.25

real vs. predicted oscillation covariance

RMSProp
central flow
stable flow (top 1)

3
2

0.0004

1

0.0002

0.50
0.25
0.00

0

500 1000 1500 2000 2500 3000

gradient norm2

0.5
0.3
0.1

100

0.0

50
500 1000 1500 2000 2500 3000

network outputs on test example

10

60

40
0

0

coordinates of

0

500 1000 1500 2000 2500 3000

distance to RMSProp
RMSProp
central flow

central flow
stable flow

5
4
3
2

10 10

10

RMSProp (midpoints)
central flow

500 1000 1500 2000 2500 3000

10 8

5

500 1000 1500 2000 2500 3000

50

10 6

0

0

test accuracy (%)

RMSProp
central flow
stable flow (top 1)

10 4

RMSProp
central flow

5

0

0.0000

top hessian eigenvalues

200
150

0

500 1000 1500 2000 2500 3000

250

0.2

0.1

0

300

RMSProp
RMSProp (average)
central flow
central flow prediction

0.4

0

empirical variance
along each mode
predicted variances

0.0006

500 1000 1500 2000 2500 3000

1
0

0

500 1000 1500 2000 2500 3000

0

500 1000 1500 2000 2500 3000

Figure 56.3: RMSProp central flow for a CNN with CE loss, η = 2e-05, β2 = 0.95, ϵ = 1e-08, and bias correction.

train loss

1.4

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

1.2
1.0
0.8
0.6
0

1000

2000

3000

4000

gradient norm2
1.25
1.00

RMSProp
central flow
stable flow (top 1)

3

4

2

1

1

0

0

1000

2000

3000

4000

top hessian eigenvalues
RMSProp
central flow
stable flow (top 1)

800

0

60

400

55

0.25

200

50

0

1000

2000

3000

4000

0

0

1000

RMSProp
central flow

3000

10 14

2

10 17
0

1000

2000

3000

4000

4000

0

1000

2000

3000

4000

central flow
stable flow

0.6
0.4

10 11

1

3000

distance to RMSProp

10 8

0

2000

RMSProp (midpoints)
central flow

4000

RMSProp
central flow

10 5

1000

test accuracy (%)

coordinates of

network outputs on test example
1

2000

0

65

600

0.00

empirical variance
along each mode
predicted variances

70

0.50

0.75

1e 5

3

2

1000

RMSProp
RMSProp (average)
central flow
central flow prediction

real vs. predicted oscillation covariance

top effective hessian eigenvalues

4

0.2
0

1000

2000

3000

4000

0.0

0

1000

2000

3000

4000

Figure 56.4: RMSProp central flow for a ResNet with CE loss, η = 1e-05, β2 = 0.95, ϵ = 1e-08, and bias correction.

159

train loss

1.4
1.2
1.0
0.8
0.6
0.4
0.2

real vs. predicted oscillation covariance

RMSProp
central flow
stable flow (top 1)

3
2
1

0

1000

2000

3000

4000

gradient norm2

2

0

0

1000

2000

3000

4000

top hessian eigenvalues

3000

RMSProp
RMSProp (average)
central flow
central flow prediction

3

RMSProp
central flow
stable flow (top 1)

2500
2000

1000

2000

3000

4000

50
45

0

1000

2000

3000

4000

RMSProp
central flow

10 4

RMSProp (midpoints)
central flow
0

2000

3000

4000

central flow
stable flow

2.0

1.0
0.5

10 16
4000

1000

1.5

10 13

3000

4000

distance to RMSProp

10 10

2000

3000

65

0

10 7

1000

2000

test accuracy (%)

coordinates of

RMSProp
central flow

0

1000

70

500

network outputs on test example
2
1
0
1
2
3

0

55

1000

0

empirical variance
along each mode
predicted variances

0.0006
0.0005
0.0004
0.0003
0.0002
0.0001
0.0000

60

1500

1
0

top effective hessian eigenvalues

4

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

0

1000

2000

3000

4000

0.0

0

1000

2000

3000

4000

Figure 56.5: RMSProp central flow for a ResNet with CE loss, η = 2e-05, β2 = 0.95, ϵ = 1e-08, and bias correction.
train loss
RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

1.25
1.00
0.75

top effective hessian eigenvalues

4

RMSProp
central flow
stable flow (top 1)

3

0.00

0

1000

2000

3000

4000

0.0010

1

0.0005

0

0

1000

gradient norm2
6

3000

4000

0.0000

RMSProp
central flow
stable flow (top 1)

2000
1500

55
50

3000

4000

0

0

1000

RMSProp
central flow

2.5

3000

4000

45

0

2000

3000

4000

distance to RMSProp
RMSProp
central flow

10 3

1000

central flow
stable flow

3

10 6

0.0
2.5
5.0
7.5

2000

RMSProp (midpoints)
central flow

coordinates of

network outputs on test example
5.0

4000

60

500
2000

3000

65

2
1000

2000

70

1000

0

1000

test accuracy (%)

4

0

0

top hessian eigenvalues

RMSProp
RMSProp (average)
central flow
central flow prediction

8

2000

empirical variance
along each mode
predicted variances

0.0015

2

0.50
0.25

real vs. predicted oscillation covariance
0.0020

10 9

2

10 12

1

10 15
0

1000

2000

3000

4000

0

1000

2000

3000

4000

0

0

1000

2000

3000

4000

Figure 56.6: RMSProp central flow for a ResNet with CE loss, η = 4e-05, β2 = 0.95, ϵ = 1e-08, and bias correction.

160

train loss

1.50
1.25
1.00
0.75

top effective hessian eigenvalues

4

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

RMSProp
central flow
stable flow (top 1)

3
2

0.50

15.0
12.5
10.0
7.5
5.0
2.5
0.0

1000

2000

3000

4000

0.0002
0

1000

gradient norm2

1000

2000

3000

4000

network outputs on test example
2

2000

3000

4000

6000
5000
4000
3000
2000
1000
0

0
1
3000

2000

3000

4000

60
50

RMSProp (midpoints)
central flow

30
0

1000

2000

3000

4000

0

coordinates of

1000

2000

3000

4000

distance to RMSProp
RMSProp
central flow

central flow
stable flow

1.0
0.8

10 6

2000

1000

40

10 4

1

1000

0

test accuracy (%)

RMSProp
central flow
stable flow (top 1)

10 2

RMSProp
central flow

0

0.0000

top hessian eigenvalues

RMSProp
RMSProp (average)
central flow
central flow prediction

0

0.0008
0.0004

1
0

empirical variance
along each mode
predicted variances

0.0010
0.0006

0.25
0

real vs. predicted oscillation covariance

0.0012

0.6

10 8

0.4

10 10

0.2

4000

0

1000

2000

3000

4000

0.0

0

1000

2000

3000

4000

Figure 56.7: RMSProp central flow for a ViT with CE loss, η = 5e-06, β2 = 0.95, ϵ = 1e-08, and bias correction.
train loss

1.50

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

1.25
1.00
0.75
0.50

0

1000

2000

3000

4000

gradient norm2

15

2

0

0

1000

2000

3000

4000

RMSProp
central flow
stable flow (top 1)

2000

3000

4000

2
1

0

1000

2000

3000

RMSProp
central flow

10 2
10 4

0

3000

4000

central flow
stable flow

0.8
0.4

10 10

0.2

4000

2000

1.0

2
3000

1000

distance to RMSProp

1.2

0.6

2000

4000

RMSProp (midpoints)
central flow

4000

coordinates of

10 8

1000

3000

50

1
0

2000

60

10 6

0

1000

30

network outputs on test example
RMSProp
central flow

0

40

1000
1000

0.0000

test accuracy (%)

2000

0

0.0015

top hessian eigenvalues

3000

0

empirical variance
along each mode
predicted variances

0.0005

4000

5

real vs. predicted oscillation covariance
0.0020

0.0010

5000

10

3

3

6000

RMSProp
RMSProp (average)
central flow
central flow prediction

20

0

RMSProp
central flow
stable flow (top 1)

1

0.25
0.00

top effective hessian eigenvalues

4

0

1000

2000

3000

4000

0.0

0

1000

2000

3000

4000

Figure 56.8: RMSProp central flow for a ViT with CE loss, η = 7e-06, β2 = 0.95, ϵ = 1e-08, and bias correction.

161

train loss

1.50
1.25
1.00
0.75
0.25
0

1000

2000

3000

gradient norm2

20

0

1000

0.001
1000

2000

2000

3000

50

2000

40

1000

30
0

1000

2000

3000

2
3000

2000

3000

4000

RMSProp (midpoints)
central flow

4000

0

1000

2000

3000

4000

distance to RMSProp

1.2

RMSProp
central flow

10 2

central flow
stable flow

1.0
0.8

10 6

0.6

10 8

0.4

10 10

0.2

0

2000

1000

60

10 4

1000

0

coordinates of

RMSProp
central flow

0

0.000

test accuracy (%)

RMSProp
central flow
stable flow (top 1)

network outputs on test example
2

4000

3000

0

4000

3000

top hessian eigenvalues

4000

10
0

1

5000

RMSProp
RMSProp (average)
central flow
central flow prediction

30

0.002

0

empirical variance
along each mode
predicted variances

0.003

2

0

4000

real vs. predicted oscillation covariance

RMSProp
central flow
stable flow (top 1)

3

0.50
0.00

top effective hessian eigenvalues

4

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

4000

0

1000

2000

3000

4000

0.0

0

1000

2000

3000

4000

Figure 56.9: RMSProp central flow for a ViT with CE loss, η = 1e-05, β2 = 0.95, ϵ = 1e-08, and bias correction.

train loss

1.4

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

1.2
1.0
0.8

RMSProp
central flow
stable flow (top 1)

3

0

1000

2000

3000

4000

5000

0

0.05

0

1000

100

0.10

0

1000

2000

3000

3000

4000

5000

RMSProp
central flow
stable flow (top 1)

200

0.05
0

2000

300

0.00

4000

5000

network outputs on test example
RMSProp
central flow

0

1000

2000

3000

4000

RMSProp
central flow

10 7

3000

4000

5000

1000

2000

3000

4000

5000

test accuracy (%)

90
80
70
60
50
40
30

RMSProp (midpoints)
central flow
0

1000

2000

3000

4000

5000

distance to RMSProp

0.08

central flow
stable flow

0.06

0.02

10 10
2000

0

0.04

10 9

1000

0.0

5000

coordinates of

10 6

10 8

0

2.0

empirical variance
along each mode
predicted variances

0.5

top hessian eigenvalues

RMSProp
RMSProp (average)
central flow
central flow prediction

2.5

1.0

gradient norm2
0.10

3.0 1e 6

1.5

2
1

0.6

0.00
0.25
0.50
0.75
1.00
1.25
1.50

real vs. predicted oscillation covariance

top effective hessian eigenvalues

4

0

1000

2000

3000

4000

5000

0.00

0

1000

2000

3000

4000

5000

Figure 56.10: RMSProp central flow for a LSTM with CE loss, η = 1e-05, β2 = 0.95, ϵ = 1e-08, and bias correction.

162

train loss
1.25
1.00
0.75
0.25
1000

2000

3000

4000

RMSProp
central flow
stable flow (top 1)

3

0.50

0

top effective hessian eigenvalues

4

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

5000

0.0002

1

0.0001

0

0

1000

1000

0.0

0

2000

3000

4000

5000

network outputs on test example
0.0

5000

2000

0.5
1000

4000

RMSProp
central flow
stable flow (top 1)

3000

1.0

0

3000

top hessian eigenvalues

RMSProp
RMSProp (average)
central flow
central flow prediction

1.5

2000

0.5

10 5
10 6

1.0

10 7
10 8

0

1000 2000 3000 4000 5000

0

1000

2000

3000

4000

5000

test accuracy (%)

100
80

40
0

1000

2000

3000

4000

RMSProp (midpoints)
central flow

5000

0

coordinates of

1000 2000 3000 4000 5000

distance to RMSProp
RMSProp
central flow

central flow
stable flow

0.6
0.4
0.2

10 9

1.5

0.0000

60

10 4

RMSProp
central flow

empirical variance
along each mode
predicted variances

0.0003

2

gradient norm2
2.0

real vs. predicted oscillation covariance

10 10

0

1000 2000 3000 4000 5000

0.0

0

1000

2000

3000

4000

5000

Figure 56.11: RMSProp central flow for a LSTM with CE loss, η = 2e-05, β2 = 0.95, ϵ = 1e-08, and bias correction.
train loss
RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

1.25
1.00
0.75
0.50
0.25
0.00

0

1000

2000

3000

4000

5000

top effective hessian eigenvalues

4

RMSProp
central flow
stable flow (top 1)

3

0.002

1

0.001

0

0

1000

1000

0

0

2000

3000

4000

5000

network outputs on test example
RMSProp
central flow

0.0
0.5
1.0
1.5
0

5000

2000

5
1000

4000

RMSProp
central flow
stable flow (top 1)

3000

10

0

3000

top hessian eigenvalues

RMSProp
RMSProp (average)
central flow
central flow prediction

15

2000

1000 2000 3000 4000 5000

empirical variance
along each mode
predicted variances

0.003

2

gradient norm2
20

real vs. predicted oscillation covariance

0.000

0

1000

2000

3000

4000

5000

test accuracy (%)

100
80
60
40

0

1000

2000

3000

4000

RMSProp (midpoints)
central flow

5000

0

coordinates of

10 2

distance to RMSProp
RMSProp
central flow

10 4

1000 2000 3000 4000 5000

2.0

central flow
stable flow

1.5

10 6

1.0

10 8

0.5
0

1000 2000 3000 4000 5000

0.0

0

1000

2000

3000

4000

5000

Figure 56.12: RMSProp central flow for a LSTM with CE loss, η = 6e-05, β2 = 0.95, ϵ = 1e-08, and bias correction.

163

train loss

2.5
2.4
2.3
2.2
2.1
2.0
1.9

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

1e 5

RMSProp
central flow
stable flow (top 1)

3

500

1000

1500

2000

2500

0

0.2
0

500

1000

1500

2000

2500

80
60

1500

2000

2500

network outputs on test example
RMSProp
central flow

0.0
0.2
0.4
0.6
0

500

1000

1500

1000

2000

0

1500

2000

2500

50
40
20

20
1000

500

30

40

500

0

test accuracy (%)

RMSProp
central flow
stable flow (top 1)

100

0.5
0

0.0

top hessian eigenvalues

RMSProp
RMSProp (average)
central flow
central flow prediction

1.0

0.8
0.4

gradient norm2
1.5

empirical variance
along each mode
predicted variances

1.0
0.6

2
1

0

0.0

real vs. predicted oscillation covariance

top effective hessian eigenvalues

4

RMSProp (midpoints)
central flow

10
0

500

1000

1500

2000

2500

0

500

coordinates of

10 6
10 7
10 8
10 9
10 10
10 11
10 12

1000

1500

2000

2500

distance to RMSProp
RMSProp
central flow

central flow
stable flow

0.8
0.6
0.4
0.2

2500

0

500

1000

1500

2000

2500

0.0

0

500

1000

1500

2000

2500

Figure 56.13: RMSProp central flow for a Transformer with CE loss, η = 1e-05, β2 = 0.95, ϵ = 1e-08, and bias
correction.
train loss
RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

2.4
2.2
2.0
1.8

top effective hessian eigenvalues

4

RMSProp
central flow
stable flow (top 1)

3

1.4
0

500

1000

1500

2000

2500

0

0.00010
0.00005
0

500

gradient norm2

1.0

400

0.2

1000

1500

2000

2500

RMSProp
central flow

0.2
0.4
0.6
0.8
1.0

0

500

2500

0.00000

0

500

20
500

1000 1500 2000 2500

1000

1500

2000

2500

0

500

500

1000 1500 2000 2500

distance to RMSProp
RMSProp
central flow

0

2500

RMSProp (midpoints)
central flow

coordinates of
10 6
10 7
10 8
10 9
10 10
10 11
10 12

2000

60

100
0

1500

80

40

0

1000

test accuracy (%)

200

network outputs on test example

0.0

2000

300

0.0
500

1500

RMSProp
central flow
stable flow (top 1)

500

0.5

0

1000

top hessian eigenvalues

RMSProp
RMSProp (average)
central flow
central flow prediction

1.5

empirical variance
along each mode
predicted variances

0.00020
0.00015

2
1

1.6

real vs. predicted oscillation covariance

1000 1500 2000 2500

1.2
1.0
0.8
0.6
0.4
0.2
0.0

central flow
stable flow

0

500

1000

1500

2000

2500

Figure 56.14: RMSProp central flow for a Transformer with CE loss, η = 2e-05, β2 = 0.95, ϵ = 1e-08, and bias
correction.

164

train loss

2.5
2.0
1.5

0.5

0

1000

1500

2000

2500

0.00050
0.00025
0

500

1.5

400

0.5

200

0.0

0

500

1000

1500

2000

2500

network outputs on test example

2000

2500

0.5

10 7

1.0

10 9

1.5
500

0

500

1000

1500

2000

2500

test accuracy (%)

100
80

40
RMSProp (midpoints)
central flow

20
0

500

1000

1500

2000

2500

0

coordinates of

500

1000 1500 2000 2500

distance to RMSProp
RMSProp
central flow

central flow
stable flow

2.5
2.0
1.5
1.0
0.5

10 11
0

0.00000

60

10 5

RMSProp
central flow

0.0

1500

RMSProp
central flow
stable flow (top 1)

600

1.0

0

1000

top hessian eigenvalues

RMSProp
RMSProp (average)
central flow
central flow prediction

empirical variance
along each mode
predicted variances

0.00100
0.00075

gradient norm2
2.0

real vs. predicted oscillation covariance
0.00125

2
1

500

RMSProp
central flow
stable flow (top 1)

3

1.0
0

top effective hessian eigenvalues

4

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

1000 1500 2000 2500

0

500

1000 1500 2000 2500

0.0

0

500

1000

1500

2000

2500

Figure 56.15: RMSProp central flow for a Transformer with CE loss, η = 4e-05, β2 = 0.95, ϵ = 1e-08, and bias
correction.
train loss

1.4

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

1.2
1.0
0.8

top effective hessian eigenvalues

4

RMSProp
central flow
stable flow (top 1)

3

0

1000

2000

3000

4000

5000

0

0.0001
0

1000

15

15000
10000

5

5000
0

1000

2000

3000

4000

5000

RMSProp
central flow

0.25

0

5000

0

1000

2000

3000

4000

10 8

0.50

10 10
0

1000 2000 3000 4000 5000

1000

2000

3000

4000

5000

test accuracy (%)

70
60

RMSProp (midpoints)
central flow

5000

RMSProp
central flow

10 4

0.25

0

30

10 2
10 6

0.0000

40

coordinates of

0.00

0.75

4000

50

network outputs on test example
0.50

3000

RMSProp
central flow
stable flow (top 1)

20000

10

0

2000

top hessian eigenvalues

RMSProp
RMSProp (average)
central flow
central flow prediction

0.0004
0.0002

gradient norm2
20

empirical variance
along each mode
predicted variances

0.0003

2
1

0.6

real vs. predicted oscillation covariance
0.0005

0

1000 2000 3000 4000 5000

distance to RMSProp

0.10

central flow
stable flow

0.08
0.06
0.04
0.02

0

1000 2000 3000 4000 5000

0.00

0

1000

2000

3000

4000

5000

Figure 56.16: RMSProp central flow for a Mamba with CE loss, η = 7e-06, β2 = 0.95, ϵ = 1e-08, and bias correction.

165

train loss

1.4

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

1.2
1.0
0.8
0.6
0.4
0

1000

2000

3000

4000

5000

gradient norm2
100

3

1

0.0005

0

0

1000

2000

3000

4000

5000

top hessian eigenvalues
RMSProp
central flow
stable flow (top 1)

10000

25
0

1000

2000

3000

4000

5000

network outputs on test example
RMSProp
central flow

1.0
0.5
0.0

0

0

1000

2000

3000

4000

0

1000

100

70
65
60
55
50
45
40

4000

5000

RMSProp (midpoints)
central flow
0

1000 2000 3000 4000 5000
central flow
stable flow

0.20

10 4

0.15

10 6

0.10

1000 2000 3000 4000 5000

3000

distance to RMSProp
RMSProp
central flow

10 2

2000

test accuracy (%)

5000

0.05

10 10
0

0.0000

coordinates of

10 8

0.5

empirical variance
along each mode
predicted variances

0.0015
0.0010

20000

50

real vs. predicted oscillation covariance

2

30000

75

0

RMSProp
central flow
stable flow (top 1)

40000

RMSProp
RMSProp (average)
central flow
central flow prediction

125

top effective hessian eigenvalues

4

0

1000 2000 3000 4000 5000

0.00

0

1000

2000

3000

4000

5000

Figure 56.17: RMSProp central flow for a Mamba with CE loss, η = 1e-05, β2 = 0.95, ϵ = 1e-08, and bias correction.
train loss

1.4
1.2
1.0
0.8
0.6
0.4
0.2

RMSProp
RMSProp (average)
central flow
central flow prediction
stable flow

RMSProp
central flow
stable flow (top 1)

3

1000

2000

3000

4000

5000

0

200
100
0

1000

2000

3000

0.001
0

1000

4000

5000

30000
25000
20000
15000
10000
5000
0

RMSProp
central flow

2

3000

4000

5000

0
0

0.000

0

1000

1000 2000 3000 4000 5000

2000

3000

4000

5000

test accuracy (%)
70
65
60
55
RMSProp (midpoints)
central flow

50
0

1000

2000

3000

4000

5000

0

coordinates of
100
10 2

1000 2000 3000 4000 5000

distance to RMSProp
RMSProp
central flow

central flow
stable flow

0.5
0.4

10 4
10 6

1

1

2000

RMSProp
central flow
stable flow (top 1)

network outputs on test example
3

empirical variance
along each mode
predicted variances

0.004

top hessian eigenvalues

RMSProp
RMSProp (average)
central flow
central flow prediction

300

real vs. predicted oscillation covariance

0.002

gradient norm2

400

0.005

0.003

2
1

0

0

top effective hessian eigenvalues

4

0.3

10 8

0.2

10 10

0.1
0

1000 2000 3000 4000 5000

0.0

0

1000

2000

3000

4000

5000

Figure 56.18: RMSProp central flow for a Mamba with CE loss, η = 2e-05, β2 = 0.95, ϵ = 1e-08, and bias correction.

166

